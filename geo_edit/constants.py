API_KEY = ""

SYSTEM_PROMPT = '''
You are an advanced AI agent capable of complex
reasoning and tool usage. You must strictly adhere
to the following protocol for every interaction:
1. ALWAYS call the appropriate tool first;
2. NEVER provide answers without tool results;
3. Call appropriate tools based on the task;
4. If you call multiple tools in one action, only the final result will be returned;
5. Reasoning Before Action: before selecting a tool,
you must analyze the user's request and determine
the necessary steps. Output your internal monologue
and logic inside <think> and </think> tags.
6. Reasoning After Action: Once you receive the
output from a tool, you must analyze the results to
determine if further actions are needed or if the task
is complete. Output this analysis inside <think> and
</think> tags;
7. Final Output: When you have formulated your
conclusion, you must wrap your final answer in
<answer> and </answer> tags.
'''

MATHVISION_INPUT_TEMPLATE = '''
Please solve the problem with provided tools. After you confirm the final answer, put your answer in one '<answer>\\boxed{{}}</answer>'. If it is a multiple choice question, only one letter is allowed in the '<answer>\\boxed{{}}</answer>'.\n{question}\n{options}
'''

NOTOOL_INPUT_TEMPLATE = '''
Please solve the problem step by step. After you confirm the final answer, put your answer in one '<answer>\\boxed{{}}</answer>'. If it is a multiple choice question, only one letter is allowed in the '<answer>\\boxed{{}}</answer>'.\n{question}\n{options}
'''

TOOL_EXECUTION_SUCCESS_PROMPT="All your tool calls were executed successfully. Now you can check the tool execution results and decide your next action."

TOOL_EXECUTION_FAILURE_PROMPT="Some of your tool calls failed. Please carefully check the tool execution results, identify the failed tool calls, and decide your next action accordingly."

EVAL_SYSTEM_PROMPT = (
    "You are an intelligent chatbot designed for evaluating the correctness of generative outputs "
    "for question-answer pairs.\n"
    "Your task is to compare the predicted answer with the correct answer and determine if they match meaningfully.\n"
    "------\n"
    "##INSTRUCTIONS:\n"
    "- Focus on the meaningful match between the predicted answer and the correct answer.\n"
    "- Consider synonyms or paraphrases as valid matches.\n"
    "- Evaluate the correctness of the prediction compared to the answer."
)

EVAL_QUERY_PROMPT = (
    "I will give you a question related to an image and the following text as inputs:\n\n"
    "1. **Question Related to the Image**: {question}\n"
    "2. **Ground Truth Answer**: {ground_truth}\n"
    "3. **Model Predicted Answer**: {prediction}\n\n"
    "Your task is to evaluate the model's predicted answer against the ground truth answer, "
    "based on the context provided by the question related to the image. Consider the following criteria for evaluation:\n"
    "- **Relevance**: Does the predicted answer directly address the question posed?\n"
    "- **Accuracy**:\n"
    "(1) If the ground truth answer is open-ended, consider whether the prediction accurately reflects the information "
    "given in the ground truth without introducing factual inaccuracies.\n"
    "(2) If the ground truth answer is a definitive answer, strictly compare the model's prediction to the actual answer. "
    "Pay attention to unit conversions such as length and angle, etc. As long as the results are consistent, the model's "
    "prediction should be deemed correct.\n\n"
    "**Output Format**:\n"
    'Your response should include an integer score indicating the correctness of the prediction: 1 for correct and 0 for incorrect.\n'
    'The format should be "Score: 0 or 1"'
)

MAX_TOOL_CALLS = 8

