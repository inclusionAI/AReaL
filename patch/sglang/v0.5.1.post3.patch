diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py
index 8d5b7c715..c83b99191 100644
--- a/python/sglang/srt/model_executor/model_runner.py
+++ b/python/sglang/srt/model_executor/model_runner.py
@@ -108,6 +108,7 @@ from sglang.srt.torch_memory_saver_adapter import TorchMemorySaverAdapter
 from sglang.srt.utils import (
     MultiprocessingSerializer,
     cpu_has_amx_support,
+    debug_function,
     dynamic_import,
     enable_show_time_cost,
     get_available_gpu_memory,
@@ -253,6 +254,8 @@ class ModelRunner:
         self._model_update_group = {}
 
     def initialize(self, min_per_gpu_memory: float):
+        logger.info("SGLang v0.5.1.post3 is patched with customized weight loading.")
+        print("[Debug] SGLang v0.5.1.post3 is patched with customized weight loading.")
         server_args = self.server_args
 
         self.memory_saver_adapter = TorchMemorySaverAdapter.create(
@@ -644,6 +647,7 @@ class ModelRunner:
         )
         return min_per_gpu_memory
 
+    @debug_function
     def load_model(self):
         before_avail_memory = get_available_gpu_memory(self.device, self.gpu_id)
         logger.info(
@@ -764,6 +768,7 @@ class ModelRunner:
             rank=self.tp_rank,
         )
 
+    @debug_function
     def update_weights_from_disk(
         self, model_path: str, load_format: str
     ) -> tuple[bool, str]:
@@ -775,8 +780,11 @@ class ModelRunner:
 
         target_device = torch.device(self.device)
         self.model_config.model_path = model_path
-        load_config = LoadConfig(load_format=load_format)
-
+        load_config = LoadConfig(
+            load_format=load_format,
+            # XXX: This should be in function args, passed in by requests
+            model_loader_extra_config=self.server_args.model_loader_extra_config,
+        )
         # Only support DefaultModelLoader for now
         loader = get_model_loader(load_config)
         if not isinstance(loader, DefaultModelLoader):
@@ -790,7 +798,9 @@ class ModelRunner:
             return iter
 
         def model_load_weights(model, iter):
-            DefaultModelLoader.load_weights_and_postprocess(model, iter, target_device)
+            DefaultModelLoader.load_weights_and_postprocess(
+                model, iter, target_device, load_config=load_config
+            )
             return model
 
         with set_default_torch_dtype(self.model_config.dtype):
diff --git a/python/sglang/srt/model_loader/loader.py b/python/sglang/srt/model_loader/loader.py
index 23d70be44..5c44a9278 100644
--- a/python/sglang/srt/model_loader/loader.py
+++ b/python/sglang/srt/model_loader/loader.py
@@ -62,6 +62,7 @@ from sglang.srt.model_loader.weight_utils import (
     set_runai_streamer_env,
 )
 from sglang.srt.utils import (
+    debug_function,
     get_bool_env_var,
     get_device_capability,
     is_npu,
@@ -263,7 +264,7 @@ class DefaultModelLoader(BaseModelLoader):
     def __init__(self, load_config: LoadConfig):
         super().__init__(load_config)
         extra_config = load_config.model_loader_extra_config
-        allowed_keys = {"enable_multithread_load", "num_threads"}
+        allowed_keys = {"enable_multithread_load", "num_threads", "enable_fast_load"}
         unexpected_keys = set(extra_config.keys()) - allowed_keys
 
         if unexpected_keys:
@@ -299,6 +300,7 @@ class DefaultModelLoader(BaseModelLoader):
             return model_path
         return None
 
+    @debug_function
     def _prepare_weights(
         self, model_name_or_path: str, revision: Optional[str], fall_back_to_pt: bool
     ) -> Tuple[str, List[str], bool]:
@@ -379,11 +381,15 @@ class DefaultModelLoader(BaseModelLoader):
 
         return hf_folder, hf_weights_files, use_safetensors
 
+    @debug_function
     def _get_weights_iterator(
         self, source: "Source"
     ) -> Generator[Tuple[str, torch.Tensor], None, None]:
         """Get an iterator for the model weights based on the load format."""
         extra_config = self.load_config.model_loader_extra_config
+        if extra_config.get("enable_fast_load"):
+            return source.model_or_path
+
         hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
             source.model_or_path, source.revision, source.fall_back_to_pt
         )
@@ -426,16 +432,14 @@ class DefaultModelLoader(BaseModelLoader):
                 )
             else:
                 weights_iterator = pt_weights_iterator(hf_weights_files)
-
-        # Apply the prefix.
         return ((source.prefix + name, tensor) for (name, tensor) in weights_iterator)
 
+    @debug_function
     def _get_all_weights(
         self,
         model_config: ModelConfig,
         model: nn.Module,
     ) -> Generator[Tuple[str, torch.Tensor], None, None]:
-
         primary_weights = DefaultModelLoader.Source.init_new(model_config, model)
         yield from self._get_weights_iterator(primary_weights)
 
@@ -450,6 +454,7 @@ class DefaultModelLoader(BaseModelLoader):
             model_config.model_path, model_config.revision, fall_back_to_pt=True
         )
 
+    @debug_function
     def load_model(
         self,
         *,
@@ -464,15 +469,23 @@ class DefaultModelLoader(BaseModelLoader):
                     self.load_config,
                 )
 
+        extra_config = self.load_config.model_loader_extra_config
+        if extra_config.get("enable_fast_load"):
+            weights_iter_or_path = model_config.model_path
+        else:
+            weights_iter_or_path = self._get_all_weights(model_config, model)
         self.load_weights_and_postprocess(
-            model, self._get_all_weights(model_config, model), target_device
+            model, weights_iter_or_path, target_device, load_config=self.load_config
         )
-
         return model.eval()
 
     @staticmethod
-    def load_weights_and_postprocess(model, weights, target_device):
-        model.load_weights(weights)
+    def load_weights_and_postprocess(model, weights, target_device, load_config=None):
+        extra_config = load_config.model_loader_extra_config
+        if extra_config.get("enable_fast_load"):
+            model.load_weights_from_path(weights)
+        else:
+            model.load_weights(weights)
 
         for _, module in model.named_modules():
             quant_method = getattr(module, "quant_method", None)
diff --git a/python/sglang/srt/models/qwen3.py b/python/sglang/srt/models/qwen3.py
index 042159a50..4f8b8a1d7 100644
--- a/python/sglang/srt/models/qwen3.py
+++ b/python/sglang/srt/models/qwen3.py
@@ -27,7 +27,7 @@ from sglang.srt.model_executor.forward_batch_info import ForwardBatch, PPProxyTe
 from sglang.srt.model_loader.weight_utils import default_weight_loader
 from sglang.srt.models.qwen2 import Qwen2MLP as Qwen3MLP
 from sglang.srt.models.qwen2 import Qwen2Model
-from sglang.srt.utils import add_prefix, is_cuda
+from sglang.srt.utils import add_prefix, debug_function, is_cuda
 
 Qwen3Config = None
 
@@ -415,6 +415,99 @@ class Qwen3ForCausalLM(nn.Module):
     def end_layer(self):
         return self.model.end_layer
 
+    @property
+    def stacked_params_mapping(self) -> List[Tuple[str, str, str]]:
+        return [
+            # (param_name, shard_name, shard_id)
+            ("qkv_proj", "q_proj", "q"),
+            ("qkv_proj", "k_proj", "k"),
+            ("qkv_proj", "v_proj", "v"),
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+
+    def _load_weights_with_worker(
+        self,
+        params: Dict[str, torch.nn.Parameter],
+        local_names: List[str],
+        filenames: List[str],
+        weight_path: str,
+    ):
+        import os
+
+        from safetensors import safe_open
+
+        from sglang.srt.model_loader.weight_utils import default_weight_loader
+
+        all_slices = {}
+        for filename in filenames:
+            safetensor_file = os.path.join(weight_path, filename)
+            with safe_open(safetensor_file, framework="pt", device="cpu") as f:
+                for name in f.keys():
+                    # all_slices[name] = f.get_slice(name)
+                    all_slices[name] = f.get_tensor(name)
+
+        for local_name in local_names:
+            # Skip loading extra bias for GPTQ models.
+            if local_name.endswith(".bias") and local_name not in params:
+                continue
+            # Handle special cases
+            if "rotary_emb.inv_freq" in local_name or "projector" in local_name:
+                continue
+            if (
+                "rotary_emb.cos_cached" in local_name
+                or "rotary_emb.sin_cached" in local_name
+            ):
+                # Models trained using ColossalAI may include these tensors in
+                # the checkpoint. Skip them.
+                continue
+            if local_name.startswith("model.vision_tower") and local_name not in params:
+                continue
+
+            param = params[local_name]
+            # Handle weight tying
+            if self.config.tie_word_embeddings and "lm_head.weight" in local_name:
+                if self.pp_group.world_size > 1 and self.pp_group.is_last_rank:
+                    local_name = "model.embed_tokens.weight"
+
+            loaded = False
+            for param_name, shard_name, shard_id in self.stacked_params_mapping:
+                if param_name not in local_name:
+                    continue
+                # If local_name weight is sharded into multiple keys
+                weight_loader = param.weight_loader
+                slice_name = local_name.replace(param_name, shard_name)
+                loaded_weight = all_slices[slice_name]
+                weight_loader(param, loaded_weight, shard_id)
+                loaded = True
+
+            if not loaded:
+                # If local_name weight is not sharded
+                if local_name in all_slices:
+                    loaded_weight = all_slices[local_name]
+                    weight_loader = getattr(
+                        param, "weight_loader", default_weight_loader
+                    )
+                    weight_loader(param, loaded_weight)
+                else:
+                    raise KeyError(
+                        f"Cannot find weight {local_name} in the loaded slices."
+                    )
+
+    @debug_function
+    def load_weights_from_path(self, path: str):
+        # Customized weights loading from a given path of huggingface model
+        from sglang.srt.models.utils.load import load_weights_with_hf_path_fast
+
+        load_weights_with_hf_path_fast(
+            model=self,
+            weight_path=path,
+            load_weights_with_worker_fn=self._load_weights_with_worker,
+            stacked_params_mapping=self.stacked_params_mapping,
+            tie_word_embeddings=self.config.tie_word_embeddings,
+        )
+
+    @debug_function
     def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
         stacked_params_mapping = [
             # (param_name, shard_name, shard_id)
@@ -462,11 +555,11 @@ class Qwen3ForCausalLM(nn.Module):
             for param_name, weight_name, shard_id in stacked_params_mapping:
                 if weight_name not in name:
                     continue
-                name = name.replace(weight_name, param_name)
+                _name = name.replace(weight_name, param_name)
                 # Skip loading extra bias for GPTQ models.
-                if name.endswith(".bias") and name not in params_dict:
+                if _name.endswith(".bias") and _name not in params_dict:
                     continue
-                param = params_dict[name]
+                param = params_dict[_name]
                 weight_loader = param.weight_loader
                 weight_loader(param, loaded_weight, shard_id)
                 break
diff --git a/python/sglang/srt/models/qwen3_moe.py b/python/sglang/srt/models/qwen3_moe.py
index fcb45b947..e354e7a47 100644
--- a/python/sglang/srt/models/qwen3_moe.py
+++ b/python/sglang/srt/models/qwen3_moe.py
@@ -739,6 +739,130 @@ class Qwen3MoeForCausalLM(nn.Module):
         else:
             self.model.layers_to_capture = [val + 1 for val in layer_ids]
 
+    @property
+    def stacked_params_mapping(self) -> List[Tuple[str, str, str]]:
+        return [
+            # (param_name, shard_name, shard_id)
+            ("qkv_proj", "q_proj", "q"),
+            ("qkv_proj", "k_proj", "k"),
+            ("qkv_proj", "v_proj", "v"),
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+
+    @property
+    def expert_params_mapping(self) -> List[Tuple[str, str, int, str]]:
+        return get_moe_impl_class().make_expert_params_mapping(
+            ckpt_gate_proj_name="gate_proj",
+            ckpt_down_proj_name="down_proj",
+            ckpt_up_proj_name="up_proj",
+            num_experts=self.config.num_experts,
+        )
+
+    def _load_weights_with_worker(
+        self,
+        params: Dict[str, torch.nn.Parameter],
+        local_names: List[str],
+        filenames: List[str],
+        weight_path: str,
+    ):
+        import os
+
+        from safetensors import safe_open
+
+        from sglang.srt.model_loader.weight_utils import default_weight_loader
+
+        all_slices = {}
+        for filename in filenames:
+            safetensor_file = os.path.join(weight_path, filename)
+            with safe_open(safetensor_file, framework="pt", device="cpu") as f:
+                for name in f.keys():
+                    # all_slices[name] = f.get_slice(name)
+                    all_slices[name] = f.get_tensor(name)
+
+        for local_name in local_names:
+            # Skip loading extra bias for GPTQ models.
+            if local_name.endswith(".bias") and local_name not in params:
+                continue
+            # Handle special cases
+            if "rotary_emb.inv_freq" in local_name or "projector" in local_name:
+                continue
+            if (
+                "rotary_emb.cos_cached" in local_name
+                or "rotary_emb.sin_cached" in local_name
+            ):
+                # Models trained using ColossalAI may include these tensors in
+                # the checkpoint. Skip them.
+                continue
+            if local_name.startswith("model.vision_tower") and local_name not in params:
+                continue
+
+            param = params[local_name]
+            # Handle weight tying
+            if self.config.tie_word_embeddings and "lm_head.weight" in local_name:
+                if self.pp_group.world_size > 1 and self.pp_group.is_last_rank:
+                    local_name = "model.embed_tokens.weight"
+
+            loaded = False
+            for param_name, shard_name, shard_id in self.stacked_params_mapping:
+                if param_name not in local_name:
+                    continue
+                if "mlp.experts" in local_name:
+                    # Skip experts here, handled below
+                    continue
+                # If local_name weight is sharded into multiple keys
+                weight_loader = param.weight_loader
+                slice_name = local_name.replace(param_name, shard_name)
+                loaded_weight = all_slices[slice_name]
+                weight_loader(param, loaded_weight, shard_id)
+                loaded = True
+
+            for (
+                param_name,
+                shard_name,
+                expert_id,
+                shard_id,
+            ) in self.expert_params_mapping:
+                if param_name not in local_name:
+                    continue
+                # If local_name weight is sharded into multiple keys
+                weight_loader = param.weight_loader
+                slice_name = local_name.replace(param_name, shard_name)
+                loaded_weight = all_slices[slice_name]
+                weight_loader(
+                    param,
+                    loaded_weight,
+                    local_name,
+                    shard_id=shard_id,
+                    expert_id=expert_id,
+                )
+                loaded = True
+
+            if not loaded:
+                # If local_name weight is not sharded
+                if local_name in all_slices:
+                    loaded_weight = all_slices[local_name]
+                    weight_loader = getattr(
+                        param, "weight_loader", default_weight_loader
+                    )
+                    weight_loader(param, loaded_weight)
+                else:
+                    raise KeyError(
+                        f"Cannot find weight {local_name} in the loaded slices."
+                    )
+
+    def load_weights_from_path(self, path: str):
+        # Customized weights loading from a given path of huggingface model
+        from sglang.srt.models.utils.load import load_weights_with_hf_path_fast
+
+        load_weights_with_hf_path_fast(
+            model=self,
+            weight_path=path,
+            load_weights_with_worker_fn=self._load_weights_with_worker,
+            stacked_params_mapping=self.stacked_params_mapping,
+            expert_params_mapping=self.expert_params_mapping,
+        )
+
     def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
         stacked_params_mapping = [
             # (param_name, shard_name, shard_id)
diff --git a/python/sglang/srt/models/utils/load.py b/python/sglang/srt/models/utils/load.py
new file mode 100644
index 000000000..f3cb03012
--- /dev/null
+++ b/python/sglang/srt/models/utils/load.py
@@ -0,0 +1,140 @@
+import json
+import os
+from collections import defaultdict
+from concurrent.futures import ThreadPoolExecutor
+from glob import glob
+from typing import Callable, Dict, List, Tuple
+
+import torch
+from safetensors import safe_open
+from transformers.utils.hub import cached_file
+
+from sglang.srt.model_loader.weight_utils import default_weight_loader
+
+
+def get_actual_hf_path(weight_path: str):
+    return os.path.dirname(cached_file(weight_path, "config.json"))
+
+
+def load_weights_with_hf_path_fast(
+    model: torch.nn.Module,
+    weight_path: str,
+    load_weights_with_worker_fn: Callable,
+    stacked_params_mapping: List[Tuple[str, str, str]] | None = None,
+    expert_params_mapping: List[Tuple[str, str, str]] | None = None,
+    tie_word_embeddings: bool = False,
+    max_workers: int = None,
+):
+    if not os.path.exists(weight_path):
+        weight_path = get_actual_hf_path(weight_path)
+    index_file = os.path.join(weight_path, "model.safetensors.index.json")
+    index = {}
+    if os.path.exists(index_file):
+        with open(index_file, "r") as f:
+            index = json.load(f)["weight_map"]
+    else:
+        # Search all safetensors files
+        safetensor_files = glob(os.path.join(weight_path, "*.safetensors"))
+        # If there are safetensors files
+        if safetensor_files:
+            # Iterate through each safetensors file
+            for safetensor_file in safetensor_files:
+                with safe_open(safetensor_file, framework="pt", device="cpu") as f:
+                    for k in f.keys():
+                        index[k] = safetensor_file
+        else:
+            raise FileNotFoundError("No safetensors found in the model path to load.")
+
+    params = dict(model.named_parameters())
+    local_names = list(params.keys())
+
+    worker_args = []
+
+    # local name -> set of filenames that contains the weight
+    local_to_file_map = defaultdict(set)
+    # model.layers.31.mlp.experts
+    for local_name in local_names:
+        hf_names = []
+        if "mlp.experts" not in local_name and stacked_params_mapping is not None:
+            for param_name, shard_name, _ in stacked_params_mapping:
+                if param_name in local_name:
+                    hf_names.append(local_name.replace(param_name, shard_name))
+        if expert_params_mapping is not None:
+            for param_name, shard_name, _, _ in expert_params_mapping:
+                if param_name in local_name:
+                    hf_names.append(local_name.replace(param_name, shard_name))
+        if tie_word_embeddings and "lm_head.weight" in local_name:
+            hf_names.append("model.embed_tokens.weight")
+        if len(hf_names) == 0:
+            hf_names.append(local_name)
+        for name in hf_names:
+            filename = index[name]
+            if filename not in local_to_file_map[local_name]:
+                local_to_file_map[local_name].add(filename)
+
+    # Use union find to create local_name groups with no file conflicts
+    parent = {name: name for name in local_names}
+    weight_groups = {name: [name] for name in local_names}
+    file_groups = {name: local_to_file_map[name] for name in local_names}
+    roots = [name for name in local_names]
+    ranks = {name: 0 for name in local_names}
+
+    def find(x):
+        if parent[x] != x:
+            parent[x] = find(parent[x])
+        return parent[x]
+
+    def union(x, y):
+        root_x = find(x)
+        root_y = find(y)
+        if root_x != root_y:
+            if ranks[root_x] > ranks[root_y]:
+                parent[root_y] = root_x
+                roots.remove(root_y)
+            elif ranks[root_x] < ranks[root_y]:
+                parent[root_x] = root_y
+                roots.remove(root_x)
+            else:
+                parent[root_y] = root_x
+                roots.remove(root_y)
+                ranks[root_x] += 1
+            # Merge file groups
+            file_groups[root_x].update(file_groups[root_y])
+            file_groups[root_y] = file_groups[root_x]
+            # Merge weight groups
+            weight_groups[root_x].extend(weight_groups[root_y])
+            weight_groups[root_y] = weight_groups[root_x]
+            return True
+        return False
+
+    for i, weight1 in enumerate(local_names):
+        for weight2 in local_names[i + 1 :]:
+            # If two weights share any files, they conflict
+            if any(fn in file_groups[weight1] for fn in file_groups[weight2]):
+                union(weight1, weight2)
+
+    grouped_local_names = [weight_groups[root] for root in roots]
+    grouped_filenames = [list(file_groups[root]) for root in roots]
+
+    if max_workers is None:
+        # assume all GPUs are used by SGLang servers
+        max_workers = min(8, max(1, os.cpu_count() // torch.cuda.device_count()))
+
+    for local_names, filenames in zip(grouped_local_names, grouped_filenames):
+        worker_args.append(
+            dict(
+                params=params,
+                local_names=local_names,
+                filenames=filenames,
+                weight_path=weight_path,
+            )
+        )
+
+    max_workers = min(max_workers, len(worker_args))
+    with ThreadPoolExecutor(max_workers=max_workers) as executor:
+        results = executor.map(
+            lambda kwargs: load_weights_with_worker_fn(**kwargs), worker_args
+        )
+        # Consume all results to make result all tasks complete
+        for _ in results:
+            pass
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b5f6626a2..a5ac2a2bd 100644
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -21,6 +21,7 @@ import ctypes
 import dataclasses
 import functools
 import importlib
+import inspect
 import io
 import ipaddress
 import itertools
@@ -3011,3 +3012,52 @@ def check_cuda_result(raw_output):
         raise Exception(f"CUDA error: {err}")
 
     return results
+
+
+def debug_function(func: Callable):
+    """Decorator that logs function entry/exit with contextual information."""
+
+    if func is None:
+        raise ValueError("debug_function decorator requires a function")
+
+    @functools.wraps(func)
+    def wrapper(*args, **kwargs):
+        def _resolve_class_name():
+            qualname = getattr(func, "__qualname__", "")
+            if qualname:
+                parts = qualname.split(".")
+                if len(parts) > 1 and parts[-2] != "<locals>":
+                    return parts[-2]
+            if args:
+                instance = args[0]
+                if hasattr(instance, "__class__"):
+                    return instance.__class__.__name__
+            return None
+
+        def _resolve_filename():
+            try:
+                source = inspect.getsourcefile(func) or inspect.getfile(func)
+            except (TypeError, OSError):
+                source = None
+            if source is None:
+                return "<unknown>"
+            return Path(source).name
+
+        class_name = _resolve_class_name()
+        filename = _resolve_filename()
+        display_name = func.__name__
+        if class_name:
+            display_name = f"{class_name}.{display_name}"
+
+        print(f"[Debug] Entering {display_name} (file: {filename})", flush=True)
+
+        start_time = time.perf_counter()
+        try:
+            return func(*args, **kwargs)
+        finally:
+            elapsed_ms = (time.perf_counter() - start_time) * 1000
+            print(
+                f"[Debug] Exiting {display_name} (file: {filename}) - elapsed {elapsed_ms:.3f} ms"
+            )
+
+    return wrapper
