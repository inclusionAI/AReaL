{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aa9f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70894b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9462bfa79df543f7bf821b58fa571f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848e624f32564a2c9b832c9a62230988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15630d0ac04420ca39a7e94970b8516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba27bb4a00a44c07acc70b3d30c545c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6306d29314e4f428f4a03e1d9edae78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/683 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb91139ffbf74903b636b5685d0560e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcc577a7e6d4ff0ba48f15b448e4196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95554bc43d0b40f7b0a9bae67f4c892e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339b6229602d4073963377cd298da681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Model checkpoint\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "# Load tokenizer and model (4-bit optional)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "# print(tokenizer.eos_token_id)\n",
    "# print(tokenizer.pad_token_id)\n",
    "# print(tokenizer.chat_template)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\", # Automatically determines the best dtype (e.g., float16, bfloat16)\n",
    "    device_map=\"auto\"   # Automatically distributes the model across available devices (e.g., GPUs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb06ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ddd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results from /Users/victorbarres/code/tau2-bench-private/data/exp/qwen2.5-7b-retries/ollama_chat/qwen2.5:7b_telecom_no-user_gpt-4.1-2025-04-14_1trials.json\n",
      "Domain: telecom\n",
      "Agent implementation: llm_agent_solo\n",
      "Solo mode: True\n",
      "Loading environment\n",
      "Tools (43):\n",
      "[Tool(name='get_customer_by_phone', short_desc='Finds a customer by their primary contact or line phone number.', long_desc='', params=<class 'tau2.environment.tool.parameters'>, returns=<class 'tau2.environment.tool.returns'>, raises=[], examples=[], info={}), Tool(name='get_customer_by_id', short_desc='Retrieves a customer directly by their unique ID.', long_desc='', params=<class 'tau2.environment.tool.parameters'>, returns=<class 'tau2.environment.tool.returns'>, raises=[], examples=[], info={}), Tool(name='get_customer_by_name', short_desc='Searches for customers by name and DOB. May return multiple matches if names are similar,', long_desc='DOB helps disambiguate.', params=<class 'tau2.environment.tool.parameters'>, returns=<class 'tau2.environment.tool.returns'>, raises=[], examples=[], info={}), Tool(name='get_details_by_id', short_desc='Retrieves the details for a given ID.', long_desc='The ID must be a valid ID for a Customer, Line, Device, Bill, or Plan.', params\n",
      "Agent constructor: <class 'tau2.agent.llm_agent.LLMSoloAgent'>\n",
      "Building agent system prompt\n",
      "Number trajectories: 74 (train) and 40 (test)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     73\u001b[39m save_path = \u001b[33m\"\u001b[39m\u001b[33m./data/test_sft_dataset.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# sft_dataset.save_to_disk(save_path)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43msft_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_json\u001b[49m(save_path, lines=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# sft_dataset = []\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# for x, y in trajs:\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m#     x = tokenizer.apply_chat_template(x, tools=openai_tools, tokenize=False)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    120\u001b[39m \n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# print(environment.get_tools())\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'DatasetDict' object has no attribute 'to_json'"
     ]
    }
   ],
   "source": [
    "from tau2.registry import registry\n",
    "from tau2.data_model.simulation import Results\n",
    "from tau2.utils.utils import DATA_DIR\n",
    "\n",
    "from tau2.utils.llm_utils import to_litellm_messages\n",
    "from tau2.data_model.message import SystemMessage\n",
    "from tau2.agent.base import is_valid_agent_history_message\n",
    "\n",
    "res_path = DATA_DIR / \"exp\" / \"qwen2.5-7b-retries\" / \"ollama_chat\" / \"qwen2.5:7b_telecom_no-user_gpt-4.1-2025-04-14_1trials.json\"\n",
    "\n",
    "def make_sft_dataset(result_path):\n",
    "    print(f\"Loading results from {result_path}\")\n",
    "    results = Results.load(result_path)\n",
    "    domain_name = results.info.environment_info.domain_name\n",
    "    print(f\"Domain: {domain_name}\")\n",
    "    agent_implementation = results.info.agent_info.implementation\n",
    "    print(f\"Agent implementation: {agent_implementation}\")\n",
    "    if \"solo\" in agent_implementation:\n",
    "        solo_mode = True\n",
    "    else:\n",
    "        solo_mode = False\n",
    "    print(f\"Solo mode: {solo_mode}\")\n",
    "    print(f\"Loading environment\")\n",
    "    environment = registry.get_env_constructor(domain_name)(solo_mode=solo_mode)\n",
    "    tools = environment.get_tools()\n",
    "    if solo_mode:\n",
    "        tools += environment.get_user_tools()\n",
    "    print(f\"Tools ({len(tools)}):\\n{tools}\"[:1000])\n",
    "    openai_tools = [tool.openai_schema for tool in tools] if tools else None\n",
    "\n",
    "    agent_constructor = registry.get_agent_constructor(agent_implementation)\n",
    "    print(f\"Agent constructor: {agent_constructor}\")\n",
    "\n",
    "    print(f\"Building agent system prompt\")\n",
    "    def get_system_prompt(task): # FIXME: This is not generic. Only works for solo mode.\n",
    "        agent = agent_constructor(tools=tools, domain_policy=environment.get_policy(), task=task)\n",
    "        return agent.system_prompt\n",
    "\n",
    "    tasks = {task.id: task for task in results.tasks}\n",
    "    data_splits = registry.get_task_splits_loader(domain_name)()\n",
    "    train_tasks, test_tasks = data_splits[\"train\"], data_splits[\"test\"]\n",
    "\n",
    "    def get_split(task_id):\n",
    "        if task_id in train_tasks:\n",
    "            return \"train\"\n",
    "        elif task_id in test_tasks:\n",
    "            return \"test\"\n",
    "        else:\n",
    "            raise ValueError(f\"Task {task_id} not found in data split\")\n",
    "\n",
    "    sft_dataset = {\"train\": [], \"test\": []}\n",
    "    for simulation in results.simulations:\n",
    "        task = tasks[simulation.task_id]\n",
    "        datapoint = make_sft_from_simulation(task, get_system_prompt, simulation, openai_tools)\n",
    "        sft_dataset[get_split(task.id)].append(datapoint)\n",
    "    print(f\"Number trajectories: {len(sft_dataset['train'])} (train) and {len(sft_dataset['test'])} (test)\")\n",
    "    sft_dataset = DatasetDict({\"train\": Dataset.from_list(sft_dataset[\"train\"]), \"test\": Dataset.from_list(sft_dataset[\"test\"])})\n",
    "    return sft_dataset\n",
    "\n",
    "def make_sft_from_simulation(task, get_system_prompt, simulation, openai_tools):\n",
    "    system_prompt = get_system_prompt(task)\n",
    "    assert simulation.task_id == task.id\n",
    "    system_message = SystemMessage(role=\"system\", content=system_prompt)\n",
    "    messages = to_litellm_messages([system_message] + prepare_messages(simulation.messages[:]))\n",
    "    datapoint = {\"messages\": messages, \"tools\": openai_tools, \"task_id\": task.id, \"simulation_id\": simulation.id, \"turn_idx\": i}\n",
    "    return datapoint\n",
    "\n",
    "def prepare_messages(messages):\n",
    "    return [msg for msg in messages if is_valid_agent_history_message(msg)]\n",
    "\n",
    "sft_dataset = make_sft_dataset(res_path)\n",
    "# Save to disk in Hugging Face format\n",
    "save_path = \"./data/test_sft_dataset\"\n",
    "sft_dataset.save_to_disk(save_path)\n",
    "\n",
    "\n",
    "\n",
    "# sft_dataset = []\n",
    "# for x, y in trajs:\n",
    "#     x = tokenizer.apply_chat_template(x, tools=openai_tools, tokenize=False)\n",
    "#     y = tokenizer.apply_chat_template([y], tokenize=False, add_generation_prompt=False)\n",
    "#     sft_dataset.append((x, y))\n",
    "\n",
    "# print(f\"Number trajectories: {len(sft_dataset)}\")\n",
    "\n",
    "# prompt, response = sft_dataset[0]\n",
    "# print(f\"Prompt:\\n{prompt}\")\n",
    "# print(f\"Response:\\n{response}\")\n",
    "\n",
    "# # prompt = tokenizer.apply_chat_template(messages, tools=openai_tools, tokenize=False)\n",
    "# # print(f\"Prompt:\\n{prompt}\")\n",
    "\n",
    "# GPU_BACKEND = \"mps\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# def generate_response(messages, tools, tokenizer, model, max_new_tokens=1024, use_gpu=True):\n",
    "#     if use_gpu:\n",
    "#         model.to(GPU_BACKEND)\n",
    "#     else:\n",
    "#         model.to(\"cpu\")\n",
    "#     prompt = tokenizer.apply_chat_template(messages, tools=tools, tokenize=False)\n",
    "#     print(f\"Prompt:\\n{prompt}\")\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(GPU_BACKEND)\n",
    "#     # inputs.pop('token_type_ids', None)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(**inputs, \n",
    "#                                   max_new_tokens=max_new_tokens,\n",
    "#                                   do_sample=False,\n",
    "#                                   temperature=0.0,\n",
    "#                                   pad_token_id=tokenizer.eos_token_id)\n",
    "#     input_length = inputs.input_ids.shape[1]\n",
    "#     geneated_token_ids = outputs[0][input_length+1:]\n",
    "#     response = tokenizer.decode(geneated_token_ids, skip_special_tokens=True)\n",
    "#     return response\n",
    "\n",
    "\n",
    "# generate_response([{\"role\": \"user\", \"content\": \"Hello. How are you doing?\"}], [], tokenizer, model, max_new_tokens=100, use_gpu=True)\n",
    "\n",
    "\n",
    "\n",
    "# print(environment.get_tools())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c51d7ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['messages', 'tools', 'task_id', 'simulation_id', 'turn_idx'],\n",
      "        num_rows: 74\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['messages', 'tools', 'task_id', 'simulation_id', 'turn_idx'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "sft_dataset = load_from_disk(\"./data/test_sft_dataset\")\n",
    "print(sft_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, EarlyStoppingCallback\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B\"\n",
    "\n",
    "USE_PEFT = True\n",
    "if USE_PEFT:\n",
    "    learning_rate = 1e-4 # Higher learning rate for PEFT?\n",
    "else:\n",
    "    learning_rate = 8e-5\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    assistant_only_loss=True,                # Only compute the loss on the assistant messages\n",
    "    report_to=\"none\",                        # disable logging to W&B\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=learning_rate,                      # Learning rate for training. \n",
    "    num_train_epochs=20,                     #  Set the number of epochs to train the model.\n",
    "    per_device_train_batch_size=2,           # Batch size for each device (e.g., GPU) during training. \n",
    "    gradient_accumulation_steps=8,           # Number of steps before performing a backward/update pass to accumulate gradients.\n",
    "    gradient_checkpointing=True,             # Enable gradient checkpointing to reduce memory usage during training at the cost of slower training speed.\n",
    "    logging_steps=2,                         # Frequency of logging training progress (log every 2 steps).\n",
    "    eval_strategy=\"epoch\",                   # evaluate at end of each epoch\n",
    "    save_strategy=\"epoch\",                   # save checkpoint at end of each epoch\n",
    "    save_total_limit=1,                      # keep only the best/latest model\n",
    "    load_best_model_at_end=True,             # load best model according to eval loss\n",
    "    metric_for_best_model=\"eval_loss\",       # use eval loss for best model selection\n",
    "    greater_is_better=False,                 # lower eval_loss is better\n",
    "    output_dir=\"./SFTcheckpoints\"               # directory to save checkpoints\n",
    ")\n",
    "\n",
    "\n",
    "# Instantiate early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2  # Stop if no improvement for 2 evals (epochs)\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "if USE_PEFT: # FIXME: Check what's the right config.\n",
    "    # lora_config = LoraConfig(\n",
    "    #     r=64,\n",
    "    #     lora_alpha=16,\n",
    "    #     target_modules=[\"c_attn\", \"q_proj\", \"v_proj\"],  # adjust to Qwen architecture\n",
    "    #     lora_dropout=0.05,\n",
    "    #     bias=\"none\",\n",
    "    #     task_type=TaskType.CAUSAL_LM,\n",
    "    # )\n",
    "    lora_config = LoraConfig()\n",
    "else:\n",
    "    lora_config = None\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=sft_dataset[\"train\"],\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[early_stopping_callback],\n",
    "    peft_config=lora_config \n",
    "    \n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
