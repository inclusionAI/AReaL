{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37c1b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install datasets\n",
    "# ! pip install transformers\n",
    "# ! pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa9f26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/tau2/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816ddd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-04 09:26:50.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtau2.utils.utils\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mUsing data directory from source: /lambda/nfs/victor-north-tx/tau2-bench-private/data\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-04 09:26:51.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtau2.utils.llm_utils\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mLiteLLM: Cache is disabled\u001b[0m\n",
      "\u001b[32m2025-08-04 09:26:51.696\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtau2.utils.llm_utils\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m73\u001b[0m - \u001b[33m\u001b[1mSonnet thinking is disabled\u001b[0m\n",
      "\u001b[32m2025-08-04 09:26:51.775\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtau2.registry\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m194\u001b[0m - \u001b[34m\u001b[1mRegistering default components...\u001b[0m\n",
      "\u001b[32m2025-08-04 09:26:51.776\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtau2.registry\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m236\u001b[0m - \u001b[34m\u001b[1mDefault components registered successfully. Registry info: {\n",
      "  \"domains\": [\n",
      "    \"mock\",\n",
      "    \"airline\",\n",
      "    \"retail\",\n",
      "    \"telecom\",\n",
      "    \"telecom-workflow\"\n",
      "  ],\n",
      "  \"agents\": [\n",
      "    \"llm_agent\",\n",
      "    \"llm_agent_gt\",\n",
      "    \"llm_agent_solo\",\n",
      "    \"llm_agent_completion\",\n",
      "    \"llm_agent_gt_completion\",\n",
      "    \"llm_agent_solo_completion\"\n",
      "  ],\n",
      "  \"users\": [\n",
      "    \"user_simulator\",\n",
      "    \"dummy_user\"\n",
      "  ],\n",
      "  \"task_sets\": [\n",
      "    \"mock\",\n",
      "    \"airline\",\n",
      "    \"retail\",\n",
      "    \"telecom\",\n",
      "    \"telecom-workflow\"\n",
      "  ]\n",
      "}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1107\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from experiments.model_training.prepare_dataset import OpenAICompletionDataPoint\n",
    "dataset_path = \"data/test_full-v1.jsonl\"\n",
    "dataset = []\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        datapoint = OpenAICompletionDataPoint.model_validate_json(line)\n",
    "        dataset.append(datapoint)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13164c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'experiments.model_training.prepare_dataset.OpenAICompletionDataPoint'>\n",
      "True\n",
      "18\n",
      "14\n",
      "1 assistant: Hi! How can I help you today?\n",
      "2 user: Hi, I need to change my upcoming flight from JFK. My cat is really sick and I need to get home sooner. Can you help me look into changing my flight to an earlier nonstop option?\n",
      "3 assistant: To assist you with changing your flight, I need to know your user ID and the reservation ID for the flight you want to change. Could you please provide those? If you don't know your reservation ID, I can help you locate it.\n",
      "4 user: I don‚Äôt have my reservation ID handy, but my user ID is daiki_lee_6144. Can you look up my reservation with that?\n",
      "5 assistant: tool call...\n",
      "7 assistant: You have three reservations under your user ID daiki_lee_6144: DF89BM, COVE6R, and IIHXDG.\n",
      "\n",
      "Could you please specify which reservation you want to change? If you are not sure, you can provide more details about the flight such as the destination or date, and I can help identify the correct reservation.\n",
      "8 user: I‚Äôm not sure which one it is, but the flight I want to change is departing from JFK on May 17. Can you check which reservation matches that?\n",
      "9 assistant: tool call...\n",
      "13 assistant: The reservation DF89BM has a flight departing from JFK on May 17. It is a round trip with basic economy cabin. The flights are JFK to ORD on May 17, ORD to DTW on May 17, and DTW to JFK on May 24.\n",
      "\n",
      "Would you like to change this reservation to an earlier nonstop flight from JFK? If yes, please confirm and provide the preferred date for the earlier flight. Note that basic economy flights cannot be modified to change flights, only cabin changes are allowed. Would you like me to check for available nonstop flights from JFK on your preferred date?\n",
      "14 user: Yes, that‚Äôs the reservation I want to change. I‚Äôd like to switch to an earlier nonstop flight from JFK to my destination on May 17. Can you check what nonstop options are available that day? Also, I only want to make the change if it costs less than $100. I don‚Äôt want to buy a new ticket‚Äîjust change my existing one if possible.\n",
      "15 assistant: Since your current reservation DF89BM is booked in basic economy cabin, changing flights is not allowed under the policy. Only cabin changes are allowed for basic economy, but not changing the flights themselves.\n",
      "\n",
      "Would you like me to check for available nonstop flights from JFK to your destination on May 17 anyway, or assist you with any other request?\n",
      "16 user: If I can‚Äôt change to an earlier nonstop flight without buying a new ticket, then I don‚Äôt want to proceed. I‚Äôm not interested in upgrading the cabin or purchasing a new flight. Thanks for checking for me.\n",
      "17 assistant: You're welcome. If you need any further assistance in the future, feel free to reach out. Have a good day!\n"
     ]
    }
   ],
   "source": [
    "dp0: OpenAICompletionDataPoint = dataset[0]\n",
    "print(type(dp0))\n",
    "print(dp0.parallel_tool_calls)\n",
    "print(len(dp0.messages))\n",
    "print(len(dp0.tools))\n",
    "for i, message in enumerate(dp0.messages):\n",
    "    if message[\"role\"] in [\"system\", \"tool\"]:\n",
    "        continue\n",
    "    print(f\"{i} {message['role']}: {message.get('content', 'tool call...')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f9c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model for /home/ubuntu/victor-north-tx/models/Qwen2.5-0.5B-instruct\n",
      "Tokenizer has a chat template.\n",
      "Tokenizer eos_token_id: 151645, pad_token_id: 151643\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "\n",
    "# Model checkpoint\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "\n",
    "## NOTE: Use instruct models! Non instruct models require better generation stopping strategies.\n",
    "# MODEL_NAME = \"/home/ubuntu/victor-north-tx/models/Qwen2.5-0.5B\"\n",
    "# MODEL_NAME = \"/home/ubuntu/victor-north-tx/models/Qwen2.5-3B\"\n",
    "# MODEL_NAME = \"/home/ubuntu/victor-north-tx/models/Qwen2.5-7B\"\n",
    "MODEL_NAME = \"/home/ubuntu/victor-north-tx/models/Qwen2.5-0.5B-instruct\"\n",
    "# MODEL_NAME = \"/home/ubuntu/victor-north-tx/models/Qwen2.5-3B-instruct\"\n",
    "# MODEL_NAME = \"/home/ubuntu/victor-north-tx/models/Qwen2.5-7B-instruct\"\n",
    "\n",
    "\n",
    "\n",
    "def get_model_and_tokenizer(model_name, torch_dtype=\"auto\", device_map=\"auto\"):\n",
    "    try:\n",
    "        global model, tokenizer # Just to be sure that we don't have any old model or tokenizer\n",
    "        del model, tokenizer\n",
    "    except:\n",
    "        pass\n",
    "    # Load tokenizer and model (4-bit optional)\n",
    "    print(f\"Loading tokenizer and model for {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.chat_template is None:\n",
    "        raise ValueError(f\"Tokenizer for model {model_name} does not have a chat template.\")\n",
    "    else:\n",
    "        print(\"Tokenizer has a chat template.\")\n",
    "    print(f\"Tokenizer eos_token_id: {tokenizer.eos_token_id}, pad_token_id: {tokenizer.pad_token_id}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch_dtype, # Automatically determines the best dtype (e.g., float16, bfloat16)\n",
    "        device_map=device_map   # Automatically distributes the model across available devices (e.g., GPUs)\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "model, tokenizer = get_model_and_tokenizer(MODEL_NAME, torch_dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4a8b1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of France?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "****************************************************************************************************\n",
      "Full output:\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of France?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The capital of France is Paris.<|im_end|>\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Full generated output:\n",
      "The capital of France is Paris.<|im_end|>\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Thinking content: \n",
      "Content: The capital of France is Paris.\n",
      "****************************************************************************************************\n",
      "Testing parse_tool_calls_qwen\n",
      "[{'name': 'get_capital', 'arguments': {'country': 'France'}}]\n"
     ]
    }
   ],
   "source": [
    "# Load gemnerate response function\n",
    "# GPU_BACKEND = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# GPU_BACKEND = \"mps\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def generate_response_qwen(messages, tools, tokenizer, model, temperature=0.5, add_generation_prompt=True, enable_thinking=False, max_new_tokens=32768, verbose=False, gpu_backend=None):\n",
    "    \"\"\"\n",
    "    Generate completion response for a given prompt.\n",
    "    Args:\n",
    "        messages: List of messages in the chat format.\n",
    "        tools: List of tools to use for the completion.\n",
    "        tokenizer: Tokenizer to use for the completion.\n",
    "        model: Model to use for the completion.\n",
    "        max_new_tokens: Maximum number of new tokens to generate.\n",
    "    \"\"\"\n",
    "    prompt = tokenizer.apply_chat_template(messages, tools=tools, tokenize=False, add_generation_prompt=add_generation_prompt, enable_thinking=enable_thinking)\n",
    "    if verbose:\n",
    "        print(f\"Prompt:\\n{prompt}\")\n",
    "    if gpu_backend is not None:\n",
    "        print(\"NOTE: If model was loaded using auto device map, you shouldn't specify gpu_backend\")\n",
    "    if gpu_backend is not None:\n",
    "        model.to(gpu_backend)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    # inputs.pop('token_type_ids', None)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, \n",
    "                                  max_new_tokens=max_new_tokens,\n",
    "                                  temperature=temperature\n",
    "                                  )\n",
    "        if verbose:\n",
    "            print(\"*\"*100)\n",
    "            print(f\"Full output:\\n{tokenizer.decode(outputs[0], skip_special_tokens=False)}\")\n",
    "            print(\"*\"*100)\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    generated_token_ids = outputs[0][input_length:]\n",
    "    if verbose:\n",
    "        print(\"*\"*100)\n",
    "        print(f\"Full generated output:\\n{tokenizer.decode(generated_token_ids, skip_special_tokens=False)}\")\n",
    "        print(\"*\"*100)\n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(generated_token_ids) - generated_token_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "\n",
    "    thinking_content = tokenizer.decode(generated_token_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(generated_token_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    if verbose:\n",
    "        print(\"*\"*100)\n",
    "        print(f\"Thinking content: {thinking_content}\")\n",
    "        print(f\"Content: {content}\")\n",
    "        print(\"*\"*100)\n",
    "    return content, thinking_content\n",
    "\n",
    "\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "test_tools = None\n",
    "test_response, test_thinking_content = generate_response_qwen(test_messages, test_tools, tokenizer, model, add_generation_prompt=True, enable_thinking=False, verbose=True)\n",
    "# print(\"thinking content:\", test_thinking_content)\n",
    "# print(\"content:\", test_response)\n",
    "\n",
    "# Parse tool calls\n",
    "import json\n",
    "import re\n",
    "\n",
    "def parse_tool_calls_qwen(response: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Parse tool calls from a response.\n",
    "    System instructions in Qwen2.5:\n",
    "    ```\n",
    "    For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
    "    <tool_call>\n",
    "    {\"name\": <function-name>, \"arguments\": <args-json-object>}\n",
    "    </tool_call>\n",
    "    ```\n",
    "    \"\"\"\n",
    "    pattern = r\"<tool_call>(.*?)</tool_call>\"\n",
    "    matches = re.findall(pattern, response, re.DOTALL)\n",
    "    tool_calls = []\n",
    "    \n",
    "    for match in matches:\n",
    "        # Process each line of the tool call content\n",
    "        lines = match.strip().split('\\n')\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                try:\n",
    "                    tool_call = json.loads(line)\n",
    "                    tool_calls.append(tool_call)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # Skip lines that aren't valid JSON\n",
    "    \n",
    "    return tool_calls\n",
    "\n",
    "print(\"Testing parse_tool_calls_qwen\")\n",
    "test_response = \"I'm thinking about the capital of France. <tool_call>{\\\"name\\\": \\\"get_capital\\\", \\\"arguments\\\": {\\\"country\\\": \\\"France\\\"}}</tool_call>\"\n",
    "tool_calls = parse_tool_calls_qwen(test_response)\n",
    "print(tool_calls)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19ff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üéØ PARTIAL TRAJECTORY 0 (2 messages).\n",
      "Generation time: 0.50 seconds\n",
      "================================================================================\n",
      "\n",
      "üìù ORIGINAL RESPONSE:\n",
      "----------------------------------------\n",
      "Hi! How can I help you today?\n",
      "\n",
      "üõ†Ô∏è ORIGINAL TOOL CALLS: None\n",
      "\n",
      "üîÑ REPLAY RESPONSE:\n",
      "----------------------------------------\n",
      "Sure, let's start by sending a message to the user to confirm our interaction:\n",
      "\n",
      "**Message:** \"Hello! How can I assist you today?\"\n",
      "\n",
      "üõ†Ô∏è REPLAY TOOL CALLS: None\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üéØ PARTIAL TRAJECTORY 1 (4 messages).\n",
      "Generation time: 0.73 seconds\n",
      "================================================================================\n",
      "\n",
      "üìù ORIGINAL RESPONSE:\n",
      "----------------------------------------\n",
      "To assist you with changing your flight, I need to know your user ID and the reservation ID for the flight you want to change. Could you please provide those? If you don't know your reservation ID, I can help you locate it.\n",
      "\n",
      "üõ†Ô∏è ORIGINAL TOOL CALLS: None\n",
      "\n",
      "üîÑ REPLAY RESPONSE:\n",
      "----------------------------------------\n",
      "<tool_call>\n",
      "{\"name\": \"search_onestop_flight\", \"arguments\": {\"origin\": \"JFK\", \"destination\": \"LAX\", \"date\": \"2024-05-01\"}}\n",
      "</tool_call>\n",
      "\n",
      "üõ†Ô∏è REPLAY TOOL CALLS:\n",
      "----------------------------------------\n",
      "  1. search_onestop_flight: {'origin': 'JFK', 'destination': 'LAX', 'date': '2024-05-01'}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üéØ PARTIAL TRAJECTORY 2 (6 messages).\n",
      "Generation time: 0.37 seconds\n",
      "================================================================================\n",
      "\n",
      "üìù ORIGINAL RESPONSE:\n",
      "----------------------------------------\n",
      "No content\n",
      "\n",
      "üõ†Ô∏è ORIGINAL TOOL CALLS:\n",
      "----------------------------------------\n",
      "  1. get_user_details: {\"user_id\": \"daiki_lee_6144\"}\n",
      "\n",
      "üîÑ REPLAY RESPONSE:\n",
      "----------------------------------------\n",
      "Sure thing! Let's start by looking up your reservation. Please provide your user ID so we can proceed.\n",
      "\n",
      "üõ†Ô∏è REPLAY TOOL CALLS: None\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üéØ PARTIAL TRAJECTORY 3 (8 messages).\n",
      "Generation time: 0.75 seconds\n",
      "================================================================================\n",
      "\n",
      "üìù ORIGINAL RESPONSE:\n",
      "----------------------------------------\n",
      "You have three reservations under your user ID daiki_lee_6144: DF89BM, COVE6R, and IIHXDG.\n",
      "\n",
      "Could you please specify which reservation you want to change? If you are not sure, you can provide more details about the flight such as the destination or date, and I can help identify the correct reservation.\n",
      "\n",
      "üõ†Ô∏è ORIGINAL TOOL CALLS: None\n",
      "\n",
      "üîÑ REPLAY RESPONSE:\n",
      "----------------------------------------\n",
      "Sure, let's start by looking up your reservation. Please provide the flight number and the date of the flight you're interested in. If you don't have the exact flight number, I'll try to find it for you.\n",
      "\n",
      "üõ†Ô∏è REPLAY TOOL CALLS: None\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üéØ PARTIAL TRAJECTORY 4 (10 messages).\n",
      "Generation time: 0.70 seconds\n",
      "================================================================================\n",
      "\n",
      "üìù ORIGINAL RESPONSE:\n",
      "----------------------------------------\n",
      "No content\n",
      "\n",
      "üõ†Ô∏è ORIGINAL TOOL CALLS:\n",
      "----------------------------------------\n",
      "  1. get_reservation_details: {\"reservation_id\": \"DF89BM\"}\n",
      "  2. get_reservation_details: {\"reservation_id\": \"COVE6R\"}\n",
      "  3. get_reservation_details: {\"reservation_id\": \"IIHXDG\"}\n",
      "\n",
      "üîÑ REPLAY RESPONSE:\n",
      "----------------------------------------\n",
      "Sure thing! Based on the information you provided, the reservation ID for the flight departing from JFK on May 17 is COVE6R. Let's proceed with changing this reservation to a nonstop option.\n",
      "\n",
      "üõ†Ô∏è REPLAY TOOL CALLS: None\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üéØ PARTIAL TRAJECTORY 5 (14 messages).\n",
      "Generation time: 1.89 seconds\n",
      "================================================================================\n",
      "\n",
      "üìù ORIGINAL RESPONSE:\n",
      "----------------------------------------\n",
      "The reservation DF89BM has a flight departing from JFK on May 17. It is a round trip with basic economy cabin. The flights are JFK to ORD on May 17, ORD to DTW on May 17, and DTW to JFK on May 24.\n",
      "\n",
      "Would you like to change this reservation to an earlier nonstop flight from JFK? If yes, please confirm and provide the preferred date for the earlier flight. Note that basic economy flights cannot be modified to change flights, only cabin changes are allowed. Would you like me to check for available nonstop flights from JFK on your preferred date?\n",
      "\n",
      "üõ†Ô∏è ORIGINAL TOOL CALLS: None\n",
      "\n",
      "üîÑ REPLAY RESPONSE:\n",
      "----------------------------------------\n",
      "Based on the flight details you've provided:\n",
      "\n",
      "- **DF89BM**: Departing from JFK, arriving at DTW on May 17th.\n",
      "- **COVE6R**: Departing from MCO, arriving at BOS on May 27th.\n",
      "- **IIHXDG**: Departing from PHL, arriving at SFO on May 18th.\n",
      "\n",
      "Since you're interested in changing your flight from JFK to DTW, let's focus on the flight from JFK to DTW.\n",
      "\n",
      "Would you like to proceed with changing the flight from JFK to DTW?\n",
      "\n",
      "üõ†Ô∏è REPLAY TOOL CALLS: None\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üéØ PARTIAL TRAJECTORY 6 (16 messages).\n",
      "Generation time: 1.82 seconds\n",
      "================================================================================\n",
      "\n",
      "üìù ORIGINAL RESPONSE:\n",
      "----------------------------------------\n",
      "Since your current reservation DF89BM is booked in basic economy cabin, changing flights is not allowed under the policy. Only cabin changes are allowed for basic economy, but not changing the flights themselves.\n",
      "\n",
      "Would you like me to check for available nonstop flights from JFK to your destination on May 17 anyway, or assist you with any other request?\n",
      "\n",
      "üõ†Ô∏è ORIGINAL TOOL CALLS: None\n",
      "\n",
      "üîÑ REPLAY RESPONSE:\n",
      "----------------------------------------\n",
      "<tool_call>\n",
      "{\"name\": \"search_onestop_flight\", \"arguments\": {\"origin\": \"JFK\", \"destination\": \"DTW\", \"date\": \"2024-05-17\"}}\n",
      "</tool_call>\n",
      "<tool_call>\n",
      "{\"name\": \"cancel_reservation\", \"arguments\": {\"reservation_id\": \"DF89BM\"}}\n",
      "</tool_call>\n",
      "<tool_call>\n",
      "{\"name\": \"cancel_reservation\", \"arguments\": {\"reservation_id\": \"COVE6R\"}}\n",
      "</tool_call>\n",
      "<tool_call>\n",
      "{\"name\": \"cancel_reservation\", \"arguments\": {\"reservation_id\": \"IIHXDG\"}}\n",
      "</tool_call>\n",
      "\n",
      "üõ†Ô∏è REPLAY TOOL CALLS:\n",
      "----------------------------------------\n",
      "  1. search_onestop_flight: {'origin': 'JFK', 'destination': 'DTW', 'date': '2024-05-17'}\n",
      "  2. cancel_reservation: {'reservation_id': 'DF89BM'}\n",
      "  3. cancel_reservation: {'reservation_id': 'COVE6R'}\n",
      "  4. cancel_reservation: {'reservation_id': 'IIHXDG'}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üéØ PARTIAL TRAJECTORY 7 (18 messages).\n",
      "Generation time: 1.07 seconds\n",
      "================================================================================\n",
      "\n",
      "üìù ORIGINAL RESPONSE:\n",
      "----------------------------------------\n",
      "You're welcome. If you need any further assistance in the future, feel free to reach out. Have a good day!\n",
      "\n",
      "üõ†Ô∏è ORIGINAL TOOL CALLS: None\n",
      "\n",
      "üîÑ REPLAY RESPONSE:\n",
      "----------------------------------------\n",
      "Sure thing! Let's move forward with the nonstop flight from JFK to your destination on May 17. Since there isn't a direct nonstop flight from JFK to your destination, we'll need to find another option.\n",
      "\n",
      "Would you like to try searching for a one-way flight instead? Or do you prefer a different destination?\n",
      "\n",
      "üõ†Ô∏è REPLAY TOOL CALLS: None\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "def create_partial_trajectories(datapoint: OpenAICompletionDataPoint) -> list[OpenAICompletionDataPoint]:\n",
    "    messages = datapoint.messages\n",
    "    tools = datapoint.tools\n",
    "    partial_trajectories = []\n",
    "    for i, message in enumerate(messages):\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "\n",
    "            partial_trajectories.append(OpenAICompletionDataPoint(messages=messages[:i+1], tools=tools, parallel_tool_calls=datapoint.parallel_tool_calls))\n",
    "    return partial_trajectories\n",
    "\n",
    "partial_trajectories = create_partial_trajectories(dataset[0])\n",
    "\n",
    "def replay(datapoint, max_new_tokens=32768, enable_thinking=False, verbose=False, add_generation_prompt=True):\n",
    "    partial_trajectories = create_partial_trajectories(datapoint)\n",
    "    for i, partial_trajectory in enumerate(partial_trajectories):\n",
    "        original_response = partial_trajectory.messages[-1]\n",
    "        convo_history = partial_trajectory.messages[:-1]\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        replay_response, thinking_content = generate_response_qwen(convo_history, partial_trajectory.tools, tokenizer, model, add_generation_prompt=add_generation_prompt, max_new_tokens=max_new_tokens, enable_thinking=enable_thinking, verbose=verbose)\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üéØ PARTIAL TRAJECTORY {i} ({len(partial_trajectory.messages)} messages).\\nGeneration time: {time.perf_counter() - start_time:.2f} seconds\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(\"\\nüìù ORIGINAL RESPONSE:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(original_response.get('content', \"No content\"))\n",
    "        \n",
    "        # Show original tool calls\n",
    "        if original_response.get('tool_calls'):\n",
    "            print(\"\\nüõ†Ô∏è ORIGINAL TOOL CALLS:\")\n",
    "            print(\"-\" * 40)\n",
    "            for j, tool_call in enumerate(original_response.get('tool_calls', [])):\n",
    "                print(f\"  {j+1}. {tool_call['function']['name']}: {tool_call['function']['arguments']}\")\n",
    "        else:\n",
    "            print(\"\\nüõ†Ô∏è ORIGINAL TOOL CALLS: None\")\n",
    "        \n",
    "        if thinking_content:\n",
    "            print(\"\\nüí≠ THINKING CONTENT:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(thinking_content)\n",
    "        \n",
    "        print(\"\\nüîÑ REPLAY RESPONSE:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(replay_response)\n",
    "        \n",
    "        tool_calls = parse_tool_calls_qwen(replay_response)\n",
    "        if tool_calls:\n",
    "            print(\"\\nüõ†Ô∏è REPLAY TOOL CALLS:\")\n",
    "            print(\"-\" * 40)\n",
    "            for j, tool_call in enumerate(tool_calls):\n",
    "                print(f\"  {j+1}. {tool_call['name']}: {tool_call['arguments']}\")\n",
    "        else:\n",
    "            print(\"\\nüõ†Ô∏è REPLAY TOOL CALLS: None\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "\n",
    "replay(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, EarlyStoppingCallback\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B\"\n",
    "\n",
    "USE_PEFT = True\n",
    "if USE_PEFT:\n",
    "    learning_rate = 1e-4 # Higher learning rate for PEFT?\n",
    "else:\n",
    "    learning_rate = 8e-5\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    assistant_only_loss=True,                # Only compute the loss on the assistant messages\n",
    "    report_to=\"none\",                        # disable logging to W&B\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=learning_rate,                      # Learning rate for training. \n",
    "    num_train_epochs=20,                     #  Set the number of epochs to train the model.\n",
    "    per_device_train_batch_size=2,           # Batch size for each device (e.g., GPU) during training. \n",
    "    gradient_accumulation_steps=8,           # Number of steps before performing a backward/update pass to accumulate gradients.\n",
    "    gradient_checkpointing=True,             # Enable gradient checkpointing to reduce memory usage during training at the cost of slower training speed.\n",
    "    logging_steps=2,                         # Frequency of logging training progress (log every 2 steps).\n",
    "    eval_strategy=\"epoch\",                   # evaluate at end of each epoch\n",
    "    save_strategy=\"epoch\",                   # save checkpoint at end of each epoch\n",
    "    save_total_limit=1,                      # keep only the best/latest model\n",
    "    load_best_model_at_end=True,             # load best model according to eval loss\n",
    "    metric_for_best_model=\"eval_loss\",       # use eval loss for best model selection\n",
    "    greater_is_better=False,                 # lower eval_loss is better\n",
    "    output_dir=\"./SFTcheckpoints\"               # directory to save checkpoints\n",
    ")\n",
    "\n",
    "\n",
    "# Instantiate early stopping callback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2  # Stop if no improvement for 2 evals (epochs)\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "if USE_PEFT: # FIXME: Check what's the right config.\n",
    "    # lora_config = LoraConfig(\n",
    "    #     r=64,\n",
    "    #     lora_alpha=16,\n",
    "    #     target_modules=[\"c_attn\", \"q_proj\", \"v_proj\"],  # adjust to Qwen architecture\n",
    "    #     lora_dropout=0.05,\n",
    "    #     bias=\"none\",\n",
    "    #     task_type=TaskType.CAUSAL_LM,\n",
    "    # )\n",
    "    lora_config = LoraConfig()\n",
    "else:\n",
    "    lora_config = None\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=sft_dataset[\"train\"],\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[early_stopping_callback],\n",
    "    peft_config=lora_config \n",
    "    \n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tau2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
