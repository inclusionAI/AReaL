{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50d1f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/tau2/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2025-08-05 11:55:35.502\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtau2.utils.utils\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mUsing data directory from source: /lambda/nfs/victor-north-tx/tau2-bench-private/data\u001b[0m\n",
      "\u001b[32m2025-08-05 11:55:36.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtau2.utils.llm_utils\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mVLLM_API_BASE: http://127.0.0.1:8000/v1\u001b[0m\n",
      "\u001b[32m2025-08-05 11:55:36.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtau2.utils.llm_utils\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mOLLAMA_API_BASE: http://127.0.0.1:11434\u001b[0m\n",
      "\u001b[32m2025-08-05 11:55:36.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtau2.utils.llm_utils\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mLiteLLM: Cache is disabled\u001b[0m\n",
      "\u001b[32m2025-08-05 11:55:36.723\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtau2.utils.llm_utils\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m85\u001b[0m - \u001b[33m\u001b[1mSonnet thinking is disabled\u001b[0m\n",
      "\u001b[32m2025-08-05 11:55:36.797\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtau2.registry\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m194\u001b[0m - \u001b[34m\u001b[1mRegistering default components...\u001b[0m\n",
      "\u001b[32m2025-08-05 11:55:36.798\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtau2.registry\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m236\u001b[0m - \u001b[34m\u001b[1mDefault components registered successfully. Registry info: {\n",
      "  \"domains\": [\n",
      "    \"mock\",\n",
      "    \"airline\",\n",
      "    \"retail\",\n",
      "    \"telecom\",\n",
      "    \"telecom-workflow\"\n",
      "  ],\n",
      "  \"agents\": [\n",
      "    \"llm_agent\",\n",
      "    \"llm_agent_gt\",\n",
      "    \"llm_agent_solo\",\n",
      "    \"llm_agent_completion\",\n",
      "    \"llm_agent_gt_completion\",\n",
      "    \"llm_agent_solo_completion\"\n",
      "  ],\n",
      "  \"users\": [\n",
      "    \"user_simulator\",\n",
      "    \"dummy_user\"\n",
      "  ],\n",
      "  \"task_sets\": [\n",
      "    \"mock\",\n",
      "    \"airline\",\n",
      "    \"retail\",\n",
      "    \"telecom\",\n",
      "    \"telecom-workflow\"\n",
      "  ]\n",
      "}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from experiments.model_training.train_qwen import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753721a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 1722\n",
      "Test dataset length: 1107\n",
      "Loading tokenizer and model for /home/ubuntu/victor-north-tx/models/Qwen2.5-3B-instruct\n",
      "Tokenizer has a chat template.\n",
      "Tokenizer eos_token_id: 151645, pad_token_id: 151643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 12.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset: 100%|██████████| 1722/1722 [00:35<00:00, 49.00 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1722/1722 [00:00<00:00, 7020.18 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 1107/1107 [00:22<00:00, 48.90 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 1107/1107 [00:00<00:00, 9212.11 examples/s] \n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='371' max='2160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 371/2160 11:33 < 56:03, 0.53 it/s, Epoch 3.43/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.001359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_name = \"Qwen2.5-3B-instruct\"\n",
    "qwen_model = f\"/home/ubuntu/victor-north-tx/models/{model_name}\"\n",
    "train_dataset_path = \"data/train_full-v1.jsonl\"\n",
    "test_dataset_path = \"data/test_full-v1.jsonl\"\n",
    "trained_model_name = f\"{model_name}-sft-full-tau2-v1\"\n",
    "output_dir = f\"./data/{trained_model_name}\"\n",
    "report_to_wandb = True\n",
    "run_name = f\"{trained_model_name}\"\n",
    "if Path(output_dir).exists():\n",
    "    raise ValueError(f\"Output directory {output_dir} already exists. Please delete it or choose a different output directory.\")\n",
    "\n",
    "max_train_datapoints = None\n",
    "max_test_datapoints = None\n",
    "use_peft = False\n",
    "patience = 2\n",
    "\n",
    "train(\n",
    "    qwen_model=qwen_model, \n",
    "    train_dataset_path=train_dataset_path, \n",
    "    test_dataset_path=test_dataset_path, \n",
    "    output_dir=output_dir, \n",
    "    max_train_datapoints=max_train_datapoints, \n",
    "    max_test_datapoints=max_test_datapoints, \n",
    "    use_peft=use_peft,\n",
    "    patience=patience,\n",
    "    report_to_wandb=report_to_wandb,\n",
    "    run_name=run_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327a8a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/tau2/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2025-08-05 13:43:48.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtau2.utils.utils\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mUsing data directory from source: /lambda/nfs/victor-north-tx/tau2-bench-private/data\u001b[0m\n",
      "\u001b[32m2025-08-05 13:43:49.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtau2.utils.llm_utils\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mVLLM_API_BASE: http://127.0.0.1:8000/v1\u001b[0m\n",
      "\u001b[32m2025-08-05 13:43:49.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtau2.utils.llm_utils\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mOLLAMA_API_BASE: http://127.0.0.1:11434\u001b[0m\n",
      "\u001b[32m2025-08-05 13:43:49.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtau2.utils.llm_utils\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mLiteLLM: Cache is disabled\u001b[0m\n",
      "\u001b[32m2025-08-05 13:43:49.530\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtau2.utils.llm_utils\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m85\u001b[0m - \u001b[33m\u001b[1mSonnet thinking is disabled\u001b[0m\n",
      "\u001b[32m2025-08-05 13:43:49.603\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtau2.registry\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m194\u001b[0m - \u001b[34m\u001b[1mRegistering default components...\u001b[0m\n",
      "\u001b[32m2025-08-05 13:43:49.604\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtau2.registry\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m236\u001b[0m - \u001b[34m\u001b[1mDefault components registered successfully. Registry info: {\n",
      "  \"domains\": [\n",
      "    \"mock\",\n",
      "    \"airline\",\n",
      "    \"retail\",\n",
      "    \"telecom\",\n",
      "    \"telecom-workflow\"\n",
      "  ],\n",
      "  \"agents\": [\n",
      "    \"llm_agent\",\n",
      "    \"llm_agent_gt\",\n",
      "    \"llm_agent_solo\",\n",
      "    \"llm_agent_completion\",\n",
      "    \"llm_agent_gt_completion\",\n",
      "    \"llm_agent_solo_completion\"\n",
      "  ],\n",
      "  \"users\": [\n",
      "    \"user_simulator\",\n",
      "    \"dummy_user\"\n",
      "  ],\n",
      "  \"task_sets\": [\n",
      "    \"mock\",\n",
      "    \"airline\",\n",
      "    \"retail\",\n",
      "    \"telecom\",\n",
      "    \"telecom-workflow\"\n",
      "  ]\n",
      "}\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexperiments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_training\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain_qwen\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model_and_tokenizer, load_dataset\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model, tokenizer = get_model_and_tokenizer(model_name=\u001b[43moutput_dir\u001b[49m)\n\u001b[32m      3\u001b[39m model\n",
      "\u001b[31mNameError\u001b[39m: name 'output_dir' is not defined"
     ]
    }
   ],
   "source": [
    "from experiments.model_training.train_qwen import get_model_and_tokenizer, load_dataset\n",
    "model, tokenizer = get_model_and_tokenizer(model_name=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22e6ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "       {{- '<|im_start|>' + message.role }}\n",
      "        {% generation %}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- if message.tool_calls %}\n",
      "            {%- for tool_call in message.tool_calls %}\n",
      "                {%- if tool_call.function is defined %}\n",
      "                    {%- set tool_call = tool_call.function %}\n",
      "                {%- endif %}\n",
      "                {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "                {{- tool_call.name }}\n",
      "                {{- '\", \"arguments\": ' }}\n",
      "                {{- tool_call.arguments | tojson }}\n",
      "                {{- '}\\n</tool_call>' }}\n",
      "            {%- endfor %}\n",
      "        {%- endif %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "       {% endgeneration %}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Loading tokenizer and model for /home/ubuntu/victor-north-tx/models/Qwen2.5-3B-instruct\n",
      "Tokenizer has a chat template.\n",
      "Tokenizer eos_token = <|im_end|>\n",
      "Tokenizer eos_token_id: 151645, pad_token_id: 151643\n",
      "Tokenizer chat_template supports assistant only loss: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 4.63 seconds\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from experiments.model_training.train_qwen import get_model_and_tokenizer, QWEN25_TEMPLATE, check_chat_template_supports_assistant_only_loss, load_dataset\n",
    "\n",
    "with open('test.jinja', mode='r') as fp:\n",
    "    QWEN25_TEMPLATE = fp.read()\n",
    "\n",
    "print(QWEN25_TEMPLATE)\n",
    "\n",
    "model_name = \"Qwen2.5-3B-instruct\"\n",
    "qwen_model = f\"/home/ubuntu/victor-north-tx/models/{model_name}\"\n",
    "tokenizer = get_model_and_tokenizer(qwen_model)[1]\n",
    "tokenizer.chat_template = QWEN25_TEMPLATE\n",
    "tokenizer.eos_token\n",
    "tokenizer.eos_token_id\n",
    "tokenizer.pad_token_id\n",
    "tokenizer.chat_template\n",
    "\n",
    "print(check_chat_template_supports_assistant_only_loss(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3971b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.model_training.train_qwen import DEFAULT_TRAIN_DATASET_PATH, DEFAULT_TEST_DATASET_PATH, DEFAULT_MAX_TRAIN_DATAPOINTS, DEFAULT_MAX_TEST_DATAPOINTS, load_dataset\n",
    "\n",
    "train_dataset = load_dataset(DEFAULT_TRAIN_DATASET_PATH, max_datapoints=DEFAULT_MAX_TRAIN_DATAPOINTS)\n",
    "test_dataset = load_dataset(DEFAULT_TEST_DATASET_PATH, max_datapoints=DEFAULT_MAX_TEST_DATAPOINTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1f1faf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'Hi! How can I help you today?',\n",
       " 'role': 'assistant',\n",
       " 'tool_call_id': None,\n",
       " 'tool_calls': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"messages\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56ef3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tokenizer.apply_chat_template(train_dataset[0][\"messages\"])\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad1ff276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [151644, 8948, 198, 27, 62295, 397, 2610, 525, 264, 6002, 2473, 8315, 429, 8609, 279, 1196, 4092, 311, 279, 366, 34790, 29, 3897, 3685, 624, 16014, 1817, 2484, 498, 646, 2987, 510, 12, 11000, 264, 1943, 311, 279, 1196, 624, 12, 7405, 264, 5392, 1618, 624, 98743, 25, 1446, 4157, 653, 2176, 518, 279, 1852, 882, 50347, 2679, 498, 3624, 1467, 2213, 1393, 3259, 264, 5392, 1618, 11, 279, 8315, 686, 4828, 458, 1465, 624, 1178, 2213, 686, 387, 3208, 311, 279, 1196, 13, 3155, 537, 990, 419, 2070, 369, 697, 1828, 32711, 382, 21453, 311, 387, 10950, 323, 2677, 1795, 279, 4842, 13, 23240, 1281, 2704, 498, 6923, 2697, 4718, 1172, 624, 522, 62295, 397, 7926, 8018, 397, 2, 6553, 1056, 20713, 10974, 271, 785, 1482, 882, 374, 220, 17, 15, 17, 19, 12, 15, 20, 12, 16, 20, 220, 16, 20, 25, 15, 15, 25, 15, 15, 25877, 382, 2121, 458, 32475, 8315, 11, 498, 646, 1492, 3847, 3070, 2190, 97219, 3070, 34053, 97219, 476, 3070, 18515, 334, 10971, 40062, 13, 1446, 1083, 3705, 3070, 1097, 42950, 323, 19857, 334, 382, 10227, 4633, 894, 6168, 429, 2647, 279, 21857, 4625, 320, 21278, 11, 46041, 24908, 11, 15664, 60549, 11, 10018, 13386, 536, 11, 476, 21010, 23148, 1995, 701, 498, 1969, 1140, 279, 1917, 3565, 323, 6851, 11464, 1196, 19539, 320, 9693, 8, 311, 10354, 382, 2610, 1265, 537, 3410, 894, 1995, 11, 6540, 11, 476, 15966, 537, 3897, 553, 279, 1196, 476, 2500, 7375, 11, 476, 2968, 43022, 18564, 476, 6042, 382, 2610, 1265, 1172, 1281, 825, 5392, 1618, 518, 264, 882, 11, 323, 421, 498, 1281, 264, 5392, 1618, 11, 498, 1265, 537, 5889, 311, 279, 1196, 24303, 13, 1416, 498, 5889, 311, 279, 1196, 11, 498, 1265, 537, 1281, 264, 5392, 1618, 518, 279, 1852, 882, 382, 2610, 1265, 23101, 1196, 7388, 429, 525, 2348, 419, 4842, 382, 2610, 1265, 8317, 279, 1196, 311, 264, 3738, 8315, 421, 323, 1172, 421, 279, 1681, 4157, 387, 17608, 2878, 279, 6891, 315, 697, 6168, 13, 2014, 8317, 11, 1156, 1281, 264, 5392, 1618, 311, 8317, 2346, 86247, 76347, 11, 323, 1221, 3624, 279, 1943, 364, 56389, 15824, 7206, 1718, 47533, 25108, 6504, 5146, 362, 472, 67565, 15082, 1825, 13, 53133, 17542, 6197, 3159, 311, 279, 1196, 382, 565, 21070, 14625, 271, 14374, 2657, 198, 4854, 1196, 702, 264, 5526, 8482, 510, 12, 1196, 877, 198, 12, 2551, 198, 12, 14230, 198, 12, 2400, 315, 7194, 198, 12, 8160, 5413, 198, 12, 15871, 2188, 198, 12, 27667, 5109, 271, 3862, 525, 2326, 4494, 315, 8160, 5413, 25, 3070, 23311, 3701, 97219, 3070, 52330, 3701, 97219, 3070, 48510, 15748, 334, 382, 3862, 525, 2326, 15871, 5866, 25, 3070, 22308, 97219, 3070, 81914, 97219, 3070, 34537, 334, 382, 14374, 26577, 198, 4854, 10971, 702, 279, 2701, 8201, 510, 12, 10971, 1372, 198, 12, 6238, 198, 12, 9106, 198, 12, 13537, 24915, 323, 18647, 882, 320, 2438, 882, 692, 32, 10971, 646, 387, 2500, 518, 5248, 12713, 13, 1752, 1817, 2400, 510, 12, 1416, 279, 2639, 374, 3070, 10334, 97219, 279, 10971, 702, 537, 4429, 1007, 11, 2500, 16312, 323, 7576, 525, 10007, 624, 12, 1416, 279, 2639, 374, 3070, 22263, 291, 334, 476, 3070, 263, 882, 97219, 279, 10971, 702, 537, 4429, 1007, 11, 4157, 387, 32970, 624, 12, 1416, 279, 2639, 374, 3070, 69, 6711, 97219, 279, 10971, 702, 4429, 1007, 714, 537, 26120, 11, 4157, 387, 32970, 382, 3862, 525, 2326, 13386, 6846, 25, 3070, 22342, 8584, 97219, 3070, 68, 70071, 97219, 3070, 26151, 334, 13, 3070, 22342, 8584, 334, 374, 1181, 1828, 536, 11, 6587, 12460, 504, 3070, 68, 70071, 334, 382, 75324, 18048, 323, 7576, 525, 10007, 369, 1817, 13386, 536, 382, 14374, 53312, 198, 4854, 27667, 29102, 279, 2701, 510, 12, 27667, 877, 198, 12, 1196, 877, 198, 12, 8411, 943, 198, 12, 24908, 198, 12, 22172, 198, 12, 8160, 5413, 198, 12, 3465, 882, 198, 12, 8968, 55044, 198, 12, 5821, 8113, 1995, 271, 3862, 525, 1378, 4494, 315, 8411, 25, 3070, 603, 1616, 334, 323, 3070, 1049, 8411, 334, 382, 565, 5893, 10971, 271, 785, 8315, 1969, 1156, 6851, 279, 1196, 877, 504, 279, 1196, 13, 4710, 785, 8315, 1265, 1221, 2548, 369, 279, 8411, 943, 11, 6238, 11, 9106, 382, 34, 8892, 510, 12, 24764, 536, 1969, 387, 279, 1852, 3941, 678, 279, 24908, 304, 264, 27667, 13, 4710, 12187, 14886, 25, 715, 12, 8886, 27667, 646, 614, 518, 1429, 4236, 22172, 13, 715, 12, 576, 8315, 3880, 311, 6530, 279, 1156, 829, 11, 1537, 829, 11, 323, 2400, 315, 7194, 369, 1817, 23148, 13, 715, 12, 2009, 22172, 1969, 11466, 279, 1852, 24908, 304, 279, 1852, 13386, 382, 20188, 25, 715, 12, 8886, 27667, 646, 990, 518, 1429, 825, 5821, 15748, 11, 518, 1429, 825, 6668, 3701, 11, 323, 518, 1429, 2326, 8189, 7411, 13, 715, 12, 576, 9664, 3311, 315, 264, 5821, 15748, 374, 537, 20965, 480, 13, 715, 12, 2009, 8160, 5413, 1969, 2669, 387, 304, 1196, 5526, 369, 7149, 7966, 382, 12666, 8968, 40358, 25, 715, 12, 1416, 279, 21857, 1196, 374, 264, 5792, 4462, 510, 220, 481, 220, 15, 1910, 10067, 8968, 369, 1817, 6770, 8584, 23148, 198, 220, 481, 220, 16, 1910, 10067, 8968, 369, 1817, 8584, 23148, 198, 220, 481, 220, 17, 1910, 10067, 17899, 369, 1817, 2562, 23148, 198, 12, 1416, 279, 21857, 1196, 374, 264, 14961, 4462, 510, 220, 481, 220, 16, 1910, 10067, 8968, 369, 1817, 6770, 8584, 23148, 198, 220, 481, 220, 17, 1910, 10067, 8968, 369, 1817, 8584, 23148, 198, 220, 481, 220, 18, 1910, 10067, 17899, 369, 1817, 2562, 23148, 198, 12, 1416, 279, 21857, 1196, 374, 264, 6623, 4462, 510, 220, 481, 220, 17, 1910, 10067, 8968, 369, 1817, 6770, 8584, 23148, 198, 220, 481, 220, 18, 1910, 10067, 8968, 369, 1817, 8584, 23148, 198, 220, 481, 220, 19, 1910, 10067, 17899, 369, 1817, 2562, 23148, 198, 12, 8886, 4960, 60549, 374, 220, 20, 15, 11192, 382, 5404, 537, 912, 10067, 17899, 429, 279, 1196, 1558, 537, 1184, 382, 40248, 8113, 25, 715, 12, 576, 8315, 1265, 2548, 421, 279, 1196, 6801, 311, 3695, 279, 5821, 8113, 624, 12, 576, 5821, 8113, 374, 220, 18, 15, 11192, 817, 23148, 323, 20081, 2480, 20965, 421, 279, 1196, 3880, 311, 9121, 279, 10971, 2661, 2820, 476, 9104, 7966, 382, 565, 49370, 10971, 271, 5338, 11, 279, 8315, 1969, 6851, 279, 1196, 877, 323, 27667, 877, 13, 715, 12, 576, 1196, 1969, 3410, 862, 1196, 877, 13, 715, 12, 1416, 279, 1196, 3171, 944, 1414, 862, 27667, 877, 11, 279, 8315, 1265, 1492, 24523, 432, 1667, 2500, 7375, 382, 4072, 24908, 25, 715, 12, 14625, 8584, 24908, 4157, 387, 10807, 624, 12, 6944, 40062, 646, 387, 10807, 2041, 10018, 279, 6238, 11, 9106, 11, 323, 8411, 943, 624, 12, 4329, 10971, 20632, 646, 387, 8604, 11, 714, 862, 7576, 686, 537, 387, 6049, 3118, 389, 279, 1482, 3349, 624, 12, 576, 5333, 1558, 537, 1779, 1493, 369, 279, 8315, 11, 773, 279, 8315, 1969, 1281, 2704, 279, 5601, 3796, 1573, 8098, 279, 5333, 2219, 4072, 13386, 25, 715, 12, 24764, 4157, 387, 5497, 421, 894, 10971, 304, 279, 27667, 702, 2669, 1012, 52750, 624, 12, 758, 1008, 5048, 11, 678, 40062, 11, 2670, 6770, 8584, 11, 646, 2297, 13386, 2041, 10018, 279, 24908, 624, 12, 24764, 536, 1969, 7146, 279, 1852, 3941, 678, 279, 24908, 304, 279, 1852, 27667, 26, 10018, 13386, 369, 1101, 825, 10971, 10238, 374, 537, 3204, 624, 12, 1416, 279, 3349, 1283, 13386, 2297, 374, 5080, 1091, 279, 4024, 3349, 11, 279, 1196, 374, 2567, 311, 2291, 369, 279, 6672, 624, 12, 1416, 279, 3349, 1283, 13386, 2297, 374, 4722, 1091, 279, 4024, 3349, 11, 279, 1196, 374, 1265, 387, 93452, 279, 6672, 382, 4072, 60549, 323, 8113, 25, 715, 12, 576, 1196, 646, 912, 714, 537, 4057, 10067, 17899, 624, 12, 576, 1196, 4157, 912, 8113, 1283, 2856, 21857, 382, 4072, 22172, 510, 12, 576, 1196, 646, 5602, 22172, 714, 4157, 5602, 279, 1372, 315, 22172, 624, 12, 7418, 264, 3738, 8315, 4157, 5602, 279, 1372, 315, 22172, 382, 20188, 25, 715, 12, 1416, 279, 24908, 525, 5497, 11, 279, 1196, 3880, 311, 3410, 264, 3175, 8189, 3701, 476, 6668, 3701, 369, 8160, 476, 20965, 1714, 13, 576, 8160, 1714, 1969, 2669, 387, 304, 1196, 5526, 369, 7149, 7966, 382, 565, 23542, 10971, 271, 5338, 11, 279, 8315, 1969, 6851, 279, 1196, 877, 323, 27667, 877, 13, 715, 12, 576, 1196, 1969, 3410, 862, 1196, 877, 13, 715, 12, 1416, 279, 1196, 3171, 944, 1414, 862, 27667, 877, 11, 279, 8315, 1265, 1492, 24523, 432, 1667, 2500, 7375, 382, 785, 8315, 1969, 1083, 6851, 279, 2874, 369, 35835, 320, 3373, 315, 3119, 11, 32475, 25681, 10971, 11, 476, 1008, 7966, 692, 2679, 894, 13348, 315, 279, 10971, 702, 2669, 1012, 52750, 11, 279, 8315, 4157, 1492, 323, 8317, 374, 4362, 382, 80456, 11, 10971, 646, 387, 25681, 421, 894, 315, 279, 2701, 374, 830, 510, 12, 576, 21857, 572, 1865, 2878, 279, 1537, 220, 17, 19, 40040, 198, 12, 576, 10971, 374, 25681, 553, 32475, 198, 12, 1084, 374, 264, 2562, 10971, 198, 12, 576, 1196, 702, 5821, 8113, 323, 279, 2874, 369, 35835, 374, 9761, 553, 8113, 382, 785, 5333, 1558, 537, 1779, 429, 35835, 5601, 525, 2270, 11, 773, 279, 8315, 1969, 1281, 2704, 279, 5601, 3796, 1573, 8098, 279, 5333, 2219, 3945, 1241, 510, 12, 576, 20965, 686, 728, 311, 4024, 8160, 5413, 2878, 220, 20, 311, 220, 22, 2562, 2849, 382, 565, 8550, 42950, 323, 69296, 198, 5404, 537, 462, 63019, 3010, 264, 19857, 7241, 279, 1196, 20975, 17064, 369, 825, 382, 5404, 537, 45694, 421, 279, 1196, 374, 5792, 4462, 323, 702, 902, 5821, 8113, 323, 37104, 320, 22342, 8, 8584, 382, 37095, 42396, 279, 13064, 1573, 10004, 19857, 382, 7308, 45694, 421, 279, 1196, 374, 264, 14961, 4846, 813, 4462, 476, 702, 5821, 8113, 476, 37104, 2562, 382, 12, 1416, 279, 1196, 88958, 911, 25681, 24908, 304, 264, 27667, 11, 279, 8315, 646, 3010, 264, 15748, 438, 264, 30157, 1283, 48996, 279, 13064, 11, 448, 279, 3311, 1660, 400, 16, 15, 15, 3039, 279, 1372, 315, 22172, 382, 12, 1416, 279, 1196, 88958, 911, 22706, 24908, 304, 264, 27667, 323, 6801, 311, 2297, 476, 9121, 279, 27667, 11, 279, 8315, 646, 3010, 264, 15748, 438, 264, 30157, 1283, 48996, 279, 13064, 323, 10018, 476, 96915, 279, 27667, 11, 448, 279, 3311, 1660, 400, 20, 15, 3039, 279, 1372, 315, 22172, 382, 5404, 537, 3010, 19857, 369, 894, 1008, 2874, 1091, 279, 6174, 10007, 3403, 624, 522, 34790, 29, 151645, 198, 151644, 77091, 271, 13048, 0, 2585, 646, 358, 1492, 498, 3351, 30, 151645, 271, 151644, 872, 198, 13048, 11, 358, 6925, 1075, 1045, 1492, 448, 847, 3213, 21857, 13, 358, 1366, 311, 9121, 279, 8113, 358, 14733, 369, 847, 10971, 323, 633, 264, 2480, 20965, 369, 432, 11, 714, 358, 1513, 1405, 1366, 311, 9121, 279, 10971, 5086, 13, 2980, 498, 7789, 752, 448, 429, 30, 151645, 198, 151644, 77091, 271, 11190, 311, 279, 4842, 11, 5821, 8113, 4157, 387, 33446, 476, 6963, 1283, 279, 2856, 21857, 13, 15277, 11, 358, 1079, 11889, 311, 7789, 448, 9121, 287, 279, 8113, 25156, 504, 279, 10971, 382, 2679, 498, 614, 894, 1008, 7388, 476, 1184, 12994, 448, 2494, 770, 11, 4486, 1077, 752, 1414, 13, 151645, 271, 151644, 872, 198, 40, 4249, 2167, 24402, 311, 6723, 429, 13, 358, 5710, 1405, 6247, 448, 279, 2473, 358, 3949, 11, 323, 358, 1513, 1405, 1744, 432, 748, 6624, 429, 358, 646, 1405, 633, 264, 20965, 369, 279, 8113, 11, 5310, 2474, 358, 2058, 1366, 311, 2506, 847, 10971, 13, 2160, 1052, 894, 1616, 498, 646, 88590, 419, 476, 1779, 421, 458, 4683, 646, 387, 1865, 30, 151645, 198, 151644, 77091, 271, 151657, 198, 4913, 606, 788, 330, 24188, 2346, 86247, 76347, 497, 330, 16370, 788, 54734, 1708, 11693, 7245, 1474, 374, 42151, 448, 279, 4842, 429, 5821, 8113, 4157, 387, 33446, 476, 93452, 25156, 504, 279, 10971, 13, 2657, 6801, 311, 2506, 279, 10971, 714, 633, 264, 20965, 369, 279, 8113, 323, 374, 34692, 70289, 476, 4683, 86865, 9863, 532, 151658, 151645, 271, 151644, 872, 198, 27, 14172, 9655, 397, 21970, 6849, 198, 522, 14172, 9655, 29, 151645, 198, 151644, 77091, 271, 56389, 15824, 7206, 1718, 47533, 25108, 6504, 5146, 362, 472, 67565, 15082, 1825, 13, 53133, 17542, 6197, 13, 151645, 271], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'assistant_masks': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "res = tokenizer.apply_chat_template(train_dataset[0][\"messages\"], tokenize=True, return_dict=True, add_generation_prompt=False, return_assistant_tokens_mask=True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e5e067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "       {{- '<|im_start|>' + message.role }}\n",
      "        {% generation %}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- if message.tool_calls %}\n",
      "            {%- for tool_call in message.tool_calls %}\n",
      "                {%- if tool_call.function is defined %}\n",
      "                    {%- set tool_call = tool_call.function %}\n",
      "                {%- endif %}\n",
      "                {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "                {{- tool_call.name }}\n",
      "                {{- '\", \"arguments\": ' }}\n",
      "                {{- tool_call.arguments | tojson }}\n",
      "                {{- '}\\n</tool_call>' }}\n",
      "            {%- endfor %}\n",
      "        {%- endif %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "       {% endgeneration %}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# QWEN25_TEMPLATE = \"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- messages[0]['content'] }}\\n    {%- else %}\\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\\n    {%- endif %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\\n    {%- else %}\\n        {{- '<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n       {% generation %}\\n       {{- '<|im_start|>' + message.role }}\\n        {%- if message.content %}\\n            {{- '\\\\n' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\", \\\"arguments\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- '}\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n       {% endgeneration %}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97b2ac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 3838, 374, 279, 6722, 315, 9625, 30, 151645, 198, 151644, 77091, 198, 785, 6722, 315, 9625, 374, 12095, 13, 151645, 271], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'assistant_masks': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|im_start|>',\n",
       " 'assistant',\n",
       " '\\n',\n",
       " 'The',\n",
       " ' capital',\n",
       " ' of',\n",
       " ' France',\n",
       " ' is',\n",
       " ' Paris',\n",
       " '.',\n",
       " '<|im_end|>',\n",
       " '\\n\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"}\n",
    "]\n",
    "\n",
    "res = tokenizer.apply_chat_template(messages, tokenize=True, return_dict=True, add_generation_prompt=False, return_assistant_tokens_mask=True)\n",
    "print(res)\n",
    "[tokenizer.decode(token_id) for i, token_id in enumerate(res['input_ids']) if res['assistant_masks'][i]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tau2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
