experiment_name: tau2-grpo
trial_name: trial0

seed: 1
enable_offload: false
total_train_epochs: 20
tokenizer_path: ${actor.path}

cluster:
  n_nodes: 2
  n_gpus_per_node: 8
  fileroot: /storage/openpsi/experiments
  name_resolve:
    type: nfs
    nfs_record_root: /storage/openpsi/experiments/name_resolve
    etcd3_addr: etcd-client.openpsi-etcd.svc.sigma-su18-01.hn01.su18-hn.local:2379

allocation_mode: sglang:d4t2+archon:d4p2t1

scheduler:
  type: slurm

rollout:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  tokenizer_path: ${tokenizer_path}
  max_concurrent_rollouts: 64  # Reduced from 512 to avoid SGLang OOM
  queue_size: null
  consumer_batch_size: ${train_dataset.batch_size}
  max_head_offpolicyness: 2
  enable_rollout_tracing: false
  scheduling_spec: ${actor.scheduling_spec}
  openai:
    mode: inline
    tool_call_parser: qwen25
    reasoning_parser: qwen3
    export_style: individual
    turn_discount: ${econfig.turn_discount}
    engine_max_tokens: ${gconfig.max_tokens}

gconfig:
  n_samples: 16
  min_new_tokens: 0
  max_new_tokens: 512
  max_tokens: 32768
  greedy: false
  temperature: 1.0

actor:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  path: /storage/openpsi/models/Qwen__Qwen3-1.7B
  init_from_scratch: false
  disable_dropout: true
  gradient_checkpointing: true
  dtype: bfloat16
  mb_spec:
    max_tokens_per_mb: 32768
  optimizer:
    type: adam
    lr: 1.70e-5
    weight_decay: 0.017
    beta1: 0.9
    beta2: 0.999
    eps: 1e-8
    lr_scheduler_type: constant
    gradient_clipping: 1.0
    warmup_steps_proportion: 0.001
  eps_clip: 0.4
  temperature: ${gconfig.temperature}  # Must match gconfig.temperature for consistent logprob computation
  reward_scaling: 10.0
  reward_bias: -0.5
  kl_ctl: 0.0
  ppo_n_minibatches: 1
  recompute_logprob: true
  use_decoupled_loss: true
  behav_imp_weight_cap: 5.0
  weight_update_mode: xccl
  reward_norm:
    mean_level: group
    std_level: group
    group_size: ${gconfig.n_samples}
  adv_norm:
    mean_level: batch
    std_level: batch
  max_new_tokens: ${gconfig.max_new_tokens}
  scheduling_spec:
    - task_type: worker
      port_count: 2
      gpu: 1
      cpu: 15
      mem: 32
      image: /storage/openpsi/images/areal-dev.sif
      cmd: python3 -m areal.scheduler.rpc.rpc_server
      env_vars:
        NCCL_SOCKET_IFNAME: "bond0"
        NCCL_IB_DISABLE: "0"
        NCCL_NET: "IB"
        NCCL_NET_PLUGIN: ""
        NCCL_IB_GID_INDEX: "3"
        NCCL_IB_TIMEOUT: "22"
        NCCL_IB_RETRY_CNT: "7"
        NCCL_IB_SL: "5"
        NCCL_IB_TC: "136"
        NCCL_IB_HCA: "mlx5_bond"
        NCCL_IB_QPS_PER_CONNECTION: "8"
        NCCL_SET_THREAD_NAME: "1"
        NCCL_DEBUG: "WARN"
        HF_ENDPOINT: "https://hf-mirror.com"
      additional_bash_cmds:
        - pip install tenacity
        - pip install torch_memory_saver
        - pip install /storage/openpsi/users/donghonghua.dhh/workspace/tau2-bench
        - pip install "transformers==4.57.1"
        - export TAU2_DATA_DIR=/storage/openpsi/users/donghonghua.dhh/workspace/tau2-bench/data

ref:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  path: ${actor.path}
  init_from_scratch: false
  disable_dropout: true
  dtype: ${actor.dtype}
  mb_spec:
    max_tokens_per_mb: 32768
  optimizer: null
  scheduling_strategy:
    type: colocation
    target: actor
  scheduling_spec: ${actor.scheduling_spec}

# Tau2 environment config
econfig:
  domain: airline
  max_steps: 200
  solo_mode: false
  add_thinking_tool: true
  user_llm_base_url: http://33.180.172.15:35862/v1
  user_llm: openai/qwen
  user_llm_args:
    temperature: 0.0
    max_completion_tokens: 2048
  turn_discount: 1.0
  invalid_format_penalty: 0.1
  save_trajectories: true                                              
  trajectory_save_dir: null

# SGLang config
sglang:
  model_path: ${actor.path}
  random_seed: ${seed}
  skip_tokenizer_init: true
  dtype: ${actor.dtype}
  max_running_requests: null
  context_length: 32768
  mem_fraction_static: 0.8

vllm:
  model: ${actor.path}
  seed: ${seed}
  skip_tokenizer_init: false
  dtype: ${actor.dtype}
  max_model_len: 32768
  gpu_memory_utilization: 0.9

# datasets
train_dataset:
  batch_size: 16
  shuffle: true
  pin_memory: true
  num_workers: 4
  path: tau2/train
  type: rl
  max_length: 1024

valid_dataset:
  batch_size: 16
  shuffle: true
  pin_memory: true
  num_workers: 4
  path: tau2/test
  type: rl
  drop_last: false

# Utilities
saver:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  freq_epochs: 1
  freq_steps: null
  freq_secs: null

recover:
  mode: disabled
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  freq_epochs: 1
  freq_steps: null
  freq_secs: 3600

evaluator:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  freq_epochs: 1
  freq_steps: null
  freq_secs: null

stats_logger:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  wandb:
    mode: online
    wandb_base_url: http://8.150.1.98:8080
    wandb_api_key: local-b12dd3e0df637c92fc312ad1a797986f8b8a012f

perf_tracer:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  enabled: false
  session_tracer:
    enabled: false
