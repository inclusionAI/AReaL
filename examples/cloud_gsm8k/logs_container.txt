2025-11-15T23:07:05.736918943Z ==========
2025-11-15T23:07:05.736928151Z == CUDA ==
2025-11-15T23:07:05.736966522Z ==========
2025-11-15T23:07:05.738592102Z CUDA Version 12.9.1
2025-11-15T23:07:05.739054506Z Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2025-11-15T23:07:05.739453142Z This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2025-11-15T23:07:05.739454384Z By pulling and using the container, you accept the terms and conditions of this license:
2025-11-15T23:07:05.739455446Z https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2025-11-15T23:07:05.739456989Z A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
2025-11-15T23:07:05.816463927Z Writing to /root/.config/pip/pip.conf
2025-11-15T23:07:05.897174591Z Writing to /root/.config/pip/pip.conf
2025-11-15T23:07:05.908331071Z Cloning into 'AReaL'...
2025-11-15T23:07:07.791036161Z Checking AReaL installation...
2025-11-15T23:07:07.844355338Z AReaL already installed. Skipping installation.
2025-11-15T23:07:07.844367441Z Checking GPU...
2025-11-15T23:07:07.858379731Z NVIDIA GeForce RTX 5090, 32607 MiB
2025-11-15T23:07:07.858393106Z Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True for better memory management
2025-11-15T23:07:07.871282897Z Using 1-HOUR training configuration (~1-2 hours)
2025-11-15T23:07:07.871290822Z Note: Uses limited dataset (500 samples) from docker_gsm8k script
2025-11-15T23:07:07.872441173Z ==========================================
2025-11-15T23:07:07.872442706Z Starting GRPO Training (Cloud)
2025-11-15T23:07:07.872443858Z ==========================================
2025-11-15T23:07:07.872444559Z Config: 1hour
2025-11-15T23:07:07.872445271Z Config file: examples/cloud_gsm8k/gsm8k_grpo_1hour.yaml
2025-11-15T23:07:07.872447044Z Training script: examples/docker_gsm8k/gsm8k_grpo_1hour.py
2025-11-15T23:07:07.872447585Z Experiment: gsm8k-grpo-cloud-1hour
2025-11-15T23:07:07.872448116Z Trial: trial_20251115_230707
2025-11-15T23:07:07.872450811Z WandB API key: e1adc5be02...
2025-11-15T23:07:07.872451623Z ==========================================
2025-11-15T23:07:08.653119272Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-15T23:07:08.653146744Z   import pynvml  # type: ignore[import]
2025-11-15T23:07:11.671273254Z [37m20251115-23:07:11.671 Platform init INFO: Detected CUDA device: NVIDIA GEFORCE RTX 5090[0m
2025-11-15T23:07:11.671305976Z [37m20251115-23:07:11.671 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-15T23:07:11.671465404Z [37m20251115-23:07:11.671 name-resolve INFO: No such name resolve path: ./tmp/areal/name_resolve/root/gsm8k-grpo-cloud-1hour/trial_20251115_230707[0m
2025-11-15T23:07:11.729627406Z [37m20251115-23:07:11.729 Local Scheduler INFO: LocalLauncher: experiment_name=gsm8k-grpo-cloud-1hour, trial_name=trial_20251115_230707, fileroot=/workspace/outputs/grpo, run_id=0, is_recover_run=False[0m
2025-11-15T23:07:11.734760433Z [37m20251115-23:07:11.734 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL CUDA_VISIBLE_DEVICES=0 stdbuf -oL python3 -m areal.launcher.sglang_server examples/docker_gsm8k/gsm8k_grpo_1hour.py --config examples/cloud_gsm8k/gsm8k_grpo_1hour.yaml experiment_name=gsm8k-grpo-cloud-1hour trial_name=trial_20251115_230707 sglang.random_seed=1 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-1hour/trial_20251115_230707/llm_server.log[0m
2025-11-15T23:07:12.123849526Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-15T23:07:12.123887627Z   import pynvml  # type: ignore[import]
2025-11-15T23:07:12.884420068Z [37m20251115-23:07:12.884 Platform init INFO: Detected CUDA device: NVIDIA GEFORCE RTX 5090[0m
2025-11-15T23:07:12.884699320Z [37m20251115-23:07:12.884 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-15T23:07:12.952465911Z [37m20251115-23:07:12.952 SGLangServer Wrapper INFO: Launch command: python3 -m sglang.launch_server --host 172.18.0.2 --port 19409 --tokenizer-path Qwen/Qwen2.5-0.5B-Instruct --tokenizer-mode auto --load-format auto --trust-remote-code --device cuda --tp-size 1 --base-gpu-id 0 --nnodes 1 --node-rank 0 --dist-init-addr localhost:44897 --model-path Qwen/Qwen2.5-0.5B-Instruct --random-seed 1 --skip-tokenizer-init --disable-radix-cache --torch-compile-max-bs 32 --triton-attention-num-kv-splits 8 --num-continuous-decode-steps 1 --attention-backend fa3 --context-length 4096 --mem-fraction-static 0.8 --chunked-prefill-size -1 --max-prefill-tokens 32768 --schedule-policy lpm --schedule-conservativeness 1.0 --cpu-offload-gb 0 --dtype bfloat16 --kv-cache-dtype auto --dp-size 1 --ep-size 1 --max-loaded-loras 1 --max-loras-per-batch 1 --lora-backend triton --log-level warning --log-level-http warning --log-requests-level 0 --enable-metrics --decode-log-interval 1[0m
2025-11-15T23:07:13.627156684Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-15T23:07:13.627186801Z   import pynvml  # type: ignore[import]
2025-11-15T23:07:17.077376437Z INFO 11-15 23:07:17 [__init__.py:216] Automatically detected platform cuda.
2025-11-15T23:07:18.061017749Z All deep_gemm operations loaded successfully!
2025-11-15T23:07:18.253380190Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-15T23:07:18.676431145Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-15T23:07:18.676457294Z   import pynvml  # type: ignore[import]
2025-11-15T23:07:18.676459267Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-15T23:07:18.676460520Z   import pynvml  # type: ignore[import]
2025-11-15T23:07:21.925636990Z INFO 11-15 23:07:21 [__init__.py:216] Automatically detected platform cuda.
2025-11-15T23:07:21.929514771Z INFO 11-15 23:07:21 [__init__.py:216] Automatically detected platform cuda.
2025-11-15T23:07:22.567644392Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-15T23:07:22.931651926Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-15T23:07:22.938163311Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-15T23:07:22.938419140Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-15T23:07:22.938688594Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-15T23:07:22.976403633Z [2025-11-15 23:07:22] MOE_RUNNER_BACKEND is not initialized, using triton backend
2025-11-15T23:07:23.390509112Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-15T23:07:23.390541923Z   warnings.warn(
2025-11-15T23:07:23.390925140Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-15T23:07:23.390939747Z   warnings.warn(
2025-11-15T23:07:26.777247690Z All deep_gemm operations loaded successfully!
2025-11-15T23:07:26.777420103Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-15T23:07:26.871660514Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.61it/s]
2025-11-15T23:07:26.981019675Z [2025-11-15 23:07:26] Scheduler hit an exception: Traceback (most recent call last):
2025-11-15T23:07:26.981037979Z   File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2587, in run_scheduler_process
2025-11-15T23:07:26.981040233Z     scheduler = Scheduler(
2025-11-15T23:07:26.981041756Z                 ^^^^^^^^^^
2025-11-15T23:07:26.981042648Z   File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 329, in __init__
2025-11-15T23:07:26.981043610Z     self.tp_worker = TpWorkerClass(
2025-11-15T23:07:26.981045413Z                      ^^^^^^^^^^^^^^
2025-11-15T23:07:26.981046104Z   File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py", line 71, in __init__
2025-11-15T23:07:26.981047557Z     self.worker = TpModelWorker(
2025-11-15T23:07:26.981048198Z                   ^^^^^^^^^^^^^^
2025-11-15T23:07:26.981048769Z   File "/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py", line 93, in __init__
2025-11-15T23:07:26.981049691Z     self.model_runner = ModelRunner(
2025-11-15T23:07:26.981050583Z                         ^^^^^^^^^^^^
2025-11-15T23:07:26.981051074Z   File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 250, in __init__
2025-11-15T23:07:26.981051605Z     self.initialize(min_per_gpu_memory)
2025-11-15T23:07:26.981052075Z   File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 385, in initialize
2025-11-15T23:07:26.981052636Z     self.init_attention_backend()
2025-11-15T23:07:26.981053107Z   File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 1542, in init_attention_backend
2025-11-15T23:07:26.981053829Z     self.attn_backend = self._get_attention_backend()
2025-11-15T23:07:26.981054370Z                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-15T23:07:26.981054901Z   File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 1580, in _get_attention_backend
2025-11-15T23:07:26.981055432Z     attn_backend = self._get_attention_backend_from_str(
2025-11-15T23:07:26.981055913Z                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-15T23:07:26.981056383Z   File "/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py", line 1651, in _get_attention_backend_from_str
2025-11-15T23:07:26.981056945Z     assert (
2025-11-15T23:07:26.981057806Z            ^
2025-11-15T23:07:26.981058297Z AssertionError: FlashAttention v3 Backend requires SM>=80 and SM<=90. Please use `--attention-backend flashinfer`.
2025-11-15T23:07:26.981264493Z [2025-11-15 23:07:26] Received sigquit from a child process. It usually means the child failed.
2025-11-15T23:07:26.981612714Z Exception ignored in atexit callback: <function move_cutlass_compiled_cache at 0x7e41586b9080>
2025-11-15T23:07:26.981902275Z Traceback (most recent call last):
2025-11-15T23:07:26.981914779Z   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/codegen/cuda/cutlass_utils.py", line 39, in move_cutlass_compiled_cache
2025-11-15T23:07:26.981916662Z     if not os.path.exists(cutlass.CACHE_FILE):
2025-11-15T23:07:26.981917143Z                           ^^^^^^^^^^^^^^^^^^
2025-11-15T23:07:26.981917694Z AttributeError: module 'cutlass' has no attribute 'CACHE_FILE'