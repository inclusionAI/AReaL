2025-11-26T14:32:10.785254894Z ==========
2025-11-26T14:32:10.785261784Z == CUDA ==
2025-11-26T14:32:10.785423173Z ==========
2025-11-26T14:32:10.801119974Z CUDA Version 12.9.1
2025-11-26T14:32:10.801132044Z Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2025-11-26T14:32:10.801144554Z This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2025-11-26T14:32:10.801150534Z By pulling and using the container, you accept the terms and conditions of this license:
2025-11-26T14:32:10.801155614Z https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2025-11-26T14:32:10.801166164Z A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
2025-11-26T14:32:11.024316641Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:32:11.235516787Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:32:11.269732182Z Cloning into 'AReaL'...
2025-11-26T14:32:13.983467659Z Checking AReaL installation...
2025-11-26T14:32:14.591252901Z AReaL already installed. Skipping installation.
2025-11-26T14:32:14.591303151Z Cleaning up any leftover GPU processes...
2025-11-26T14:32:14.591867229Z Installing cleanup tools (psmisc, lsof)...
2025-11-26T14:32:14.857080281Z Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]
2025-11-26T14:32:15.303854863Z Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,153 kB]
2025-11-26T14:32:15.336203833Z Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]
2025-11-26T14:32:15.646040574Z Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]
2025-11-26T14:32:16.633855053Z Ign:1 https://developer.download.nvidia.com/devtools/repos/ubuntu2004/amd64  InRelease
2025-11-26T14:32:16.769849786Z Hit:7 https://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy InRelease
2025-11-26T14:32:16.825589547Z Get:6 https://developer.download.nvidia.com/devtools/repos/ubuntu2004/amd64  Release [496 B]
2025-11-26T14:32:17.075957346Z Get:8 https://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-updates InRelease [128 kB]
2025-11-26T14:32:17.113252744Z Get:9 https://developer.download.nvidia.com/devtools/repos/ubuntu2004/amd64  Release.gpg [833 B]
2025-11-26T14:32:17.684759956Z Get:10 https://developer.download.nvidia.com/devtools/repos/ubuntu2004/amd64  Packages [18.8 kB]
2025-11-26T14:32:17.967733474Z Get:11 https://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-backports InRelease [127 kB]
2025-11-26T14:32:18.372874689Z Get:12 https://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-security InRelease [129 kB]
2025-11-26T14:32:18.758797461Z Get:13 https://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-updates/main amd64 Packages [3,876 kB]
2025-11-26T14:32:19.879701160Z Get:14 https://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]
2025-11-26T14:32:20.373273195Z Get:15 https://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]
2025-11-26T14:32:20.375093571Z Get:16 https://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-updates/universe amd64 Packages [1,596 kB]
2025-11-26T14:32:20.487240732Z Get:17 https://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]
2025-11-26T14:32:20.488196440Z Get:18 https://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-backports/main amd64 Packages [83.9 kB]
2025-11-26T14:32:20.490263315Z Get:19 https://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-security/main amd64 Packages [3,539 kB]
2025-11-26T14:32:20.851356449Z Get:20 https://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-security/universe amd64 Packages [1,290 kB]
2025-11-26T14:32:20.884189957Z Get:21 https://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-security/restricted amd64 Packages [6,008 kB]
2025-11-26T14:32:21.379441529Z Get:22 https://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy-security/multiverse amd64 Packages [60.9 kB]
2025-11-26T14:32:21.673871448Z Fetched 25.4 MB in 7s (3,669 kB/s)
2025-11-26T14:32:22.849309462Z Reading package lists...
2025-11-26T14:32:22.877548222Z W: http://developer.download.nvidia.com/devtools/repos/ubuntu2004/amd64/Release.gpg: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.
2025-11-26T14:32:24.096359178Z Reading package lists...
2025-11-26T14:32:24.391988115Z Building dependency tree...
2025-11-26T14:32:24.392571643Z Reading state information...
2025-11-26T14:32:24.627361971Z lsof is already the newest version (4.93.2+dfsg-1.1build2).
2025-11-26T14:32:24.627414900Z The following NEW packages will be installed:
2025-11-26T14:32:24.627453480Z   psmisc
2025-11-26T14:32:26.982984796Z 0 upgraded, 1 newly installed, 0 to remove and 70 not upgraded.
2025-11-26T14:32:26.983011106Z Need to get 119 kB of archives.
2025-11-26T14:32:26.983015786Z After this operation, 463 kB of additional disk space will be used.
2025-11-26T14:32:26.983020346Z Get:1 https://mirrors.tuna.tsinghua.edu.cn/ubuntu jammy/main amd64 psmisc amd64 23.4-2build3 [119 kB]
2025-11-26T14:32:27.750226553Z debconf: delaying package configuration, since apt-utils is not installed
2025-11-26T14:32:27.807683090Z Fetched 119 kB in 3s (41.6 kB/s)
2025-11-26T14:32:27.854703593Z Selecting previously unselected package psmisc.
2025-11-26T14:32:27.950528726Z (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 70438 files and directories currently installed.)
2025-11-26T14:32:27.952287751Z Preparing to unpack .../psmisc_23.4-2build3_amd64.deb ...
2025-11-26T14:32:27.953201139Z Unpacking psmisc (23.4-2build3) ...
2025-11-26T14:32:28.015949073Z Setting up psmisc (23.4-2build3) ...
2025-11-26T14:32:28.023003076Z Processing triggers for man-db (2.10.2-1) ...
2025-11-26T14:32:31.183155895Z Checking for processes holding GPU device files...
2025-11-26T14:32:31.869896572Z Found processes holding GPU devices: 1
2025-11-26T14:32:31.869939661Z 644
2025-11-26T14:32:31.869943991Z 645
2025-11-26T14:32:31.869946761Z 94
2025-11-26T14:32:31.869949271Z Killing process 1...
2025-11-26T14:32:31.869960071Z Killing process 644...
2025-11-26T14:32:31.870703670Z Killing process 645...
2025-11-26T14:32:33.874735327Z Using fuser to kill processes on GPU devices...
2025-11-26T14:32:35.901413869Z Checking GPU...
2025-11-26T14:32:35.940356873Z NVIDIA A100 80GB PCIe, 81920 MiB
2025-11-26T14:32:35.968512383Z Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True for better memory management
2025-11-26T14:32:35.968559203Z Detected 1 GPU(s)
2025-11-26T14:32:35.968566833Z Checking GPU status...
2025-11-26T14:32:35.999017687Z index, name, utilization.gpu [%], memory.used [MiB], compute_mode
2025-11-26T14:32:36.008779873Z 0, NVIDIA A100 80GB PCIe, 0, 0, Default
2025-11-26T14:32:36.064500495Z Verifying GPU accessibility...
2025-11-26T14:32:36.865379478Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:32:36.865428257Z   import pynvml  # type: ignore[import]
2025-11-26T14:32:38.119054277Z GPU accessibility verified on attempt 1
2025-11-26T14:32:38.770561601Z Starting training...
2025-11-26T14:32:41.774969017Z Using FAST training configuration (20-30 minutes)
2025-11-26T14:32:41.780892232Z ==========================================
2025-11-26T14:32:41.780904692Z Starting GRPO Training (Cloud)
2025-11-26T14:32:41.780912532Z ==========================================
2025-11-26T14:32:41.780918442Z Config: fast
2025-11-26T14:32:41.780924392Z Config file: examples/cloud_gsm8k/gsm8k_grpo_fast.yaml
2025-11-26T14:32:41.780930552Z Training script: examples/cloud_gsm8k/gsm8k_grpo_train.py
2025-11-26T14:32:41.780969512Z Experiment: gsm8k-grpo-cloud-fast
2025-11-26T14:32:41.780975552Z Trial: trial_20251126_143241
2025-11-26T14:32:41.780980972Z GPU: NVIDIA A100 80GB PCIe (81920 MB)
2025-11-26T14:32:41.780994012Z WandB API key: e1adc5be02...
2025-11-26T14:32:41.780999512Z ==========================================
2025-11-26T14:32:43.368698762Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:32:43.368745302Z   import pynvml  # type: ignore[import]
2025-11-26T14:32:48.555642043Z [37m20251126-14:32:48.555 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:32:48.555687443Z [37m20251126-14:32:48.555 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:32:48.556121282Z [37m20251126-14:32:48.555 name-resolve INFO: No such name resolve path: ./tmp/areal/name_resolve/root/gsm8k-grpo-cloud-fast/trial_20251126_143241[0m
2025-11-26T14:32:48.723354937Z [37m20251126-14:32:48.723 Local Scheduler INFO: LocalLauncher: experiment_name=gsm8k-grpo-cloud-fast, trial_name=trial_20251126_143241, fileroot=/workspace/outputs/grpo, run_id=0, is_recover_run=False[0m
2025-11-26T14:32:48.731572497Z [37m20251126-14:32:48.731 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL CUDA_VISIBLE_DEVICES=0 stdbuf -oL python3 -m areal.launcher.sglang_server examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_143241 sglang.random_seed=1 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143241/llm_server.log[0m
2025-11-26T14:32:50.272521064Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:32:50.272583024Z   import pynvml  # type: ignore[import]
2025-11-26T14:32:52.239377514Z [37m20251126-14:32:52.238 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:32:52.239403264Z [37m20251126-14:32:52.239 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:32:52.387189137Z [37m20251126-14:32:52.386 SGLangServer Wrapper INFO: Launch command: python3 -m sglang.launch_server --host 172.18.0.2 --port 18424 --tokenizer-path Qwen/Qwen2.5-0.5B-Instruct --tokenizer-mode auto --load-format auto --trust-remote-code --device cuda --tp-size 1 --base-gpu-id 0 --nnodes 1 --node-rank 0 --dist-init-addr localhost:35834 --model-path Qwen/Qwen2.5-0.5B-Instruct --random-seed 1 --skip-tokenizer-init --disable-radix-cache --torch-compile-max-bs 32 --triton-attention-num-kv-splits 8 --num-continuous-decode-steps 1 --attention-backend flashinfer --context-length 4096 --mem-fraction-static 0.8 --chunked-prefill-size -1 --max-prefill-tokens 32768 --schedule-policy lpm --schedule-conservativeness 1.0 --cpu-offload-gb 0 --dtype bfloat16 --kv-cache-dtype auto --dp-size 1 --ep-size 1 --max-loaded-loras 1 --max-loras-per-batch 1 --lora-backend triton --log-level warning --log-level-http warning --log-requests-level 0 --enable-metrics --decode-log-interval 1[0m
2025-11-26T14:32:53.829303829Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:32:53.829361529Z   import pynvml  # type: ignore[import]
2025-11-26T14:33:01.074156264Z INFO 11-26 14:33:01 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:33:02.222102596Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:33:03.588573506Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:33:03.588595686Z   import pynvml  # type: ignore[import]
2025-11-26T14:33:03.596112237Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:33:03.596139847Z   import pynvml  # type: ignore[import]
2025-11-26T14:33:10.554530723Z INFO 11-26 14:33:10 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:33:10.636702399Z INFO 11-26 14:33:10 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:33:11.363620826Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:33:11.680034881Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:33:11.693185128Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:33:11.694241235Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:33:11.695269723Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:33:11.772321142Z [2025-11-26 14:33:11] MOE_RUNNER_BACKEND is not initialized, using triton backend
2025-11-26T14:33:12.493311563Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:33:12.493347383Z   warnings.warn(
2025-11-26T14:33:12.493354053Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:33:12.493359873Z   warnings.warn(
2025-11-26T14:33:22.707201912Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:33:22.913871739Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.85it/s]
2025-11-26T14:33:22.913994489Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.84it/s]
2025-11-26T14:34:14.297771903Z   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   4%|â–         | 1/23 [00:48<17:45, 48.45s/it]Capturing batches (bs=152 avail_mem=14.84 GB):   4%|â–         | 1/23 [00:48<17:45, 48.45s/it]Capturing batches (bs=144 avail_mem=14.82 GB):   4%|â–         | 1/23 [00:48<17:45, 48.45s/it]Capturing batches (bs=144 avail_mem=14.82 GB):  13%|â–ˆâ–Ž        | 3/23 [00:48<04:12, 12.61s/it]Capturing batches (bs=136 avail_mem=14.81 GB):  13%|â–ˆâ–Ž        | 3/23 [00:48<04:12, 12.61s/it]Capturing batches (bs=128 avail_mem=14.79 GB):  13%|â–ˆâ–Ž        | 3/23 [00:48<04:12, 12.61s/it]Capturing batches (bs=128 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:48<01:50,  6.16s/it]Capturing batches (bs=120 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:48<01:50,  6.16s/it]Capturing batches (bs=112 avail_mem=14.78 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:48<01:50,  6.16s/it]Capturing batches (bs=112 avail_mem=14.78 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:48<00:57,  3.57s/it]Capturing batches (bs=104 avail_mem=14.76 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:48<00:57,  3.57s/it]Capturing batches (bs=96 avail_mem=14.75 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:48<00:57,  3.57s/it] Capturing batches (bs=96 avail_mem=14.75 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:48<00:31,  2.25s/it]Capturing batches (bs=88 avail_mem=14.73 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:48<00:31,  2.25s/it]Capturing batches (bs=80 avail_mem=14.72 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:48<00:31,  2.25s/it]Capturing batches (bs=80 avail_mem=14.72 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:49<00:17,  1.48s/it]Capturing batches (bs=72 avail_mem=14.71 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:49<00:17,  1.48s/it]Capturing batches (bs=64 avail_mem=14.69 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:49<00:17,  1.48s/it]Capturing batches (bs=64 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:49<00:10,  1.01s/it]Capturing batches (bs=56 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:49<00:10,  1.01s/it]Capturing batches (bs=48 avail_mem=14.66 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:49<00:10,  1.01s/it]Capturing batches (bs=48 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:49<00:05,  1.43it/s]Capturing batches (bs=40 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:49<00:05,  1.43it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:49<00:05,  1.43it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:49<00:03,  2.00it/s]Capturing batches (bs=24 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:49<00:03,  2.00it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:49<00:03,  2.00it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:49<00:01,  2.68it/s]Capturing batches (bs=8 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:49<00:01,  2.68it/s] Capturing batches (bs=4 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:49<00:01,  2.68it/s]Capturing batches (bs=4 avail_mem=14.60 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:49<00:00,  3.62it/s]Capturing batches (bs=2 avail_mem=14.59 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:49<00:00,  3.62it/s]Capturing batches (bs=1 avail_mem=14.57 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:49<00:00,  3.62it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:49<00:00,  4.75it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:49<00:00,  2.17s/it]
2025-11-26T14:34:20.538149691Z [37m20251126-14:34:20.537 SGLangServer Wrapper INFO: SGLang server launched at: http://172.18.0.2:18424[0m
2025-11-26T14:34:20.811647172Z [37m20251126-14:34:20.811 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:18424[0m
2025-11-26T14:34:20.811688772Z [37m20251126-14:34:20.811 Local Scheduler INFO: LLM inference server launched at: AREAL_LLM_SERVER_ADDRS=172.18.0.2:18424[0m
2025-11-26T14:34:20.812490410Z [37m20251126-14:34:20.812 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL AREAL_LLM_SERVER_ADDRS=172.18.0.2:18424 AREAL_RECOVER_RUN=0 NCCL_CUMEM_ENABLE=0 NCCL_NVLS_ENABLE=0 CUDA_VISIBLE_DEVICES=0 stdbuf -oL torchrun --nnodes 1 --nproc-per-node 1 --master-addr localhost --master-port 28324 examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_143241 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143241/trainer.log[0m
2025-11-26T14:34:20.813390298Z [37m20251126-14:34:20.813 Local Scheduler INFO: Waiting for 2 local running processes, pids: 887 1944[0m
2025-11-26T14:34:21.563457547Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:34:21.563491777Z   import pynvml  # type: ignore[import]
2025-11-26T14:34:23.260509736Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:34:23.260539236Z   import pynvml  # type: ignore[import]
2025-11-26T14:34:33.072542022Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:34:33.072624152Z   warnings.warn(
2025-11-26T14:34:33.072630702Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:34:33.072636342Z   warnings.warn(
2025-11-26T14:34:34.773071903Z [37m20251126-14:34:34.772 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:34:34.773129013Z [37m20251126-14:34:34.772 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:34:34.992528139Z [37m20251126-14:34:34.992 [FSDP Engine Rank 0] INFO: Initializing device mesh with parallel dims (dp=1, sp=1, tp=1, ep=1, etp=1, world_size=1).[0m
2025-11-26T14:34:34.995372542Z [37m20251126-14:34:34.995 [FSDP Engine Rank 0] INFO: Data parallel head 0 and rank 0[0m
2025-11-26T14:34:36.281955530Z Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:00<00:00, 272107.25 examples/s]
2025-11-26T14:34:36.289301492Z Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 198190.41 examples/s]
2025-11-26T14:34:36.992872056Z Map:   0%|          | 0/7473 [00:00<?, ? examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 2000/7473 [00:00<00:00, 19455.36 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4958/7473 [00:00<00:00, 25340.30 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:00<00:00, 10750.34 examples/s]
2025-11-26T14:34:38.453769401Z Filter:   0%|          | 0/7473 [00:00<?, ? examples/s]Filter:  13%|â–ˆâ–Ž        | 1000/7473 [00:00<00:01, 5134.44 examples/s]Filter:  27%|â–ˆâ–ˆâ–‹       | 2000/7473 [00:00<00:01, 5390.35 examples/s]Filter:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 3000/7473 [00:00<00:00, 5559.97 examples/s]Filter:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 4000/7473 [00:00<00:00, 5457.94 examples/s]Filter:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 5000/7473 [00:00<00:00, 5489.48 examples/s]Filter:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 6000/7473 [00:01<00:00, 5532.32 examples/s]Filter:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 7000/7473 [00:01<00:00, 5495.64 examples/s]Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:01<00:00, 5481.12 examples/s]
2025-11-26T14:34:38.899948104Z Map:   0%|          | 0/1319 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 16436.42 examples/s]
2025-11-26T14:34:39.228571019Z Filter:   0%|          | 0/1319 [00:00<?, ? examples/s]Filter:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1000/1319 [00:00<00:00, 5248.20 examples/s]Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 5283.39 examples/s]
2025-11-26T14:34:39.228870958Z [FAST] Limiting dataset from 7473 to 200 samples
2025-11-26T14:34:39.233519657Z [37m20251126-14:34:39.233 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:18424[0m
2025-11-26T14:34:39.233544807Z [37m20251126-14:34:39.233 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:34:39.233630116Z [37m20251126-14:34:39.233 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:34:40.236450278Z [37m20251126-14:34:40.236 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:34:40.239591521Z [37m20251126-14:34:40.239 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:18424[0m
2025-11-26T14:34:40.239614401Z [37m20251126-14:34:40.239 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:34:40.239693430Z [37m20251126-14:34:40.239 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:34:41.242338203Z [37m20251126-14:34:41.241 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:34:42.139791346Z [37m20251126-14:34:42.139 [FSDP Engine Rank 0] INFO: Model creation and loading time: 0.7451011310331523[0m
2025-11-26T14:34:42.256165307Z [37m20251126-14:34:42.255 [FSDP Engine Rank 0] INFO: Applying FSDP2 with N-D parallelism for 0.12 seconds[0m
2025-11-26T14:34:42.257115975Z [37m20251126-14:34:42.256 [FSDP Engine Rank 0] INFO: Create optimizer time: 0.0009621679782867432[0m
2025-11-26T14:34:42.791946638Z wandb: Currently logged in as: tong-zhao (tong-zhao-georgia-institute-of-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-11-26T14:34:42.794871061Z wandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.
2025-11-26T14:34:43.544558951Z wandb: setting up run gsm8k-grpo-cloud-fast_trial_20251126_143241_train
2025-11-26T14:34:43.846073613Z wandb: Tracking run with wandb version 0.22.2
2025-11-26T14:34:43.846140733Z wandb: Run data is saved locally in /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143241/wandb/run-20251126_143442-gsm8k-grpo-cloud-fast_trial_20251126_143241_train
2025-11-26T14:34:43.846150563Z wandb: Run `wandb offline` to turn off syncing.
2025-11-26T14:34:43.846306042Z wandb: Syncing run trial_20251126_143241
2025-11-26T14:34:43.846334162Z wandb: â­ï¸ View project at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local
2025-11-26T14:34:43.846404322Z wandb: ðŸš€ View run at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local/runs/gsm8k-grpo-cloud-fast_trial_20251126_143241_train
2025-11-26T14:34:44.175912094Z wandb: Detected [openai] in use.
2025-11-26T14:34:44.176204224Z wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-11-26T14:34:44.176446253Z wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-11-26T14:34:44.182188749Z swanlab: SwanLab run disabled, the data will not be saved or uploaded.
2025-11-26T14:34:44.796250435Z ================================================================================
2025-11-26T14:34:44.797087443Z [FAST MODE]
2025-11-26T14:34:44.797556112Z   Dataset size: 200 samples (limited from 7473)
2025-11-26T14:34:44.797967721Z   Batch size: 8
2025-11-26T14:34:44.798280120Z   Steps per epoch: 25
2025-11-26T14:34:44.798581550Z   Total epochs: 1
2025-11-26T14:34:44.798977629Z   Total steps: 25
2025-11-26T14:34:44.799329528Z   Estimated time: ~25 minutes (~0.4 hours) at ~1 step/min
2025-11-26T14:34:44.799607037Z   Circuit breaker: Enabled (threshold: 50 consecutive zero rewards)
2025-11-26T14:34:44.799802967Z ================================================================================
2025-11-26T14:34:51.066412579Z /workspace/AReaL/areal/reward/math_parser.py:290: SyntaxWarning: invalid escape sequence '\%'
2025-11-26T14:34:51.066449109Z   string = string.replace("\%", "")
2025-11-26T14:34:51.067492006Z /workspace/AReaL/areal/reward/math_parser.py:412: SyntaxWarning: invalid escape sequence '\d'
2025-11-26T14:34:51.067515726Z   pattern = "-?\d*\.?\d+"
2025-11-26T14:34:51.069689681Z /workspace/AReaL/areal/reward/math_parser.py:290: SyntaxWarning: invalid escape sequence '\%'
2025-11-26T14:34:51.069720431Z   string = string.replace("\%", "")
2025-11-26T14:34:51.071190687Z /workspace/AReaL/areal/reward/math_parser.py:412: SyntaxWarning: invalid escape sequence '\d'
2025-11-26T14:34:51.071213557Z   pattern = "-?\d*\.?\d+"
2025-11-26T14:34:51.072669113Z /workspace/AReaL/areal/reward/math_parser.py:290: SyntaxWarning: invalid escape sequence '\%'
2025-11-26T14:34:51.072708293Z   string = string.replace("\%", "")
2025-11-26T14:34:51.073976100Z /workspace/AReaL/areal/reward/math_parser.py:412: SyntaxWarning: invalid escape sequence '\d'
2025-11-26T14:34:51.073999070Z   pattern = "-?\d*\.?\d+"
2025-11-26T14:34:51.076164345Z /workspace/AReaL/areal/reward/math_parser.py:290: SyntaxWarning: invalid escape sequence '\%'
2025-11-26T14:34:51.076185494Z   string = string.replace("\%", "")
2025-11-26T14:34:51.077476861Z /workspace/AReaL/areal/reward/math_parser.py:412: SyntaxWarning: invalid escape sequence '\d'
2025-11-26T14:34:51.077494051Z   pattern = "-?\d*\.?\d+"
2025-11-26T14:34:51.081075762Z /workspace/AReaL/areal/reward/math_parser.py:290: SyntaxWarning: invalid escape sequence '\%'
2025-11-26T14:34:51.081110172Z   string = string.replace("\%", "")
2025-11-26T14:34:51.081789171Z /workspace/AReaL/areal/reward/math_parser.py:412: SyntaxWarning: invalid escape sequence '\d'
2025-11-26T14:34:51.081803120Z   pattern = "-?\d*\.?\d+"
2025-11-26T14:34:54.372222167Z [37m20251126-14:34:54.371 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4953, 486], padded to: [5120, 512], padding lengths: [167, 26][0m
2025-11-26T14:34:57.967472727Z [37m20251126-14:34:57.967 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 0.93, memory reserved (GB): 3.14, device memory used/total (GB): 69.60/79.25[0m
2025-11-26T14:34:58.126614672Z [37m20251126-14:34:58.126 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 0.93, memory reserved (GB): 3.14, device memory used/total (GB): 69.61/79.25[0m
2025-11-26T14:34:58.158103854Z [37m20251126-14:34:58.157 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4953, 486], padded to: [5120, 512], padding lengths: [167, 26][0m
2025-11-26T14:35:06.496796954Z [37m20251126-14:35:06.496 /workspace/AReaL/areal/utils/device.py INFO: ppo update, memory allocated (GB): 3.70, memory reserved (GB): 9.76, device memory used/total (GB): 76.73/79.25[0m
2025-11-26T14:35:11.903473510Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:35:12.054385456Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.64it/s]
2025-11-26T14:35:12.054422556Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.63it/s]
2025-11-26T14:35:12.080480111Z [37m20251126-14:35:12.075 [Remote Inference Engine Rank 0] INFO: Loading weights from disk done in 5.58s. Respond time: 0.99s.[0m
2025-11-26T14:35:12.101866228Z [92m20251126-14:35:12.101 StatsLogger INFO: Epoch 1/1 Step 1/25 Train step 1/25 done.[0m
2025-11-26T14:35:12.103025255Z [92m20251126-14:35:12.102 StatsLogger INFO: Stats (1/1):[0m
2025-11-26T14:35:12.138041018Z [92m20251126-14:35:12.137 StatsLogger INFO:
2025-11-26T14:35:12.138081708Z â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â••
2025-11-26T14:35:12.138089818Z â”‚ grpo_actor/actor_loss/avg        â”‚ -4.0327e-04 â”‚ grpo_actor/entropy/min             â”‚  3.0978e-07 â”‚ grpo_actor/vocab_min_logits/min  â”‚ -3.1750e+01 â”‚ grpo_actor/no_eos_ratios/min    â”‚ 0.0000e+00 â”‚
2025-11-26T14:35:12.138095888Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138101308Z â”‚ grpo_actor/actor_loss/min        â”‚ -2.2021e+00 â”‚ grpo_actor/entropy/max             â”‚  5.1693e+00 â”‚ grpo_actor/vocab_min_logits/max  â”‚ -7.5625e+00 â”‚ grpo_actor/no_eos_ratios/max    â”‚ 1.0000e+00 â”‚
2025-11-26T14:35:12.138108438Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138113848Z â”‚ grpo_actor/actor_loss/max        â”‚  2.1719e+00 â”‚ grpo_actor/grad_norm               â”‚  3.1824e+00 â”‚ grpo_actor/advantages/avg        â”‚ -7.4641e-09 â”‚ grpo_actor/prompt_len/avg       â”‚ 9.8875e+01 â”‚
2025-11-26T14:35:12.138119078Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138124378Z â”‚ grpo_actor/approx_kl/avg         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/avg   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/min        â”‚ -1.9409e+00 â”‚ grpo_actor/prompt_len/min       â”‚ 8.4000e+01 â”‚
2025-11-26T14:35:12.138129878Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138151058Z â”‚ grpo_actor/approx_kl/min         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/min   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/max        â”‚  1.9419e+00 â”‚ grpo_actor/prompt_len/max       â”‚ 1.1400e+02 â”‚
2025-11-26T14:35:12.138156888Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138162468Z â”‚ grpo_actor/approx_kl/max         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/max   â”‚  1.0000e+00 â”‚ grpo_actor/behav_imp_weight_cap  â”‚  5.0000e+00 â”‚ grpo_actor/seq_len/avg          â”‚ 3.3994e+02 â”‚
2025-11-26T14:35:12.138167998Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138173338Z â”‚ grpo_actor/behave_approx_kl/avg  â”‚ -7.2381e-04 â”‚ grpo_actor/lr                      â”‚  1.7000e-05 â”‚ grpo_actor/correct_seq_len/avg   â”‚  3.5750e+02 â”‚ grpo_actor/seq_len/min          â”‚ 1.4700e+02 â”‚
2025-11-26T14:35:12.138179058Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138184388Z â”‚ grpo_actor/behave_approx_kl/min  â”‚ -2.1250e-01 â”‚ grpo_actor/n_tokens                â”‚  5.4390e+03 â”‚ grpo_actor/correct_seq_len/min   â”‚  3.5300e+02 â”‚ grpo_actor/seq_len/max          â”‚ 3.7000e+02 â”‚
2025-11-26T14:35:12.138189558Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138194928Z â”‚ grpo_actor/behave_approx_kl/max  â”‚  2.4583e-01 â”‚ grpo_actor/n_valid_tokens          â”‚  3.8570e+03 â”‚ grpo_actor/correct_seq_len/max   â”‚  3.6200e+02 â”‚ grpo_actor/task_reward/avg      â”‚ 1.2500e-01 â”‚
2025-11-26T14:35:12.138200118Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138213428Z â”‚ grpo_actor/behave_imp_weight/avg â”‚  9.9971e-01 â”‚ grpo_actor/new_logp/avg            â”‚ -3.1593e-01 â”‚ grpo_actor/eps_clip              â”‚  4.0000e-01 â”‚ grpo_actor/task_reward/min      â”‚ 0.0000e+00 â”‚
2025-11-26T14:35:12.138219078Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138224338Z â”‚ grpo_actor/behave_imp_weight/min â”‚  8.0856e-01 â”‚ grpo_actor/new_logp/min            â”‚ -1.2834e+01 â”‚ grpo_actor/final_reward/avg      â”‚  0.0000e+00 â”‚ grpo_actor/task_reward/max      â”‚ 1.0000e+00 â”‚
2025-11-26T14:35:12.138229548Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138237788Z â”‚ grpo_actor/behave_imp_weight/max â”‚  1.2787e+00 â”‚ grpo_actor/new_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/final_reward/min      â”‚ -7.0711e-01 â”‚ grpo_actor/use_dual_clip        â”‚ 0.0000e+00 â”‚
2025-11-26T14:35:12.138243448Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138248758Z â”‚ grpo_actor/clip_ratio/avg        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/avg            â”‚ -3.1521e-01 â”‚ grpo_actor/final_reward/max      â”‚  7.0711e-01 â”‚ timeperf/compute_advantage      â”‚ 1.5884e-01 â”‚
2025-11-26T14:35:12.138253898Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138259088Z â”‚ grpo_actor/clip_ratio/min        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/min            â”‚ -1.2840e+01 â”‚ grpo_actor/incorrect_seq_len/avg â”‚  3.3743e+02 â”‚ timeperf/recompute_logp         â”‚ 3.7233e+00 â”‚
2025-11-26T14:35:12.138264218Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138277048Z â”‚ grpo_actor/clip_ratio/max        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/incorrect_seq_len/min â”‚  1.4700e+02 â”‚ timeperf/rollout                â”‚ 9.4443e+00 â”‚
2025-11-26T14:35:12.138282728Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138288018Z â”‚ grpo_actor/clipped_tokens        â”‚  0.0000e+00 â”‚ grpo_actor/unclipped_behave_tokens â”‚  3.8570e+03 â”‚ grpo_actor/incorrect_seq_len/max â”‚  3.7000e+02 â”‚ timeperf/checkpoint_for_recover â”‚ 1.4634e-04 â”‚
2025-11-26T14:35:12.138293288Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138298528Z â”‚ grpo_actor/dual_clip_ratio/avg   â”‚  0.0000e+00 â”‚ grpo_actor/update_successful       â”‚  1.0000e+00 â”‚ grpo_actor/kl_rewards/avg        â”‚  0.0000e+00 â”‚ timeperf/eval                   â”‚ 7.0590e-05 â”‚
2025-11-26T14:35:12.138304198Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138309538Z â”‚ grpo_actor/dual_clip_ratio/min   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/avg    â”‚  2.6030e+01 â”‚ grpo_actor/kl_rewards/min        â”‚  0.0000e+00 â”‚ timeperf/save                   â”‚ 9.9844e-04 â”‚
2025-11-26T14:35:12.138314778Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138320048Z â”‚ grpo_actor/dual_clip_ratio/max   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/min    â”‚  1.2938e+01 â”‚ grpo_actor/kl_rewards/max        â”‚  0.0000e+00 â”‚ timeperf/train_step             â”‚ 8.3703e+00 â”‚
2025-11-26T14:35:12.138332848Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138338198Z â”‚ grpo_actor/dual_clipped_tokens   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/max    â”‚  3.9500e+01 â”‚ grpo_actor/mask_no_eos_with_zero â”‚  0.0000e+00 â”‚ timeperf/update_weights         â”‚ 5.5913e+00 â”‚
2025-11-26T14:35:12.138343848Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:35:12.138349208Z â”‚ grpo_actor/entropy/avg           â”‚  3.0668e-01 â”‚ grpo_actor/vocab_min_logits/avg    â”‚ -1.5825e+01 â”‚ grpo_actor/no_eos_ratios/avg     â”‚  6.2500e-02 â”‚ rollout/reward                  â”‚ 2.2917e-01 â”‚
2025-11-26T14:35:12.138354388Z â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•›[0m
2025-11-26T14:35:12.291810957Z [37m20251126-14:35:12.291 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [5019, 184], padded to: [5120, 256], padding lengths: [101, 72][0m
2025-11-26T14:35:13.267129746Z [37m20251126-14:35:13.265 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 3.70, memory reserved (GB): 9.76, device memory used/total (GB): 76.65/79.25[0m
2025-11-26T14:35:15.012177685Z [37m20251126-14:35:15.011 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 3.70, memory reserved (GB): 9.76, device memory used/total (GB): 76.65/79.25[0m
2025-11-26T14:35:15.179040011Z [37m20251126-14:35:15.178 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [5019, 184], padded to: [5120, 256], padding lengths: [101, 72][0m
2025-11-26T14:35:15.694531492Z Traceback (most recent call last):
2025-11-26T14:35:15.698429982Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:35:15.698444032Z     main(sys.argv[1:])
2025-11-26T14:35:15.698756501Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:35:15.698761731Z     stats = actor.ppo_update(batch)
2025-11-26T14:35:15.698764501Z             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:15.699111761Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:35:15.699121621Z     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:35:15.699124371Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:15.699496340Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:35:15.699532019Z     train_stat = self.engine.train_batch(
2025-11-26T14:35:15.699540059Z                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:15.699944148Z   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:35:15.699964928Z     loss.backward()
2025-11-26T14:35:15.700319327Z   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:35:15.700330727Z     torch.autograd.backward(
2025-11-26T14:35:15.700679737Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:35:15.700691557Z     _engine_run_backward(
2025-11-26T14:35:15.701174715Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:35:15.701341915Z     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:35:15.701356305Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:15.701618164Z torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.08 GiB is free. Process 1209517 has 64.87 GiB memory in use. Process 1212465 has 13.29 GiB memory in use. Of the allocated memory 11.09 GiB is allocated by PyTorch, and 124.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:35:15.708196308Z [rank0]: Traceback (most recent call last):
2025-11-26T14:35:15.708218828Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:35:15.708225958Z [rank0]:     main(sys.argv[1:])
2025-11-26T14:35:15.708232298Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:35:15.708238378Z [rank0]:     stats = actor.ppo_update(batch)
2025-11-26T14:35:15.708244368Z [rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:15.708250508Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:35:15.708256178Z [rank0]:     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:35:15.708261858Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:15.708267968Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:35:15.708273908Z [rank0]:     train_stat = self.engine.train_batch(
2025-11-26T14:35:15.708279498Z [rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:15.708292018Z [rank0]:   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:35:15.708299038Z [rank0]:     loss.backward()
2025-11-26T14:35:15.708306138Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:35:15.708312968Z [rank0]:     torch.autograd.backward(
2025-11-26T14:35:15.708319818Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:35:15.708325838Z [rank0]:     _engine_run_backward(
2025-11-26T14:35:15.708331638Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:35:15.708338098Z [rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:35:15.708343728Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:15.708350858Z [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.08 GiB is free. Process 1209517 has 64.87 GiB memory in use. Process 1212465 has 13.29 GiB memory in use. Of the allocated memory 11.09 GiB is allocated by PyTorch, and 124.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:35:15.727608310Z [31m20251126-14:35:15.726 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.729084356Z [37m20251126-14:35:15.728 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:35:15.729509095Z [31m20251126-14:35:15.729 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.730368803Z [37m20251126-14:35:15.730 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:35:15.731181351Z [31m20251126-14:35:15.730 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.731582580Z [37m20251126-14:35:15.731 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:35:15.732346288Z [31m20251126-14:35:15.732 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.732683367Z [31m20251126-14:35:15.732 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:35:15.734570703Z Traceback (most recent call last):
2025-11-26T14:35:15.735749220Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:35:15.735761220Z     future = loop.run_in_executor(
2025-11-26T14:35:15.735767300Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:15.736152539Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:35:15.736482238Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:35:15.736489678Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:35:15.736813637Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:35:15.738913242Z [31m20251126-14:35:15.738 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.739283981Z [37m20251126-14:35:15.739 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:35:15.739649280Z [31m20251126-14:35:15.739 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.740019769Z [37m20251126-14:35:15.739 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:35:15.740418778Z [31m20251126-14:35:15.740 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.740706047Z [37m20251126-14:35:15.740 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:35:15.741105866Z [31m20251126-14:35:15.740 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.741426076Z [31m20251126-14:35:15.741 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:35:15.741798015Z Traceback (most recent call last):
2025-11-26T14:35:15.742272673Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:35:15.742285553Z     future = loop.run_in_executor(
2025-11-26T14:35:15.742290963Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:15.742763492Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:35:15.743138391Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:35:15.743151121Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:35:15.743500940Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:35:15.745240546Z [31m20251126-14:35:15.744 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.745604425Z [37m20251126-14:35:15.745 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:35:15.746010474Z [31m20251126-14:35:15.745 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.746367843Z [37m20251126-14:35:15.746 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:35:15.746720202Z [31m20251126-14:35:15.746 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.747092692Z [37m20251126-14:35:15.746 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:35:15.747446011Z [31m20251126-14:35:15.747 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.747784180Z [31m20251126-14:35:15.747 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:35:15.748216809Z Traceback (most recent call last):
2025-11-26T14:35:15.748703338Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:35:15.748716597Z     future = loop.run_in_executor(
2025-11-26T14:35:15.748722767Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:15.749104427Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:35:15.749478926Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:35:15.749485806Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:35:15.749844625Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:35:15.751572560Z [31m20251126-14:35:15.751 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.751904140Z [37m20251126-14:35:15.751 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:35:15.752292979Z [31m20251126-14:35:15.752 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.752615918Z [37m20251126-14:35:15.752 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:35:15.753001637Z [31m20251126-14:35:15.752 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.753340096Z [37m20251126-14:35:15.753 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:35:15.753680855Z [31m20251126-14:35:15.753 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.754042374Z [31m20251126-14:35:15.753 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:35:15.754418113Z Traceback (most recent call last):
2025-11-26T14:35:15.754889302Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:35:15.754901162Z     future = loop.run_in_executor(
2025-11-26T14:35:15.754906472Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:15.755284091Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:35:15.755624060Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:35:15.755630840Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:35:15.756032569Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:35:15.757637155Z [31m20251126-14:35:15.757 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.758007454Z [37m20251126-14:35:15.757 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:35:15.758374784Z [31m20251126-14:35:15.758 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.758711003Z [37m20251126-14:35:15.758 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:35:15.759112752Z [31m20251126-14:35:15.758 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.759473961Z [37m20251126-14:35:15.759 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:35:15.759820930Z [31m20251126-14:35:15.759 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.760165799Z [31m20251126-14:35:15.759 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:35:15.760540878Z Traceback (most recent call last):
2025-11-26T14:35:15.761035527Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:35:15.761048717Z     future = loop.run_in_executor(
2025-11-26T14:35:15.761054437Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:15.761449866Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:35:15.761803775Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:35:15.761810515Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:35:15.762207284Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:35:15.763897020Z [31m20251126-14:35:15.763 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.764291169Z [37m20251126-14:35:15.764 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:35:15.764663778Z [31m20251126-14:35:15.764 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.765067087Z [37m20251126-14:35:15.764 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:35:15.765460176Z [31m20251126-14:35:15.765 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.765813295Z [37m20251126-14:35:15.765 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:35:15.766227124Z [31m20251126-14:35:15.766 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:35:15.766587313Z [31m20251126-14:35:15.766 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:35:15.767018202Z Traceback (most recent call last):
2025-11-26T14:35:15.767572011Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:35:15.767579221Z     future = loop.run_in_executor(
2025-11-26T14:35:15.767584511Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:15.768073129Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:35:15.768472168Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:35:15.768479678Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:35:15.768905707Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:35:16.200022507Z [31m20251126-14:35:16.100 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 31 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:35:16.200048697Z Traceback (most recent call last):
2025-11-26T14:35:16.200051977Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:35:16.200055987Z     result = await async_task
2025-11-26T14:35:16.200059157Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.200061917Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:35:16.200064917Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:35:16.200068327Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.200071457Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:35:16.200075287Z     reward = await self.async_reward_fn(
2025-11-26T14:35:16.200078167Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.200081007Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:35:16.200083987Z     raise e
2025-11-26T14:35:16.200087047Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:35:16.200090107Z     future = loop.run_in_executor(
2025-11-26T14:35:16.200093037Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.200096027Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:35:16.200098917Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:35:16.200112887Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:35:16.200115757Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:35:16.200118787Z [31m20251126-14:35:16.197 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 26 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:35:16.200121817Z Traceback (most recent call last):
2025-11-26T14:35:16.200124677Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:35:16.200127637Z     result = await async_task
2025-11-26T14:35:16.200130467Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.200133297Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:35:16.200136167Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:35:16.200139207Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.200142137Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:35:16.200145097Z     reward = await self.async_reward_fn(
2025-11-26T14:35:16.200147927Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.200150757Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:35:16.200280977Z     raise e
2025-11-26T14:35:16.200299817Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:35:16.200305077Z     future = loop.run_in_executor(
2025-11-26T14:35:16.200309877Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.200313627Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:35:16.200316857Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:35:16.200320497Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:35:16.200323837Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:35:16.200328286Z [31m20251126-14:35:16.198 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 29 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:35:16.200333256Z Traceback (most recent call last):
2025-11-26T14:35:16.200336336Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:35:16.200340156Z     result = await async_task
2025-11-26T14:35:16.200343856Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.200346936Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:35:16.200350436Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:35:16.200353676Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.200356946Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:35:16.200360356Z     reward = await self.async_reward_fn(
2025-11-26T14:35:16.200363516Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.200366826Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:35:16.200369836Z     raise e
2025-11-26T14:35:16.200373066Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:35:16.200376266Z     future = loop.run_in_executor(
2025-11-26T14:35:16.200379476Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.200382706Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:35:16.200385836Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:35:16.200389206Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:35:16.200392386Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:35:16.202106352Z [31m20251126-14:35:16.201 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 25 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:35:16.202137192Z Traceback (most recent call last):
2025-11-26T14:35:16.202141152Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:35:16.202145112Z     result = await async_task
2025-11-26T14:35:16.202160622Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.202163562Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:35:16.202166442Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:35:16.202169892Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.202172802Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:35:16.202176702Z     reward = await self.async_reward_fn(
2025-11-26T14:35:16.202179412Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.202182122Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:35:16.202185292Z     raise e
2025-11-26T14:35:16.202188982Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:35:16.202191882Z     future = loop.run_in_executor(
2025-11-26T14:35:16.202194622Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.202197402Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:35:16.202200362Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:35:16.202203352Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:35:16.202206402Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:35:16.203027530Z [31m20251126-14:35:16.202 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 30 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:35:16.203041130Z Traceback (most recent call last):
2025-11-26T14:35:16.203044860Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:35:16.203048260Z     result = await async_task
2025-11-26T14:35:16.203056360Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.203059280Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:35:16.203062870Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:35:16.203066130Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.203069090Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:35:16.203072390Z     reward = await self.async_reward_fn(
2025-11-26T14:35:16.203075580Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.203078380Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:35:16.203081620Z     raise e
2025-11-26T14:35:16.203084870Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:35:16.203088010Z     future = loop.run_in_executor(
2025-11-26T14:35:16.203091030Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.203094270Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:35:16.203097520Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:35:16.203100600Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:35:16.203103840Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:35:16.206558841Z [31m20251126-14:35:16.205 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 28 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:35:16.206586851Z Traceback (most recent call last):
2025-11-26T14:35:16.206590291Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:35:16.206593631Z     result = await async_task
2025-11-26T14:35:16.206596841Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.206600001Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:35:16.206603281Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:35:16.206606451Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.206609671Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:35:16.206612911Z     reward = await self.async_reward_fn(
2025-11-26T14:35:16.206616101Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.206619141Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:35:16.206639411Z     raise e
2025-11-26T14:35:16.206642681Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:35:16.206645961Z     future = loop.run_in_executor(
2025-11-26T14:35:16.206649091Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:16.206652291Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:35:16.206655481Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:35:16.206658571Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:35:16.206661641Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:35:17.841640675Z [1;34mwandb[0m:
2025-11-26T14:35:17.841678745Z [1;34mwandb[0m: ðŸš€ View run [33mtrial_20251126_143241[0m at: [34m[0m
2025-11-26T14:35:17.841686205Z [1;34mwandb[0m: Find logs at: [1;35m../outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143241/wandb/run-20251126_143442-gsm8k-grpo-cloud-fast_trial_20251126_143241_train/logs[0m
2025-11-26T14:35:20.351998476Z [rank0]:[W1126 14:35:20.477809685 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
2025-11-26T14:35:23.517219343Z E1126 14:35:23.515000 1945 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 2011) of binary: /usr/bin/python3
2025-11-26T14:35:23.518163090Z Traceback (most recent call last):
2025-11-26T14:35:23.518187390Z   File "/usr/local/bin/torchrun", line 7, in <module>
2025-11-26T14:35:23.518267790Z     sys.exit(main())
2025-11-26T14:35:23.518296730Z              ^^^^^^
2025-11-26T14:35:23.518306340Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
2025-11-26T14:35:23.518732799Z     return f(*args, **kwargs)
2025-11-26T14:35:23.518747889Z            ^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:23.518754199Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 901, in main
2025-11-26T14:35:23.519237247Z     run(args)
2025-11-26T14:35:23.519261137Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 892, in run
2025-11-26T14:35:23.519748646Z     elastic_launch(
2025-11-26T14:35:23.519763356Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 143, in __call__
2025-11-26T14:35:23.519823066Z     return launch_agent(self._config, self._entrypoint, list(args))
2025-11-26T14:35:23.519831836Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:35:23.519837706Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
2025-11-26T14:35:23.520288235Z     raise ChildFailedError(
2025-11-26T14:35:23.520344765Z torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
2025-11-26T14:35:23.520351835Z ============================================================
2025-11-26T14:35:23.520358625Z examples/cloud_gsm8k/gsm8k_grpo_train.py FAILED
2025-11-26T14:35:23.520365165Z ------------------------------------------------------------
2025-11-26T14:35:23.520371075Z Failures:
2025-11-26T14:35:23.520378065Z   <NO_OTHER_FAILURES>
2025-11-26T14:35:23.520384585Z ------------------------------------------------------------
2025-11-26T14:35:23.520390155Z Root Cause (first observed failure):
2025-11-26T14:35:23.520395875Z [0]:
2025-11-26T14:35:23.520401395Z   time      : 2025-11-26_14:35:23
2025-11-26T14:35:23.520407005Z   host      : 01af0063c46e
2025-11-26T14:35:23.520412565Z   rank      : 0 (local_rank: 0)
2025-11-26T14:35:23.520418005Z   exitcode  : 1 (pid: 2011)
2025-11-26T14:35:23.520423635Z   error_file: <N/A>
2025-11-26T14:35:23.520429165Z   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
2025-11-26T14:35:23.520435674Z ============================================================
2025-11-26T14:35:24.901531747Z [37m20251126-14:35:24.901 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [887][0m
2025-11-26T14:35:25.125607241Z Killed
2025-11-26T14:35:25.130145750Z [37m20251126-14:35:25.129 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [1944][0m
2025-11-26T14:35:25.131245047Z Traceback (most recent call last):
2025-11-26T14:35:25.131255337Z   File "<frozen runpy>", line 198, in _run_module_as_main
2025-11-26T14:35:25.131259927Z   File "<frozen runpy>", line 88, in _run_code
2025-11-26T14:35:25.131262847Z   File "/workspace/AReaL/areal/launcher/local.py", line 405, in <module>
2025-11-26T14:35:25.131403666Z     main()
2025-11-26T14:35:25.131442586Z   File "/workspace/AReaL/areal/launcher/local.py", line 260, in main
2025-11-26T14:35:25.131507666Z     local_main(config, run_id=0)
2025-11-26T14:35:25.131574756Z   File "/workspace/AReaL/areal/launcher/local.py", line 399, in local_main
2025-11-26T14:35:25.131631786Z     raise e
2025-11-26T14:35:25.131648026Z   File "/workspace/AReaL/areal/launcher/local.py", line 375, in local_main
2025-11-26T14:35:25.131732976Z     launcher.wait(
2025-11-26T14:35:25.131739716Z   File "/workspace/AReaL/areal/launcher/local.py", line 235, in wait
2025-11-26T14:35:25.131804775Z     raise JobException(
2025-11-26T14:35:25.131810495Z areal.utils.launcher.JobException: Job gsm8k-grpo-cloud-fast_trial_20251126_143241:trainer JobState.COMPLETED at node local
2025-11-26T14:35:25.375099402Z [37m20251126-14:35:25.374 Local Scheduler INFO: Waiting for 0 local running processes, pids: [0m
2025-11-26T14:35:43.020011143Z ==========
2025-11-26T14:35:43.020020373Z == CUDA ==
2025-11-26T14:35:43.020047853Z ==========
2025-11-26T14:35:43.027319195Z CUDA Version 12.9.1
2025-11-26T14:35:43.030432387Z Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2025-11-26T14:35:43.032810691Z This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2025-11-26T14:35:43.032820621Z By pulling and using the container, you accept the terms and conditions of this license:
2025-11-26T14:35:43.032827091Z https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2025-11-26T14:35:43.032838891Z A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
2025-11-26T14:35:43.257794683Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:35:43.461264158Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:35:44.240424985Z Branch 'feature/reasoning_model' set up to track remote branch 'feature/reasoning_model' from 'origin'.
2025-11-26T14:35:44.240460595Z Your branch is up to date with 'origin/feature/reasoning_model'.
2025-11-26T14:35:44.349440534Z Checking AReaL installation...
2025-11-26T14:35:44.440153459Z AReaL already installed. Skipping installation.
2025-11-26T14:35:44.440186129Z Cleaning up any leftover GPU processes...
2025-11-26T14:35:47.471810608Z Checking for processes holding GPU device files...
2025-11-26T14:35:48.164428399Z Found processes holding GPU devices: 1
2025-11-26T14:35:48.164497639Z 143
2025-11-26T14:35:48.164503519Z 144
2025-11-26T14:35:48.164506099Z 92
2025-11-26T14:35:48.164508669Z Killing process 1...
2025-11-26T14:35:48.164513849Z Killing process 143...
2025-11-26T14:35:48.164672849Z Killing process 144...
2025-11-26T14:35:50.169177285Z Using fuser to kill processes on GPU devices...
2025-11-26T14:35:52.194985909Z Checking GPU...
2025-11-26T14:35:52.234106262Z NVIDIA A100 80GB PCIe, 81920 MiB
2025-11-26T14:35:52.261538414Z Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True for better memory management
2025-11-26T14:35:52.261588674Z Detected 1 GPU(s)
2025-11-26T14:35:52.261593684Z Checking GPU status...
2025-11-26T14:35:52.292959206Z index, name, utilization.gpu [%], memory.used [MiB], compute_mode
2025-11-26T14:35:52.300242318Z 0, NVIDIA A100 80GB PCIe, 0, 0, Default
2025-11-26T14:35:52.353186097Z Verifying GPU accessibility...
2025-11-26T14:35:53.148515913Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:35:53.148604033Z   import pynvml  # type: ignore[import]
2025-11-26T14:35:54.363961187Z GPU accessibility verified on attempt 1
2025-11-26T14:35:54.912336667Z Starting training...
2025-11-26T14:35:57.916088465Z Using FAST training configuration (20-30 minutes)
2025-11-26T14:35:57.920859193Z ==========================================
2025-11-26T14:35:57.920864183Z Starting GRPO Training (Cloud)
2025-11-26T14:35:57.920869023Z ==========================================
2025-11-26T14:35:57.920873493Z Config: fast
2025-11-26T14:35:57.920938503Z Config file: examples/cloud_gsm8k/gsm8k_grpo_fast.yaml
2025-11-26T14:35:57.920943383Z Training script: examples/cloud_gsm8k/gsm8k_grpo_train.py
2025-11-26T14:35:57.920947183Z Experiment: gsm8k-grpo-cloud-fast
2025-11-26T14:35:57.920951533Z Trial: trial_20251126_143557
2025-11-26T14:35:57.920955783Z GPU: NVIDIA A100 80GB PCIe (81920 MB)
2025-11-26T14:35:57.921008802Z WandB API key: e1adc5be02...
2025-11-26T14:35:57.921040952Z ==========================================
2025-11-26T14:35:58.950631188Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:35:58.950690128Z   import pynvml  # type: ignore[import]
2025-11-26T14:36:04.169104511Z [37m20251126-14:36:04.168 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:36:04.169162710Z [37m20251126-14:36:04.168 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:36:04.169580979Z [37m20251126-14:36:04.169 name-resolve INFO: No such name resolve path: ./tmp/areal/name_resolve/root/gsm8k-grpo-cloud-fast/trial_20251126_143557[0m
2025-11-26T14:36:04.332849194Z [37m20251126-14:36:04.332 Local Scheduler INFO: LocalLauncher: experiment_name=gsm8k-grpo-cloud-fast, trial_name=trial_20251126_143557, fileroot=/workspace/outputs/grpo, run_id=0, is_recover_run=False[0m
2025-11-26T14:36:04.340746355Z [37m20251126-14:36:04.340 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL CUDA_VISIBLE_DEVICES=0 stdbuf -oL python3 -m areal.launcher.sglang_server examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_143557 sglang.random_seed=1 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143557/llm_server.log[0m
2025-11-26T14:36:05.354093361Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:36:05.354124851Z   import pynvml  # type: ignore[import]
2025-11-26T14:36:07.302050018Z [37m20251126-14:36:07.301 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:36:07.302105197Z [37m20251126-14:36:07.301 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:36:07.452584494Z [37m20251126-14:36:07.452 SGLangServer Wrapper INFO: Launch command: python3 -m sglang.launch_server --host 172.18.0.2 --port 33140 --tokenizer-path Qwen/Qwen2.5-0.5B-Instruct --tokenizer-mode auto --load-format auto --trust-remote-code --device cuda --tp-size 1 --base-gpu-id 0 --nnodes 1 --node-rank 0 --dist-init-addr localhost:46549 --model-path Qwen/Qwen2.5-0.5B-Instruct --random-seed 1 --skip-tokenizer-init --disable-radix-cache --torch-compile-max-bs 32 --triton-attention-num-kv-splits 8 --num-continuous-decode-steps 1 --attention-backend flashinfer --context-length 4096 --mem-fraction-static 0.8 --chunked-prefill-size -1 --max-prefill-tokens 32768 --schedule-policy lpm --schedule-conservativeness 1.0 --cpu-offload-gb 0 --dtype bfloat16 --kv-cache-dtype auto --dp-size 1 --ep-size 1 --max-loaded-loras 1 --max-loras-per-batch 1 --lora-backend triton --log-level warning --log-level-http warning --log-requests-level 0 --enable-metrics --decode-log-interval 1[0m
2025-11-26T14:36:08.927118384Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:36:08.927153134Z   import pynvml  # type: ignore[import]
2025-11-26T14:36:16.140651417Z INFO 11-26 14:36:16 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:36:16.985405961Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:36:18.371434092Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:36:18.371476872Z   import pynvml  # type: ignore[import]
2025-11-26T14:36:18.371615682Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:36:18.371623932Z   import pynvml  # type: ignore[import]
2025-11-26T14:36:25.321155627Z INFO 11-26 14:36:25 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:36:25.391438480Z INFO 11-26 14:36:25 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:36:26.075292050Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:36:26.352330681Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:36:26.357114941Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:36:26.357979199Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:36:26.358858477Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:36:26.405346730Z [2025-11-26 14:36:26] MOE_RUNNER_BACKEND is not initialized, using triton backend
2025-11-26T14:36:27.032663958Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:36:27.032697758Z   warnings.warn(
2025-11-26T14:36:27.032705108Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:36:27.032711918Z   warnings.warn(
2025-11-26T14:36:28.376335338Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:36:28.557598149Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.53it/s]
2025-11-26T14:36:28.557633469Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.52it/s]
2025-11-26T14:36:32.175064923Z   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   4%|â–         | 1/23 [00:00<00:16,  1.32it/s]Capturing batches (bs=152 avail_mem=14.84 GB):   4%|â–         | 1/23 [00:00<00:16,  1.32it/s]Capturing batches (bs=144 avail_mem=14.82 GB):   4%|â–         | 1/23 [00:00<00:16,  1.32it/s]Capturing batches (bs=144 avail_mem=14.82 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.06it/s]Capturing batches (bs=136 avail_mem=14.81 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.06it/s]Capturing batches (bs=128 avail_mem=14.79 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.06it/s]Capturing batches (bs=128 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:01<00:02,  6.50it/s]Capturing batches (bs=120 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:01<00:02,  6.50it/s]Capturing batches (bs=112 avail_mem=14.78 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:01<00:02,  6.50it/s]Capturing batches (bs=112 avail_mem=14.78 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  8.57it/s]Capturing batches (bs=104 avail_mem=14.76 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  8.57it/s]Capturing batches (bs=96 avail_mem=14.75 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  8.57it/s] Capturing batches (bs=96 avail_mem=14.75 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 10.21it/s]Capturing batches (bs=88 avail_mem=14.73 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 10.21it/s]Capturing batches (bs=80 avail_mem=14.72 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 10.21it/s]Capturing batches (bs=80 avail_mem=14.72 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:01, 11.54it/s]Capturing batches (bs=72 avail_mem=14.71 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:01, 11.54it/s]Capturing batches (bs=64 avail_mem=14.69 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:01, 11.54it/s]Capturing batches (bs=64 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 12.72it/s]Capturing batches (bs=56 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 12.72it/s]Capturing batches (bs=48 avail_mem=14.66 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 12.72it/s]Capturing batches (bs=48 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 13.75it/s]Capturing batches (bs=40 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 13.75it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 13.75it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 13.96it/s]Capturing batches (bs=24 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 13.96it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 13.96it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 12.97it/s]Capturing batches (bs=8 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 12.97it/s] Capturing batches (bs=4 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:02<00:00, 12.97it/s]Capturing batches (bs=4 avail_mem=14.60 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:02<00:00, 14.07it/s]Capturing batches (bs=2 avail_mem=14.59 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:02<00:00, 14.07it/s]Capturing batches (bs=1 avail_mem=14.57 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:02<00:00, 14.07it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 14.15it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 10.27it/s]
2025-11-26T14:36:37.519387526Z [37m20251126-14:36:37.518 SGLangServer Wrapper INFO: SGLang server launched at: http://172.18.0.2:33140[0m
2025-11-26T14:36:38.349933329Z [37m20251126-14:36:38.349 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:33140[0m
2025-11-26T14:36:38.349997079Z [37m20251126-14:36:38.349 Local Scheduler INFO: LLM inference server launched at: AREAL_LLM_SERVER_ADDRS=172.18.0.2:33140[0m
2025-11-26T14:36:38.350655528Z [37m20251126-14:36:38.350 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL AREAL_LLM_SERVER_ADDRS=172.18.0.2:33140 AREAL_RECOVER_RUN=0 NCCL_CUMEM_ENABLE=0 NCCL_NVLS_ENABLE=0 CUDA_VISIBLE_DEVICES=0 stdbuf -oL torchrun --nnodes 1 --nproc-per-node 1 --master-addr localhost --master-port 21231 examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_143557 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143557/trainer.log[0m
2025-11-26T14:36:38.351585496Z [37m20251126-14:36:38.351 Local Scheduler INFO: Waiting for 2 local running processes, pids: 386 1036[0m
2025-11-26T14:36:39.144394588Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:36:39.144427508Z   import pynvml  # type: ignore[import]
2025-11-26T14:36:40.845955089Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:36:40.845995839Z   import pynvml  # type: ignore[import]
2025-11-26T14:36:49.822361406Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:36:49.822431426Z   warnings.warn(
2025-11-26T14:36:49.822437396Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:36:49.822443796Z   warnings.warn(
2025-11-26T14:36:51.399757747Z [37m20251126-14:36:51.399 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:36:51.399801197Z [37m20251126-14:36:51.399 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:36:51.610241407Z [37m20251126-14:36:51.609 [FSDP Engine Rank 0] INFO: Initializing device mesh with parallel dims (dp=1, sp=1, tp=1, ep=1, etp=1, world_size=1).[0m
2025-11-26T14:36:51.613377731Z [37m20251126-14:36:51.612 [FSDP Engine Rank 0] INFO: Data parallel head 0 and rank 0[0m
2025-11-26T14:36:52.750042053Z [FAST] Limiting dataset from 7473 to 200 samples
2025-11-26T14:36:52.753941625Z [37m20251126-14:36:52.753 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:33140[0m
2025-11-26T14:36:52.753966135Z [37m20251126-14:36:52.753 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:36:52.753973205Z [37m20251126-14:36:52.753 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:36:53.756997817Z [37m20251126-14:36:53.756 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:36:53.760060111Z [37m20251126-14:36:53.759 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:33140[0m
2025-11-26T14:36:53.760101811Z [37m20251126-14:36:53.759 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:36:53.760117871Z [37m20251126-14:36:53.759 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:36:54.763286373Z [37m20251126-14:36:54.762 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:36:55.504987162Z [37m20251126-14:36:55.504 [FSDP Engine Rank 0] INFO: Model creation and loading time: 0.5808598950970918[0m
2025-11-26T14:36:56.042828957Z [37m20251126-14:36:56.042 [FSDP Engine Rank 0] INFO: Applying FSDP2 with N-D parallelism for 0.54 seconds[0m
2025-11-26T14:36:56.043762605Z [37m20251126-14:36:56.043 [FSDP Engine Rank 0] INFO: Create optimizer time: 0.0009226480033248663[0m
2025-11-26T14:36:56.651856303Z wandb: Currently logged in as: tong-zhao (tong-zhao-georgia-institute-of-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-11-26T14:36:56.654967377Z wandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.
2025-11-26T14:36:57.496633126Z wandb: setting up run gsm8k-grpo-cloud-fast_trial_20251126_143557_train
2025-11-26T14:36:57.565226323Z wandb: Tracking run with wandb version 0.22.2
2025-11-26T14:36:57.565294343Z wandb: Run data is saved locally in /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143557/wandb/run-20251126_143656-gsm8k-grpo-cloud-fast_trial_20251126_143557_train
2025-11-26T14:36:57.565311193Z wandb: Run `wandb offline` to turn off syncing.
2025-11-26T14:36:57.565424113Z wandb: Syncing run trial_20251126_143557
2025-11-26T14:36:57.565481282Z wandb: â­ï¸ View project at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local
2025-11-26T14:36:57.565584522Z wandb: ðŸš€ View run at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local/runs/gsm8k-grpo-cloud-fast_trial_20251126_143557_train
2025-11-26T14:36:57.903412126Z wandb: Detected [openai] in use.
2025-11-26T14:36:57.903582995Z wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-11-26T14:36:57.903739925Z wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-11-26T14:36:57.908646095Z swanlab: SwanLab run disabled, the data will not be saved or uploaded.
2025-11-26T14:36:58.519783147Z ================================================================================
2025-11-26T14:36:58.520293876Z [FAST MODE]
2025-11-26T14:36:58.520766815Z   Dataset size: 200 samples (limited from 7473)
2025-11-26T14:36:58.521156694Z   Batch size: 8
2025-11-26T14:36:58.521553243Z   Steps per epoch: 25
2025-11-26T14:36:58.521924432Z   Total epochs: 1
2025-11-26T14:36:58.522365711Z   Total steps: 25
2025-11-26T14:36:58.522918650Z   Estimated time: ~25 minutes (~0.4 hours) at ~1 step/min
2025-11-26T14:36:58.523461569Z   Circuit breaker: Enabled (threshold: 50 consecutive zero rewards)
2025-11-26T14:36:58.523731348Z ================================================================================
2025-11-26T14:37:06.087961609Z [37m20251126-14:37:06.087 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4987, 526], padded to: [5120, 768], padding lengths: [133, 242][0m
2025-11-26T14:37:07.460051539Z [37m20251126-14:37:07.459 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 0.93, memory reserved (GB): 3.22, device memory used/total (GB): 69.68/79.25[0m
2025-11-26T14:37:07.617267840Z [37m20251126-14:37:07.616 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 0.93, memory reserved (GB): 3.22, device memory used/total (GB): 69.68/79.25[0m
2025-11-26T14:37:07.649053614Z [37m20251126-14:37:07.648 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4987, 526], padded to: [5120, 768], padding lengths: [133, 242][0m
2025-11-26T14:37:09.336342725Z [37m20251126-14:37:09.335 /workspace/AReaL/areal/utils/device.py INFO: ppo update, memory allocated (GB): 3.70, memory reserved (GB): 9.64, device memory used/total (GB): 76.61/79.25[0m
2025-11-26T14:37:12.584940031Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:37:12.728542021Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.97it/s]
2025-11-26T14:37:12.728576741Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.96it/s]
2025-11-26T14:37:12.755471144Z [37m20251126-14:37:12.754 [Remote Inference Engine Rank 0] INFO: Loading weights from disk done in 3.42s. Respond time: 0.14s.[0m
2025-11-26T14:37:12.765737523Z [92m20251126-14:37:12.764 StatsLogger INFO: Epoch 1/1 Step 1/25 Train step 1/25 done.[0m
2025-11-26T14:37:12.766239752Z [92m20251126-14:37:12.765 StatsLogger INFO: Stats (1/1):[0m
2025-11-26T14:37:12.778869335Z [92m20251126-14:37:12.778 StatsLogger INFO:
2025-11-26T14:37:12.778946495Z â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â••
2025-11-26T14:37:12.778953425Z â”‚ grpo_actor/actor_loss/avg        â”‚  3.3157e-04 â”‚ grpo_actor/entropy/min             â”‚  2.2507e-07 â”‚ grpo_actor/vocab_min_logits/min  â”‚ -3.1500e+01 â”‚ grpo_actor/no_eos_ratios/min    â”‚ 0.0000e+00 â”‚
2025-11-26T14:37:12.778960665Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.778966515Z â”‚ grpo_actor/actor_loss/min        â”‚ -3.2512e+00 â”‚ grpo_actor/entropy/max             â”‚  5.6844e+00 â”‚ grpo_actor/vocab_min_logits/max  â”‚ -7.5625e+00 â”‚ grpo_actor/no_eos_ratios/max    â”‚ 1.0000e+00 â”‚
2025-11-26T14:37:12.778976525Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.778982005Z â”‚ grpo_actor/actor_loss/max        â”‚  3.4870e+00 â”‚ grpo_actor/grad_norm               â”‚  3.5072e+00 â”‚ grpo_actor/advantages/avg        â”‚  0.0000e+00 â”‚ grpo_actor/prompt_len/avg       â”‚ 9.8875e+01 â”‚
2025-11-26T14:37:12.778987225Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779008505Z â”‚ grpo_actor/approx_kl/avg         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/avg   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/min        â”‚ -2.8252e+00 â”‚ grpo_actor/prompt_len/min       â”‚ 8.4000e+01 â”‚
2025-11-26T14:37:12.779014275Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779019475Z â”‚ grpo_actor/approx_kl/min         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/min   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/max        â”‚  2.8629e+00 â”‚ grpo_actor/prompt_len/max       â”‚ 1.1400e+02 â”‚
2025-11-26T14:37:12.779025105Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779030475Z â”‚ grpo_actor/approx_kl/max         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/max   â”‚  1.0000e+00 â”‚ grpo_actor/behav_imp_weight_cap  â”‚  5.0000e+00 â”‚ grpo_actor/seq_len/avg          â”‚ 3.4456e+02 â”‚
2025-11-26T14:37:12.779035635Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779041335Z â”‚ grpo_actor/behave_approx_kl/avg  â”‚ -9.3100e-04 â”‚ grpo_actor/lr                      â”‚  1.7000e-05 â”‚ grpo_actor/correct_seq_len/avg   â”‚  3.1400e+02 â”‚ grpo_actor/seq_len/min          â”‚ 2.1200e+02 â”‚
2025-11-26T14:37:12.779047285Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779052525Z â”‚ grpo_actor/behave_approx_kl/min  â”‚ -6.4797e-01 â”‚ grpo_actor/n_tokens                â”‚  5.5130e+03 â”‚ grpo_actor/correct_seq_len/min   â”‚  3.1400e+02 â”‚ grpo_actor/seq_len/max          â”‚ 3.7000e+02 â”‚
2025-11-26T14:37:12.779063835Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779072465Z â”‚ grpo_actor/behave_approx_kl/max  â”‚  2.5610e-01 â”‚ grpo_actor/n_valid_tokens          â”‚  3.9310e+03 â”‚ grpo_actor/correct_seq_len/max   â”‚  3.1400e+02 â”‚ grpo_actor/task_reward/avg      â”‚ 6.2500e-02 â”‚
2025-11-26T14:37:12.779077685Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779082955Z â”‚ grpo_actor/behave_imp_weight/avg â”‚  9.9963e-01 â”‚ grpo_actor/new_logp/avg            â”‚ -3.8612e-01 â”‚ grpo_actor/eps_clip              â”‚  4.0000e-01 â”‚ grpo_actor/task_reward/min      â”‚ 0.0000e+00 â”‚
2025-11-26T14:37:12.779088535Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779093765Z â”‚ grpo_actor/behave_imp_weight/min â”‚  5.2311e-01 â”‚ grpo_actor/new_logp/min            â”‚ -1.4765e+01 â”‚ grpo_actor/final_reward/avg      â”‚  0.0000e+00 â”‚ grpo_actor/task_reward/max      â”‚ 1.0000e+00 â”‚
2025-11-26T14:37:12.779098955Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779104165Z â”‚ grpo_actor/behave_imp_weight/max â”‚  1.2919e+00 â”‚ grpo_actor/new_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/final_reward/min      â”‚ -7.0711e-01 â”‚ grpo_actor/use_dual_clip        â”‚ 0.0000e+00 â”‚
2025-11-26T14:37:12.779109685Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779120305Z â”‚ grpo_actor/clip_ratio/avg        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/avg            â”‚ -3.8519e-01 â”‚ grpo_actor/final_reward/max      â”‚  7.0711e-01 â”‚ timeperf/compute_advantage      â”‚ 1.5713e-01 â”‚
2025-11-26T14:37:12.779125715Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779130985Z â”‚ grpo_actor/clip_ratio/min        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/min            â”‚ -1.4703e+01 â”‚ grpo_actor/incorrect_seq_len/avg â”‚  3.4660e+02 â”‚ timeperf/recompute_logp         â”‚ 1.4788e+00 â”‚
2025-11-26T14:37:12.779139885Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779145295Z â”‚ grpo_actor/clip_ratio/max        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/incorrect_seq_len/min â”‚  2.1200e+02 â”‚ timeperf/rollout                â”‚ 7.4572e+00 â”‚
2025-11-26T14:37:12.779150925Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779156265Z â”‚ grpo_actor/clipped_tokens        â”‚  0.0000e+00 â”‚ grpo_actor/unclipped_behave_tokens â”‚  3.9310e+03 â”‚ grpo_actor/incorrect_seq_len/max â”‚  3.7000e+02 â”‚ timeperf/checkpoint_for_recover â”‚ 2.8650e-05 â”‚
2025-11-26T14:37:12.779161535Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779166705Z â”‚ grpo_actor/dual_clip_ratio/avg   â”‚  0.0000e+00 â”‚ grpo_actor/update_successful       â”‚  1.0000e+00 â”‚ grpo_actor/kl_rewards/avg        â”‚  0.0000e+00 â”‚ timeperf/eval                   â”‚ 6.4739e-05 â”‚
2025-11-26T14:37:12.779172575Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779183535Z â”‚ grpo_actor/dual_clip_ratio/min   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/avg    â”‚  2.5846e+01 â”‚ grpo_actor/kl_rewards/min        â”‚  0.0000e+00 â”‚ timeperf/save                   â”‚ 1.0781e-04 â”‚
2025-11-26T14:37:12.779188945Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779194155Z â”‚ grpo_actor/dual_clip_ratio/max   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/min    â”‚  1.2938e+01 â”‚ grpo_actor/kl_rewards/max        â”‚  0.0000e+00 â”‚ timeperf/train_step             â”‚ 1.7192e+00 â”‚
2025-11-26T14:37:12.779199295Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779236695Z â”‚ grpo_actor/dual_clipped_tokens   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/max    â”‚  3.9750e+01 â”‚ grpo_actor/mask_no_eos_with_zero â”‚  0.0000e+00 â”‚ timeperf/update_weights         â”‚ 3.4206e+00 â”‚
2025-11-26T14:37:12.779242575Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:37:12.779249165Z â”‚ grpo_actor/entropy/avg           â”‚  3.7258e-01 â”‚ grpo_actor/vocab_min_logits/avg    â”‚ -1.5692e+01 â”‚ grpo_actor/no_eos_ratios/avg     â”‚  1.2500e-01 â”‚ rollout/reward                  â”‚ 1.2500e-01 â”‚
2025-11-26T14:37:12.779254365Z â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•›[0m
2025-11-26T14:37:12.829126900Z [37m20251126-14:37:12.828 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4883], padded to: [5120], padding lengths: [237][0m
2025-11-26T14:37:13.297420491Z [37m20251126-14:37:13.296 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 3.70, memory reserved (GB): 9.65, device memory used/total (GB): 76.54/79.25[0m
2025-11-26T14:37:15.156276813Z [37m20251126-14:37:15.154 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 3.70, memory reserved (GB): 9.65, device memory used/total (GB): 76.58/79.25[0m
2025-11-26T14:37:15.320724299Z [37m20251126-14:37:15.320 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4883], padded to: [5120], padding lengths: [237][0m
2025-11-26T14:37:16.061770670Z Traceback (most recent call last):
2025-11-26T14:37:16.062557788Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:37:16.062568868Z     main(sys.argv[1:])
2025-11-26T14:37:16.063089947Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:37:16.063117597Z     stats = actor.ppo_update(batch)
2025-11-26T14:37:16.063125137Z             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.063695236Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:37:16.063707746Z     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:37:16.063715806Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.064128385Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:37:16.064150675Z     train_stat = self.engine.train_batch(
2025-11-26T14:37:16.064156925Z                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.064475764Z   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:37:16.064485554Z     loss.backward()
2025-11-26T14:37:16.064792943Z   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:37:16.064802823Z     torch.autograd.backward(
2025-11-26T14:37:16.065181572Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:37:16.065199582Z     _engine_run_backward(
2025-11-26T14:37:16.065508802Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:37:16.065523452Z     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:37:16.065531502Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.065980741Z torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.38 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.16 GiB is free. Process 1215525 has 64.85 GiB memory in use. Process 1216134 has 13.23 GiB memory in use. Of the allocated memory 11.05 GiB is allocated by PyTorch, and 103.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:37:16.069095904Z [rank0]: Traceback (most recent call last):
2025-11-26T14:37:16.069120974Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:37:16.069128194Z [rank0]:     main(sys.argv[1:])
2025-11-26T14:37:16.069134814Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:37:16.069141674Z [rank0]:     stats = actor.ppo_update(batch)
2025-11-26T14:37:16.069146894Z [rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.069152034Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:37:16.069157204Z [rank0]:     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:37:16.069162574Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.069167694Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:37:16.069172834Z [rank0]:     train_stat = self.engine.train_batch(
2025-11-26T14:37:16.069177934Z [rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.069183144Z [rank0]:   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:37:16.069188324Z [rank0]:     loss.backward()
2025-11-26T14:37:16.069217694Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:37:16.069225934Z [rank0]:     torch.autograd.backward(
2025-11-26T14:37:16.069231154Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:37:16.069236254Z [rank0]:     _engine_run_backward(
2025-11-26T14:37:16.069241444Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:37:16.069246964Z [rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:37:16.069252434Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.069258694Z [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.38 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.16 GiB is free. Process 1215525 has 64.85 GiB memory in use. Process 1216134 has 13.23 GiB memory in use. Of the allocated memory 11.05 GiB is allocated by PyTorch, and 103.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:37:16.118135582Z [31m20251126-14:37:16.117 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.118667431Z [37m20251126-14:37:16.118 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:37:16.119186430Z [31m20251126-14:37:16.118 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.119628419Z [37m20251126-14:37:16.119 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:37:16.120125468Z [31m20251126-14:37:16.119 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.120520647Z [37m20251126-14:37:16.120 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:37:16.120993296Z [31m20251126-14:37:16.120 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.121421215Z [31m20251126-14:37:16.121 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:37:16.122672192Z Traceback (most recent call last):
2025-11-26T14:37:16.123496841Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:37:16.123508211Z     future = loop.run_in_executor(
2025-11-26T14:37:16.123514380Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.124119249Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:37:16.124641658Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:37:16.124649238Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:37:16.125176677Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:37:16.127485392Z [31m20251126-14:37:16.125 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 24 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:37:16.127498752Z Traceback (most recent call last):
2025-11-26T14:37:16.127504572Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:37:16.127510442Z     result = await async_task
2025-11-26T14:37:16.127516282Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.127521492Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:37:16.127526752Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:37:16.127532742Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.127538352Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:37:16.127543542Z     reward = await self.async_reward_fn(
2025-11-26T14:37:16.127565182Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.127570482Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:37:16.127575942Z     raise e
2025-11-26T14:37:16.127581272Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:37:16.127586382Z     future = loop.run_in_executor(
2025-11-26T14:37:16.127591512Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.127596742Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:37:16.127601842Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:37:16.127607072Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:37:16.127612212Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:37:16.165989442Z [31m20251126-14:37:16.165 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.166464891Z [37m20251126-14:37:16.166 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:37:16.166856150Z [31m20251126-14:37:16.166 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.167294679Z [37m20251126-14:37:16.167 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:37:16.167777948Z [31m20251126-14:37:16.167 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.168217037Z [37m20251126-14:37:16.167 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:37:16.168639496Z [31m20251126-14:37:16.168 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.169062105Z [31m20251126-14:37:16.168 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:37:16.169463584Z Traceback (most recent call last):
2025-11-26T14:37:16.169894283Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:37:16.169906763Z     future = loop.run_in_executor(
2025-11-26T14:37:16.169912573Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.170410132Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:37:16.170842271Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:37:16.170855641Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:37:16.171290901Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:37:16.332053914Z [31m20251126-14:37:16.331 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.332608183Z [37m20251126-14:37:16.332 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:37:16.333132302Z [31m20251126-14:37:16.332 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.333583681Z [37m20251126-14:37:16.333 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:37:16.334169850Z [31m20251126-14:37:16.333 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.334637239Z [37m20251126-14:37:16.334 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:37:16.335138328Z [31m20251126-14:37:16.334 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.335546527Z [31m20251126-14:37:16.335 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:37:16.336866594Z Traceback (most recent call last):
2025-11-26T14:37:16.337691143Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:37:16.337710382Z     future = loop.run_in_executor(
2025-11-26T14:37:16.337718012Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.338411461Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:37:16.339143420Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:37:16.339175979Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:37:16.339745168Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:37:16.341450825Z [31m20251126-14:37:16.341 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.341965114Z [37m20251126-14:37:16.341 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:37:16.342772382Z [31m20251126-14:37:16.342 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.343195871Z [37m20251126-14:37:16.342 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:37:16.343576320Z [31m20251126-14:37:16.343 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.343940899Z [37m20251126-14:37:16.343 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:37:16.344343219Z [31m20251126-14:37:16.344 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:16.344697978Z [31m20251126-14:37:16.344 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:37:16.345135787Z Traceback (most recent call last):
2025-11-26T14:37:16.345670866Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:37:16.345686336Z     future = loop.run_in_executor(
2025-11-26T14:37:16.345693316Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:16.346122955Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:37:16.346507354Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:37:16.346519264Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:37:16.347028893Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:37:17.509207463Z [31m20251126-14:37:17.508 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:17.510272620Z [37m20251126-14:37:17.509 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:37:17.511064139Z [31m20251126-14:37:17.510 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:17.511871157Z [37m20251126-14:37:17.511 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:37:17.512385226Z [31m20251126-14:37:17.512 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:17.513038854Z [37m20251126-14:37:17.512 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:37:17.513460414Z [31m20251126-14:37:17.513 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:17.513908293Z [31m20251126-14:37:17.513 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:37:17.514483341Z Traceback (most recent call last):
2025-11-26T14:37:17.515065080Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:37:17.515084360Z     future = loop.run_in_executor(
2025-11-26T14:37:17.515090040Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.515488489Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:37:17.515993858Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:37:17.516010188Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:37:17.516449437Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:37:17.518438833Z [31m20251126-14:37:17.518 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:17.518966222Z [37m20251126-14:37:17.518 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:37:17.519461641Z [31m20251126-14:37:17.519 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:17.519969720Z [37m20251126-14:37:17.519 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:37:17.520683178Z [31m20251126-14:37:17.520 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:17.521436827Z [37m20251126-14:37:17.521 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:37:17.521853826Z [31m20251126-14:37:17.521 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:17.522326865Z [31m20251126-14:37:17.522 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:37:17.522801864Z Traceback (most recent call last):
2025-11-26T14:37:17.523383933Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:37:17.523402483Z     future = loop.run_in_executor(
2025-11-26T14:37:17.523408133Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.523891522Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:37:17.524308811Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:37:17.524320391Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:37:17.524760090Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:37:17.526367797Z [31m20251126-14:37:17.526 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:17.526761636Z [37m20251126-14:37:17.526 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:37:17.527194105Z [31m20251126-14:37:17.526 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:17.527562154Z [37m20251126-14:37:17.527 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:37:17.527993793Z [31m20251126-14:37:17.527 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:17.528354862Z [37m20251126-14:37:17.528 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:37:17.528763092Z [31m20251126-14:37:17.528 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:37:17.529211701Z [31m20251126-14:37:17.528 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:37:17.529729910Z Traceback (most recent call last):
2025-11-26T14:37:17.530332558Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:37:17.530350578Z     future = loop.run_in_executor(
2025-11-26T14:37:17.530356078Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.530774107Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:37:17.531918305Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:37:17.531936735Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:37:17.532426134Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:37:17.534264910Z [31m20251126-14:37:17.533 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 29 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:37:17.534286560Z Traceback (most recent call last):
2025-11-26T14:37:17.534292220Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:37:17.534297570Z     result = await async_task
2025-11-26T14:37:17.534303320Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.534308670Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:37:17.534313970Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:37:17.534319290Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.534324510Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:37:17.534330090Z     reward = await self.async_reward_fn(
2025-11-26T14:37:17.534335220Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.534340340Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:37:17.534357380Z     raise e
2025-11-26T14:37:17.534363280Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:37:17.534368500Z     future = loop.run_in_executor(
2025-11-26T14:37:17.534373690Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.534378860Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:37:17.534383990Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:37:17.534389180Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:37:17.534394310Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:37:17.535954127Z [31m20251126-14:37:17.534 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 27 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:37:17.535980057Z Traceback (most recent call last):
2025-11-26T14:37:17.535987737Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:37:17.535994526Z     result = await async_task
2025-11-26T14:37:17.536001316Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.536008046Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:37:17.536014646Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:37:17.536021236Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.536028896Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:37:17.536035756Z     reward = await self.async_reward_fn(
2025-11-26T14:37:17.536042396Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.536048886Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:37:17.536055316Z     raise e
2025-11-26T14:37:17.536062006Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:37:17.536068796Z     future = loop.run_in_executor(
2025-11-26T14:37:17.536075606Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.536082376Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:37:17.536089286Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:37:17.536095686Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:37:17.536102356Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:37:17.537347504Z [31m20251126-14:37:17.536 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 30 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:37:17.537370754Z Traceback (most recent call last):
2025-11-26T14:37:17.537376414Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:37:17.537382014Z     result = await async_task
2025-11-26T14:37:17.537387324Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.537392484Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:37:17.537397774Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:37:17.537403094Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.537408464Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:37:17.537414014Z     reward = await self.async_reward_fn(
2025-11-26T14:37:17.537419173Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.537424373Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:37:17.537429503Z     raise e
2025-11-26T14:37:17.537434773Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:37:17.537440613Z     future = loop.run_in_executor(
2025-11-26T14:37:17.537446903Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.537453253Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:37:17.537459663Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:37:17.537466443Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:37:17.537485553Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:37:17.538569061Z [31m20251126-14:37:17.537 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 25 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:37:17.538591901Z Traceback (most recent call last):
2025-11-26T14:37:17.538599711Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:37:17.538606881Z     result = await async_task
2025-11-26T14:37:17.538613941Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.538620881Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:37:17.538627881Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:37:17.538634761Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.538642021Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:37:17.538649091Z     reward = await self.async_reward_fn(
2025-11-26T14:37:17.538655801Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.538662161Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:37:17.538668831Z     raise e
2025-11-26T14:37:17.538675731Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:37:17.538682931Z     future = loop.run_in_executor(
2025-11-26T14:37:17.538689751Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.538696601Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:37:17.538703331Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:37:17.538709761Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:37:17.538716231Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:37:17.540244018Z [31m20251126-14:37:17.538 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 28 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:37:17.540266768Z Traceback (most recent call last):
2025-11-26T14:37:17.540272458Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:37:17.540277988Z     result = await async_task
2025-11-26T14:37:17.540283208Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.540288408Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:37:17.540293737Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:37:17.540309047Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.540314207Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:37:17.540319487Z     reward = await self.async_reward_fn(
2025-11-26T14:37:17.540324717Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.540329917Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:37:17.540339417Z     raise e
2025-11-26T14:37:17.540344737Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:37:17.540349937Z     future = loop.run_in_executor(
2025-11-26T14:37:17.540355157Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.540360337Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:37:17.540365547Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:37:17.540370707Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:37:17.540375817Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:37:17.541932864Z [31m20251126-14:37:17.540 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 31 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:37:17.541953674Z Traceback (most recent call last):
2025-11-26T14:37:17.541959364Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:37:17.541964654Z     result = await async_task
2025-11-26T14:37:17.541970054Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.541975194Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:37:17.541991534Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:37:17.541997084Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.542002304Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:37:17.542007504Z     reward = await self.async_reward_fn(
2025-11-26T14:37:17.542012844Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.542018034Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:37:17.542025204Z     raise e
2025-11-26T14:37:17.542031674Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:37:17.542038214Z     future = loop.run_in_executor(
2025-11-26T14:37:17.542045244Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:17.542051874Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:37:17.542058484Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:37:17.542065144Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:37:17.542071734Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:37:18.195512437Z [1;34mwandb[0m:
2025-11-26T14:37:18.195547367Z [1;34mwandb[0m: ðŸš€ View run [33mtrial_20251126_143557[0m at: [34m[0m
2025-11-26T14:37:18.195554717Z [1;34mwandb[0m: Find logs at: [1;35m../outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143557/wandb/run-20251126_143656-gsm8k-grpo-cloud-fast_trial_20251126_143557_train/logs[0m
2025-11-26T14:37:19.973780578Z [rank0]:[W1126 14:37:19.099557457 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
2025-11-26T14:37:22.201820718Z E1126 14:37:22.199000 1037 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1103) of binary: /usr/bin/python3
2025-11-26T14:37:22.203056776Z Traceback (most recent call last):
2025-11-26T14:37:22.203075136Z   File "/usr/local/bin/torchrun", line 7, in <module>
2025-11-26T14:37:22.203197236Z     sys.exit(main())
2025-11-26T14:37:22.203212585Z              ^^^^^^
2025-11-26T14:37:22.203219135Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
2025-11-26T14:37:22.203462045Z     return f(*args, **kwargs)
2025-11-26T14:37:22.203492335Z            ^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:22.203498755Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 901, in main
2025-11-26T14:37:22.204082524Z     run(args)
2025-11-26T14:37:22.204106114Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 892, in run
2025-11-26T14:37:22.204605023Z     elastic_launch(
2025-11-26T14:37:22.204618833Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 143, in __call__
2025-11-26T14:37:22.204685152Z     return launch_agent(self._config, self._entrypoint, list(args))
2025-11-26T14:37:22.204815282Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:37:22.204852162Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
2025-11-26T14:37:22.205041542Z     raise ChildFailedError(
2025-11-26T14:37:22.205065982Z torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
2025-11-26T14:37:22.205072412Z ============================================================
2025-11-26T14:37:22.205078012Z examples/cloud_gsm8k/gsm8k_grpo_train.py FAILED
2025-11-26T14:37:22.205083882Z ------------------------------------------------------------
2025-11-26T14:37:22.205089832Z Failures:
2025-11-26T14:37:22.205095572Z   <NO_OTHER_FAILURES>
2025-11-26T14:37:22.205100812Z ------------------------------------------------------------
2025-11-26T14:37:22.205106702Z Root Cause (first observed failure):
2025-11-26T14:37:22.205111992Z [0]:
2025-11-26T14:37:22.205117121Z   time      : 2025-11-26_14:37:22
2025-11-26T14:37:22.205139121Z   host      : 01af0063c46e
2025-11-26T14:37:22.205144521Z   rank      : 0 (local_rank: 0)
2025-11-26T14:37:22.205149591Z   exitcode  : 1 (pid: 1103)
2025-11-26T14:37:22.205154631Z   error_file: <N/A>
2025-11-26T14:37:22.205159701Z   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
2025-11-26T14:37:22.205165021Z ============================================================
2025-11-26T14:37:24.366615061Z [37m20251126-14:37:24.366 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [386][0m
2025-11-26T14:37:24.675613145Z Killed
2025-11-26T14:37:24.688262418Z [37m20251126-14:37:24.687 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [1036][0m
2025-11-26T14:37:24.689506336Z Traceback (most recent call last):
2025-11-26T14:37:24.689530896Z   File "<frozen runpy>", line 198, in _run_module_as_main
2025-11-26T14:37:24.689535126Z   File "<frozen runpy>", line 88, in _run_code
2025-11-26T14:37:24.689539046Z   File "/workspace/AReaL/areal/launcher/local.py", line 405, in <module>
2025-11-26T14:37:24.689653085Z     main()
2025-11-26T14:37:24.689670265Z   File "/workspace/AReaL/areal/launcher/local.py", line 260, in main
2025-11-26T14:37:24.689751985Z     local_main(config, run_id=0)
2025-11-26T14:37:24.689762805Z   File "/workspace/AReaL/areal/launcher/local.py", line 399, in local_main
2025-11-26T14:37:24.689860105Z     raise e
2025-11-26T14:37:24.689866095Z   File "/workspace/AReaL/areal/launcher/local.py", line 375, in local_main
2025-11-26T14:37:24.689952005Z     launcher.wait(
2025-11-26T14:37:24.689962345Z   File "/workspace/AReaL/areal/launcher/local.py", line 235, in wait
2025-11-26T14:37:24.690016595Z     raise JobException(
2025-11-26T14:37:24.690021295Z areal.utils.launcher.JobException: Job gsm8k-grpo-cloud-fast_trial_20251126_143557:trainer JobState.COMPLETED at node local
2025-11-26T14:37:24.934739373Z [37m20251126-14:37:24.933 Local Scheduler INFO: Waiting for 0 local running processes, pids: [0m
2025-11-26T14:37:37.393753406Z ==========
2025-11-26T14:37:37.393787016Z == CUDA ==
2025-11-26T14:37:37.393826566Z ==========
2025-11-26T14:37:37.402098309Z CUDA Version 12.9.1
2025-11-26T14:37:37.405733441Z Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2025-11-26T14:37:37.408586715Z This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2025-11-26T14:37:37.408594325Z By pulling and using the container, you accept the terms and conditions of this license:
2025-11-26T14:37:37.408599765Z https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2025-11-26T14:37:37.408610995Z A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
2025-11-26T14:37:37.632098688Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:37:37.843492856Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:37:38.081344558Z Branch 'feature/reasoning_model' set up to track remote branch 'feature/reasoning_model' from 'origin'.
2025-11-26T14:37:38.081389538Z Your branch is up to date with 'origin/feature/reasoning_model'.
2025-11-26T14:37:38.188044555Z Checking AReaL installation...
2025-11-26T14:37:38.275781812Z AReaL already installed. Skipping installation.
2025-11-26T14:37:38.275806612Z Cleaning up any leftover GPU processes...
2025-11-26T14:37:41.302050953Z Checking for processes holding GPU device files...
2025-11-26T14:37:41.988188108Z Found processes holding GPU devices: 1
2025-11-26T14:37:41.988260297Z 144
2025-11-26T14:37:41.988267007Z 145
2025-11-26T14:37:41.988273617Z 93
2025-11-26T14:37:41.988278927Z Killing process 1...
2025-11-26T14:37:41.988286757Z Killing process 144...
2025-11-26T14:37:41.988381107Z Killing process 145...
2025-11-26T14:37:43.992064717Z Using fuser to kill processes on GPU devices...
2025-11-26T14:37:46.018590288Z Checking GPU...
2025-11-26T14:37:46.057472797Z NVIDIA A100 80GB PCIe, 81920 MiB
2025-11-26T14:37:46.081058918Z Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True for better memory management
2025-11-26T14:37:46.081089868Z Detected 1 GPU(s)
2025-11-26T14:37:46.081095228Z Checking GPU status...
2025-11-26T14:37:46.110333987Z index, name, utilization.gpu [%], memory.used [MiB], compute_mode
2025-11-26T14:37:46.119778757Z 0, NVIDIA A100 80GB PCIe, 0, 0, Default
2025-11-26T14:37:46.172107857Z Verifying GPU accessibility...
2025-11-26T14:37:46.991257004Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:37:46.991327894Z   import pynvml  # type: ignore[import]
2025-11-26T14:37:48.264698741Z GPU accessibility verified on attempt 1
2025-11-26T14:37:48.885701772Z Starting training...
2025-11-26T14:37:51.889021001Z Using FAST training configuration (20-30 minutes)
2025-11-26T14:37:51.893848091Z ==========================================
2025-11-26T14:37:51.893856711Z Starting GRPO Training (Cloud)
2025-11-26T14:37:51.893863301Z ==========================================
2025-11-26T14:37:51.893869231Z Config: fast
2025-11-26T14:37:51.893936851Z Config file: examples/cloud_gsm8k/gsm8k_grpo_fast.yaml
2025-11-26T14:37:51.893947991Z Training script: examples/cloud_gsm8k/gsm8k_grpo_train.py
2025-11-26T14:37:51.893953981Z Experiment: gsm8k-grpo-cloud-fast
2025-11-26T14:37:51.893961211Z Trial: trial_20251126_143751
2025-11-26T14:37:51.893967521Z GPU: NVIDIA A100 80GB PCIe (81920 MB)
2025-11-26T14:37:51.893973861Z WandB API key: e1adc5be02...
2025-11-26T14:37:51.893980241Z ==========================================
2025-11-26T14:37:52.897464072Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:37:52.897524842Z   import pynvml  # type: ignore[import]
2025-11-26T14:37:57.927040143Z [37m20251126-14:37:57.926 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:37:57.927088873Z [37m20251126-14:37:57.926 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:37:57.927495923Z [37m20251126-14:37:57.927 name-resolve INFO: No such name resolve path: ./tmp/areal/name_resolve/root/gsm8k-grpo-cloud-fast/trial_20251126_143751[0m
2025-11-26T14:37:58.091052971Z [37m20251126-14:37:58.090 Local Scheduler INFO: LocalLauncher: experiment_name=gsm8k-grpo-cloud-fast, trial_name=trial_20251126_143751, fileroot=/workspace/outputs/grpo, run_id=0, is_recover_run=False[0m
2025-11-26T14:37:58.098943714Z [37m20251126-14:37:58.098 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL CUDA_VISIBLE_DEVICES=0 stdbuf -oL python3 -m areal.launcher.sglang_server examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_143751 sglang.random_seed=1 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143751/llm_server.log[0m
2025-11-26T14:37:59.094857181Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:37:59.094970441Z   import pynvml  # type: ignore[import]
2025-11-26T14:38:01.033047268Z [37m20251126-14:38:01.032 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:38:01.033086018Z [37m20251126-14:38:01.032 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:38:01.172455206Z [37m20251126-14:38:01.171 SGLangServer Wrapper INFO: Launch command: python3 -m sglang.launch_server --host 172.18.0.2 --port 14891 --tokenizer-path Qwen/Qwen2.5-0.5B-Instruct --tokenizer-mode auto --load-format auto --trust-remote-code --device cuda --tp-size 1 --base-gpu-id 0 --nnodes 1 --node-rank 0 --dist-init-addr localhost:44987 --model-path Qwen/Qwen2.5-0.5B-Instruct --random-seed 1 --skip-tokenizer-init --disable-radix-cache --torch-compile-max-bs 32 --triton-attention-num-kv-splits 8 --num-continuous-decode-steps 1 --attention-backend flashinfer --context-length 4096 --mem-fraction-static 0.8 --chunked-prefill-size -1 --max-prefill-tokens 32768 --schedule-policy lpm --schedule-conservativeness 1.0 --cpu-offload-gb 0 --dtype bfloat16 --kv-cache-dtype auto --dp-size 1 --ep-size 1 --max-loaded-loras 1 --max-loras-per-batch 1 --lora-backend triton --log-level warning --log-level-http warning --log-requests-level 0 --enable-metrics --decode-log-interval 1[0m
2025-11-26T14:38:02.764864166Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:38:02.764958706Z   import pynvml  # type: ignore[import]
2025-11-26T14:38:09.942200365Z INFO 11-26 14:38:09 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:38:10.779777304Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:38:12.123151334Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:38:12.123202714Z   import pynvml  # type: ignore[import]
2025-11-26T14:38:12.128526943Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:38:12.128555063Z   import pynvml  # type: ignore[import]
2025-11-26T14:38:18.916298547Z INFO 11-26 14:38:18 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:38:18.953985258Z INFO 11-26 14:38:18 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:38:19.923469871Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:38:20.303617536Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:38:20.308164116Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:38:20.309054924Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:38:20.309990242Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:38:20.355701657Z [2025-11-26 14:38:20] MOE_RUNNER_BACKEND is not initialized, using triton backend
2025-11-26T14:38:21.001203897Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:38:21.001238607Z   warnings.warn(
2025-11-26T14:38:21.001245357Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:38:21.001267507Z   warnings.warn(
2025-11-26T14:38:22.710910881Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:38:22.898117770Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.35it/s]
2025-11-26T14:38:22.898151939Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.34it/s]
2025-11-26T14:38:26.353808722Z   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   4%|â–         | 1/23 [00:00<00:15,  1.42it/s]Capturing batches (bs=152 avail_mem=14.84 GB):   4%|â–         | 1/23 [00:00<00:15,  1.42it/s]Capturing batches (bs=144 avail_mem=14.82 GB):   4%|â–         | 1/23 [00:00<00:15,  1.42it/s]Capturing batches (bs=144 avail_mem=14.82 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.40it/s]Capturing batches (bs=136 avail_mem=14.81 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.40it/s]Capturing batches (bs=128 avail_mem=14.79 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.40it/s]Capturing batches (bs=128 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:00<00:02,  7.09it/s]Capturing batches (bs=120 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:00<00:02,  7.09it/s]Capturing batches (bs=112 avail_mem=14.78 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:01<00:02,  7.09it/s]Capturing batches (bs=112 avail_mem=14.78 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.41it/s]Capturing batches (bs=104 avail_mem=14.76 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.41it/s]Capturing batches (bs=96 avail_mem=14.75 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.41it/s] Capturing batches (bs=96 avail_mem=14.75 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.32it/s]Capturing batches (bs=88 avail_mem=14.73 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.32it/s]Capturing batches (bs=80 avail_mem=14.72 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.32it/s]Capturing batches (bs=80 avail_mem=14.72 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.85it/s]Capturing batches (bs=72 avail_mem=14.71 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.85it/s]Capturing batches (bs=64 avail_mem=14.69 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.85it/s]Capturing batches (bs=64 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 14.03it/s]Capturing batches (bs=56 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 14.03it/s]Capturing batches (bs=48 avail_mem=14.66 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 14.03it/s]Capturing batches (bs=48 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.98it/s]Capturing batches (bs=40 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.98it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.98it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.98it/s]Capturing batches (bs=24 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.98it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.98it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.58it/s]Capturing batches (bs=8 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.58it/s] Capturing batches (bs=4 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.58it/s]Capturing batches (bs=4 avail_mem=14.60 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:01<00:00, 14.66it/s]Capturing batches (bs=2 avail_mem=14.59 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:01<00:00, 14.66it/s]Capturing batches (bs=1 avail_mem=14.57 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:02<00:00, 14.66it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 15.15it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.10it/s]
2025-11-26T14:38:32.254453062Z [37m20251126-14:38:32.253 SGLangServer Wrapper INFO: SGLang server launched at: http://172.18.0.2:14891[0m
2025-11-26T14:38:33.106694970Z [37m20251126-14:38:33.106 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:14891[0m
2025-11-26T14:38:33.106738499Z [37m20251126-14:38:33.106 Local Scheduler INFO: LLM inference server launched at: AREAL_LLM_SERVER_ADDRS=172.18.0.2:14891[0m
2025-11-26T14:38:33.107486478Z [37m20251126-14:38:33.107 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL AREAL_LLM_SERVER_ADDRS=172.18.0.2:14891 AREAL_RECOVER_RUN=0 NCCL_CUMEM_ENABLE=0 NCCL_NVLS_ENABLE=0 CUDA_VISIBLE_DEVICES=0 stdbuf -oL torchrun --nnodes 1 --nproc-per-node 1 --master-addr localhost --master-port 39427 examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_143751 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143751/trainer.log[0m
2025-11-26T14:38:33.108489566Z [37m20251126-14:38:33.108 Local Scheduler INFO: Waiting for 2 local running processes, pids: 387 1037[0m
2025-11-26T14:38:33.885644720Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:38:33.885695670Z   import pynvml  # type: ignore[import]
2025-11-26T14:38:35.586748273Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:38:35.586809423Z   import pynvml  # type: ignore[import]
2025-11-26T14:38:44.949164803Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:38:44.949222022Z   warnings.warn(
2025-11-26T14:38:44.949229662Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:38:44.949235882Z   warnings.warn(
2025-11-26T14:38:46.637100072Z [37m20251126-14:38:46.636 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:38:46.637142492Z [37m20251126-14:38:46.636 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:38:46.857207282Z [37m20251126-14:38:46.856 [FSDP Engine Rank 0] INFO: Initializing device mesh with parallel dims (dp=1, sp=1, tp=1, ep=1, etp=1, world_size=1).[0m
2025-11-26T14:38:46.860277176Z [37m20251126-14:38:46.860 [FSDP Engine Rank 0] INFO: Data parallel head 0 and rank 0[0m
2025-11-26T14:38:47.992632948Z [FAST] Limiting dataset from 7473 to 200 samples
2025-11-26T14:38:47.996432970Z [37m20251126-14:38:47.996 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:14891[0m
2025-11-26T14:38:47.996463880Z [37m20251126-14:38:47.996 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:38:47.996471420Z [37m20251126-14:38:47.996 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:38:49.000284910Z [37m20251126-14:38:48.999 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:38:49.003670003Z [37m20251126-14:38:49.003 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:14891[0m
2025-11-26T14:38:49.003726403Z [37m20251126-14:38:49.003 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:38:49.003739713Z [37m20251126-14:38:49.003 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:38:50.007654003Z [37m20251126-14:38:50.007 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:38:50.721137341Z [37m20251126-14:38:50.720 [FSDP Engine Rank 0] INFO: Model creation and loading time: 0.5479857639875263[0m
2025-11-26T14:38:51.307239735Z [37m20251126-14:38:51.306 [FSDP Engine Rank 0] INFO: Applying FSDP2 with N-D parallelism for 0.59 seconds[0m
2025-11-26T14:38:51.308131464Z [37m20251126-14:38:51.307 [FSDP Engine Rank 0] INFO: Create optimizer time: 0.000874347984790802[0m
2025-11-26T14:38:51.799979485Z wandb: Currently logged in as: tong-zhao (tong-zhao-georgia-institute-of-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-11-26T14:38:51.802817169Z wandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.
2025-11-26T14:38:52.545497766Z wandb: setting up run gsm8k-grpo-cloud-fast_trial_20251126_143751_train
2025-11-26T14:38:52.712801136Z wandb: Tracking run with wandb version 0.22.2
2025-11-26T14:38:52.712901676Z wandb: Run data is saved locally in /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143751/wandb/run-20251126_143851-gsm8k-grpo-cloud-fast_trial_20251126_143751_train
2025-11-26T14:38:52.712934266Z wandb: Run `wandb offline` to turn off syncing.
2025-11-26T14:38:52.713022225Z wandb: Syncing run trial_20251126_143751
2025-11-26T14:38:52.713097465Z wandb: â­ï¸ View project at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local
2025-11-26T14:38:52.713131955Z wandb: ðŸš€ View run at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local/runs/gsm8k-grpo-cloud-fast_trial_20251126_143751_train
2025-11-26T14:38:53.056489007Z wandb: Detected [openai] in use.
2025-11-26T14:38:53.056829516Z wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-11-26T14:38:53.057173356Z wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-11-26T14:38:53.062166005Z swanlab: SwanLab run disabled, the data will not be saved or uploaded.
2025-11-26T14:38:53.678921065Z ================================================================================
2025-11-26T14:38:53.679376164Z [FAST MODE]
2025-11-26T14:38:53.679482844Z   Dataset size: 200 samples (limited from 7473)
2025-11-26T14:38:53.679794454Z   Batch size: 8
2025-11-26T14:38:53.680292083Z   Steps per epoch: 25
2025-11-26T14:38:53.680593692Z   Total epochs: 1
2025-11-26T14:38:53.680829241Z   Total steps: 25
2025-11-26T14:38:53.681173051Z   Estimated time: ~25 minutes (~0.4 hours) at ~1 step/min
2025-11-26T14:38:53.681453370Z   Circuit breaker: Enabled (threshold: 50 consecutive zero rewards)
2025-11-26T14:38:53.681671600Z ================================================================================
2025-11-26T14:39:01.302662311Z [37m20251126-14:39:01.301 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4872, 522], padded to: [5120, 768], padding lengths: [248, 246][0m
2025-11-26T14:39:02.767350388Z [37m20251126-14:39:02.766 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 0.93, memory reserved (GB): 3.22, device memory used/total (GB): 69.68/79.25[0m
2025-11-26T14:39:02.930106298Z [37m20251126-14:39:02.929 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 0.93, memory reserved (GB): 3.22, device memory used/total (GB): 69.68/79.25[0m
2025-11-26T14:39:02.963978077Z [37m20251126-14:39:02.963 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4872, 522], padded to: [5120, 768], padding lengths: [248, 246][0m
2025-11-26T14:39:04.688957559Z [37m20251126-14:39:04.688 /workspace/AReaL/areal/utils/device.py INFO: ppo update, memory allocated (GB): 3.70, memory reserved (GB): 9.35, device memory used/total (GB): 76.32/79.25[0m
2025-11-26T14:39:07.905222283Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:39:08.056611536Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.62it/s]
2025-11-26T14:39:08.056646696Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.61it/s]
2025-11-26T14:39:08.081330865Z [37m20251126-14:39:08.079 [Remote Inference Engine Rank 0] INFO: Loading weights from disk done in 3.39s. Respond time: 0.11s.[0m
2025-11-26T14:39:08.092290212Z [92m20251126-14:39:08.091 StatsLogger INFO: Epoch 1/1 Step 1/25 Train step 1/25 done.[0m
2025-11-26T14:39:08.092978970Z [92m20251126-14:39:08.092 StatsLogger INFO: Stats (1/1):[0m
2025-11-26T14:39:08.106955041Z [92m20251126-14:39:08.105 StatsLogger INFO:
2025-11-26T14:39:08.106973631Z â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â••
2025-11-26T14:39:08.106979841Z â”‚ grpo_actor/actor_loss/avg        â”‚  1.9396e-04 â”‚ grpo_actor/entropy/min             â”‚  3.7608e-07 â”‚ grpo_actor/vocab_min_logits/min  â”‚ -3.1500e+01 â”‚ grpo_actor/no_eos_ratios/min    â”‚ 0.0000e+00 â”‚
2025-11-26T14:39:08.106985951Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.106991771Z â”‚ grpo_actor/actor_loss/min        â”‚ -2.6397e+00 â”‚ grpo_actor/entropy/max             â”‚  4.4419e+00 â”‚ grpo_actor/vocab_min_logits/max  â”‚ -7.5625e+00 â”‚ grpo_actor/no_eos_ratios/max    â”‚ 1.0000e+00 â”‚
2025-11-26T14:39:08.106997861Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107018491Z â”‚ grpo_actor/actor_loss/max        â”‚  2.4998e+00 â”‚ grpo_actor/grad_norm               â”‚  3.9851e+00 â”‚ grpo_actor/advantages/avg        â”‚ -2.0515e-08 â”‚ grpo_actor/prompt_len/avg       â”‚ 9.8875e+01 â”‚
2025-11-26T14:39:08.107023981Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107059211Z â”‚ grpo_actor/approx_kl/avg         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/avg   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/min        â”‚ -2.0205e+00 â”‚ grpo_actor/prompt_len/min       â”‚ 8.4000e+01 â”‚
2025-11-26T14:39:08.107064731Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107069981Z â”‚ grpo_actor/approx_kl/min         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/min   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/max        â”‚  2.0906e+00 â”‚ grpo_actor/prompt_len/max       â”‚ 1.1400e+02 â”‚
2025-11-26T14:39:08.107075621Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107081151Z â”‚ grpo_actor/approx_kl/max         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/max   â”‚  1.0000e+00 â”‚ grpo_actor/behav_imp_weight_cap  â”‚  5.0000e+00 â”‚ grpo_actor/seq_len/avg          â”‚ 3.3712e+02 â”‚
2025-11-26T14:39:08.107086591Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107091851Z â”‚ grpo_actor/behave_approx_kl/avg  â”‚ -9.6760e-04 â”‚ grpo_actor/lr                      â”‚  1.7000e-05 â”‚ grpo_actor/correct_seq_len/avg   â”‚  3.0050e+02 â”‚ grpo_actor/seq_len/min          â”‚ 2.5700e+02 â”‚
2025-11-26T14:39:08.107097651Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107108461Z â”‚ grpo_actor/behave_approx_kl/min  â”‚ -2.7865e-01 â”‚ grpo_actor/n_tokens                â”‚  5.3940e+03 â”‚ grpo_actor/correct_seq_len/min   â”‚  2.5700e+02 â”‚ grpo_actor/seq_len/max          â”‚ 3.7000e+02 â”‚
2025-11-26T14:39:08.107113841Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107123011Z â”‚ grpo_actor/behave_approx_kl/max  â”‚  2.3320e-01 â”‚ grpo_actor/n_valid_tokens          â”‚  3.8120e+03 â”‚ grpo_actor/correct_seq_len/max   â”‚  3.4400e+02 â”‚ grpo_actor/task_reward/avg      â”‚ 1.2500e-01 â”‚
2025-11-26T14:39:08.107128481Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107134361Z â”‚ grpo_actor/behave_imp_weight/avg â”‚  9.9952e-01 â”‚ grpo_actor/new_logp/avg            â”‚ -3.6707e-01 â”‚ grpo_actor/eps_clip              â”‚  4.0000e-01 â”‚ grpo_actor/task_reward/min      â”‚ 0.0000e+00 â”‚
2025-11-26T14:39:08.107140151Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107145451Z â”‚ grpo_actor/behave_imp_weight/min â”‚  7.5680e-01 â”‚ grpo_actor/new_logp/min            â”‚ -1.5757e+01 â”‚ grpo_actor/final_reward/avg      â”‚  0.0000e+00 â”‚ grpo_actor/task_reward/max      â”‚ 1.0000e+00 â”‚
2025-11-26T14:39:08.107150681Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107156061Z â”‚ grpo_actor/behave_imp_weight/max â”‚  1.2626e+00 â”‚ grpo_actor/new_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/final_reward/min      â”‚ -7.0711e-01 â”‚ grpo_actor/use_dual_clip        â”‚ 0.0000e+00 â”‚
2025-11-26T14:39:08.107161621Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107172950Z â”‚ grpo_actor/clip_ratio/avg        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/avg            â”‚ -3.6610e-01 â”‚ grpo_actor/final_reward/max      â”‚  7.0711e-01 â”‚ timeperf/compute_advantage      â”‚ 1.6255e-01 â”‚
2025-11-26T14:39:08.107178240Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107184040Z â”‚ grpo_actor/clip_ratio/min        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/min            â”‚ -1.5708e+01 â”‚ grpo_actor/incorrect_seq_len/avg â”‚  3.4236e+02 â”‚ timeperf/recompute_logp         â”‚ 1.5794e+00 â”‚
2025-11-26T14:39:08.107189280Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107194990Z â”‚ grpo_actor/clip_ratio/max        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/incorrect_seq_len/min â”‚  2.6500e+02 â”‚ timeperf/rollout                â”‚ 7.5062e+00 â”‚
2025-11-26T14:39:08.107200790Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107206110Z â”‚ grpo_actor/clipped_tokens        â”‚  0.0000e+00 â”‚ grpo_actor/unclipped_behave_tokens â”‚  3.8120e+03 â”‚ grpo_actor/incorrect_seq_len/max â”‚  3.7000e+02 â”‚ timeperf/checkpoint_for_recover â”‚ 6.3520e-05 â”‚
2025-11-26T14:39:08.107211280Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107216500Z â”‚ grpo_actor/dual_clip_ratio/avg   â”‚  0.0000e+00 â”‚ grpo_actor/update_successful       â”‚  1.0000e+00 â”‚ grpo_actor/kl_rewards/avg        â”‚  0.0000e+00 â”‚ timeperf/eval                   â”‚ 1.2211e-04 â”‚
2025-11-26T14:39:08.107227690Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107233100Z â”‚ grpo_actor/dual_clip_ratio/min   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/avg    â”‚  2.5742e+01 â”‚ grpo_actor/kl_rewards/min        â”‚  0.0000e+00 â”‚ timeperf/save                   â”‚ 2.0869e-04 â”‚
2025-11-26T14:39:08.107238500Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107243760Z â”‚ grpo_actor/dual_clip_ratio/max   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/min    â”‚  1.2938e+01 â”‚ grpo_actor/kl_rewards/max        â”‚  0.0000e+00 â”‚ timeperf/train_step             â”‚ 1.7589e+00 â”‚
2025-11-26T14:39:08.107248910Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107254200Z â”‚ grpo_actor/dual_clipped_tokens   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/max    â”‚  3.9000e+01 â”‚ grpo_actor/mask_no_eos_with_zero â”‚  0.0000e+00 â”‚ timeperf/update_weights         â”‚ 3.3942e+00 â”‚
2025-11-26T14:39:08.107259960Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:39:08.107268170Z â”‚ grpo_actor/entropy/avg           â”‚  3.6970e-01 â”‚ grpo_actor/vocab_min_logits/avg    â”‚ -1.5733e+01 â”‚ grpo_actor/no_eos_ratios/avg     â”‚  1.2500e-01 â”‚ rollout/reward                  â”‚ 1.6667e-01 â”‚
2025-11-26T14:39:08.107273580Z â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•›[0m
2025-11-26T14:39:08.160513259Z [37m20251126-14:39:08.160 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4918, 413], padded to: [5120, 512], padding lengths: [202, 99][0m
2025-11-26T14:39:11.153424620Z [37m20251126-14:39:11.152 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 3.70, memory reserved (GB): 9.36, device memory used/total (GB): 76.33/79.25[0m
2025-11-26T14:39:11.268032310Z [37m20251126-14:39:11.267 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 3.70, memory reserved (GB): 9.36, device memory used/total (GB): 76.33/79.25[0m
2025-11-26T14:39:11.293292927Z [37m20251126-14:39:11.293 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4918, 413], padded to: [5120, 512], padding lengths: [202, 99][0m
2025-11-26T14:39:11.661282388Z Traceback (most recent call last):
2025-11-26T14:39:11.663393473Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:39:11.663406173Z     main(sys.argv[1:])
2025-11-26T14:39:11.663949802Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:39:11.663964412Z     stats = actor.ppo_update(batch)
2025-11-26T14:39:11.663969492Z             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:39:11.664491751Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:39:11.664512061Z     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:39:11.664518401Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:39:11.665101530Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:39:11.665117759Z     train_stat = self.engine.train_batch(
2025-11-26T14:39:11.665124489Z                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:39:11.665677798Z   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:39:11.665689318Z     loss.backward()
2025-11-26T14:39:11.666195477Z   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:39:11.666217457Z     torch.autograd.backward(
2025-11-26T14:39:11.666677296Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:39:11.666699246Z     _engine_run_backward(
2025-11-26T14:39:11.667243655Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:39:11.667268515Z     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:39:11.667275465Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:39:11.667986304Z torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.39 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.12 GiB is free. Process 1218473 has 64.89 GiB memory in use. Process 1219139 has 13.23 GiB memory in use. Of the allocated memory 11.06 GiB is allocated by PyTorch, and 93.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:39:11.671215807Z [rank0]: Traceback (most recent call last):
2025-11-26T14:39:11.671236257Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:39:11.671243627Z [rank0]:     main(sys.argv[1:])
2025-11-26T14:39:11.671250277Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:39:11.671256627Z [rank0]:     stats = actor.ppo_update(batch)
2025-11-26T14:39:11.671262127Z [rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:39:11.671267577Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:39:11.671273517Z [rank0]:     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:39:11.671295527Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:39:11.671301027Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:39:11.671306407Z [rank0]:     train_stat = self.engine.train_batch(
2025-11-26T14:39:11.671312347Z [rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:39:11.671317496Z [rank0]:   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:39:11.671322666Z [rank0]:     loss.backward()
2025-11-26T14:39:11.671328126Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:39:11.671333826Z [rank0]:     torch.autograd.backward(
2025-11-26T14:39:11.671340156Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:39:11.671346366Z [rank0]:     _engine_run_backward(
2025-11-26T14:39:11.671352806Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:39:11.671359766Z [rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:39:11.671365706Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:39:11.671373076Z [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.39 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.12 GiB is free. Process 1218473 has 64.89 GiB memory in use. Process 1219139 has 13.23 GiB memory in use. Of the allocated memory 11.06 GiB is allocated by PyTorch, and 93.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:39:13.544690409Z [1;34mwandb[0m:
2025-11-26T14:39:13.544731009Z [1;34mwandb[0m: ðŸš€ View run [33mtrial_20251126_143751[0m at: [34m[0m
2025-11-26T14:39:13.544739329Z [1;34mwandb[0m: Find logs at: [1;35m../outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143751/wandb/run-20251126_143851-gsm8k-grpo-cloud-fast_trial_20251126_143751_train/logs[0m
2025-11-26T14:39:15.418191430Z [rank0]:[W1126 14:39:15.543820180 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
2025-11-26T14:39:17.740821183Z E1126 14:39:17.738000 1038 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1104) of binary: /usr/bin/python3
2025-11-26T14:39:17.741009613Z Traceback (most recent call last):
2025-11-26T14:39:17.741029512Z   File "/usr/local/bin/torchrun", line 7, in <module>
2025-11-26T14:39:17.741169092Z     sys.exit(main())
2025-11-26T14:39:17.741198112Z              ^^^^^^
2025-11-26T14:39:17.741203992Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
2025-11-26T14:39:17.741522991Z     return f(*args, **kwargs)
2025-11-26T14:39:17.741540291Z            ^^^^^^^^^^^^^^^^^^
2025-11-26T14:39:17.741545941Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 901, in main
2025-11-26T14:39:17.742092300Z     run(args)
2025-11-26T14:39:17.742120890Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 892, in run
2025-11-26T14:39:17.742633529Z     elastic_launch(
2025-11-26T14:39:17.742648909Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 143, in __call__
2025-11-26T14:39:17.742656889Z     return launch_agent(self._config, self._entrypoint, list(args))
2025-11-26T14:39:17.742711139Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:39:17.742718319Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
2025-11-26T14:39:17.743037168Z     raise ChildFailedError(
2025-11-26T14:39:17.743062788Z torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
2025-11-26T14:39:17.743069558Z ============================================================
2025-11-26T14:39:17.743075858Z examples/cloud_gsm8k/gsm8k_grpo_train.py FAILED
2025-11-26T14:39:17.743081958Z ------------------------------------------------------------
2025-11-26T14:39:17.743088378Z Failures:
2025-11-26T14:39:17.743094468Z   <NO_OTHER_FAILURES>
2025-11-26T14:39:17.743101158Z ------------------------------------------------------------
2025-11-26T14:39:17.743107188Z Root Cause (first observed failure):
2025-11-26T14:39:17.743113698Z [0]:
2025-11-26T14:39:17.743120008Z   time      : 2025-11-26_14:39:17
2025-11-26T14:39:17.743126288Z   host      : 01af0063c46e
2025-11-26T14:39:17.743132318Z   rank      : 0 (local_rank: 0)
2025-11-26T14:39:17.743138248Z   exitcode  : 1 (pid: 1104)
2025-11-26T14:39:17.743144578Z   error_file: <N/A>
2025-11-26T14:39:17.743150868Z   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
2025-11-26T14:39:17.743157078Z ============================================================
2025-11-26T14:39:19.122661873Z [37m20251126-14:39:19.122 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [387][0m
2025-11-26T14:39:19.424117922Z Killed
2025-11-26T14:39:19.427605855Z [37m20251126-14:39:19.427 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [1037][0m
2025-11-26T14:39:19.428686683Z Traceback (most recent call last):
2025-11-26T14:39:19.428693083Z   File "<frozen runpy>", line 198, in _run_module_as_main
2025-11-26T14:39:19.428696453Z   File "<frozen runpy>", line 88, in _run_code
2025-11-26T14:39:19.428699663Z   File "/workspace/AReaL/areal/launcher/local.py", line 405, in <module>
2025-11-26T14:39:19.428863193Z     main()
2025-11-26T14:39:19.428943262Z   File "/workspace/AReaL/areal/launcher/local.py", line 260, in main
2025-11-26T14:39:19.428979592Z     local_main(config, run_id=0)
2025-11-26T14:39:19.429012872Z   File "/workspace/AReaL/areal/launcher/local.py", line 399, in local_main
2025-11-26T14:39:19.429116812Z     raise e
2025-11-26T14:39:19.429120102Z   File "/workspace/AReaL/areal/launcher/local.py", line 375, in local_main
2025-11-26T14:39:19.429224572Z     launcher.wait(
2025-11-26T14:39:19.429228812Z   File "/workspace/AReaL/areal/launcher/local.py", line 235, in wait
2025-11-26T14:39:19.429293982Z     raise JobException(
2025-11-26T14:39:19.429298122Z areal.utils.launcher.JobException: Job gsm8k-grpo-cloud-fast_trial_20251126_143751:trainer JobState.COMPLETED at node local
2025-11-26T14:39:19.659659140Z [37m20251126-14:39:19.659 Local Scheduler INFO: Waiting for 0 local running processes, pids: [0m
2025-11-26T14:39:31.314743445Z ==========
2025-11-26T14:39:31.314770685Z == CUDA ==
2025-11-26T14:39:31.314841015Z ==========
2025-11-26T14:39:31.323003278Z CUDA Version 12.9.1
2025-11-26T14:39:31.326116411Z Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2025-11-26T14:39:31.329224525Z This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2025-11-26T14:39:31.329230935Z By pulling and using the container, you accept the terms and conditions of this license:
2025-11-26T14:39:31.329236255Z https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2025-11-26T14:39:31.329246385Z A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
2025-11-26T14:39:31.552105318Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:39:31.765200413Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:39:32.009568422Z Branch 'feature/reasoning_model' set up to track remote branch 'feature/reasoning_model' from 'origin'.
2025-11-26T14:39:32.009597892Z Your branch is up to date with 'origin/feature/reasoning_model'.
2025-11-26T14:39:32.107190738Z Checking AReaL installation...
2025-11-26T14:39:32.190194094Z AReaL already installed. Skipping installation.
2025-11-26T14:39:32.190232254Z Cleaning up any leftover GPU processes...
2025-11-26T14:39:35.216380056Z Checking for processes holding GPU device files...
2025-11-26T14:39:35.911047353Z Found processes holding GPU devices: 1
2025-11-26T14:39:35.911110973Z 144
2025-11-26T14:39:35.911118023Z 145
2025-11-26T14:39:35.911124393Z 93
2025-11-26T14:39:35.911130113Z Killing process 1...
2025-11-26T14:39:35.911136773Z Killing process 144...
2025-11-26T14:39:35.911320922Z Killing process 145...
2025-11-26T14:39:37.915548320Z Using fuser to kill processes on GPU devices...
2025-11-26T14:39:39.942356911Z Checking GPU...
2025-11-26T14:39:39.980140702Z NVIDIA A100 80GB PCIe, 81920 MiB
2025-11-26T14:39:40.007950004Z Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True for better memory management
2025-11-26T14:39:40.007996774Z Detected 1 GPU(s)
2025-11-26T14:39:40.008002994Z Checking GPU status...
2025-11-26T14:39:40.039429038Z index, name, utilization.gpu [%], memory.used [MiB], compute_mode
2025-11-26T14:39:40.046836973Z 0, NVIDIA A100 80GB PCIe, 0, 0, Default
2025-11-26T14:39:40.133165752Z Verifying GPU accessibility...
2025-11-26T14:39:40.909617549Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:39:40.909675618Z   import pynvml  # type: ignore[import]
2025-11-26T14:39:42.126317974Z GPU accessibility verified on attempt 1
2025-11-26T14:39:42.677981520Z Starting training...
2025-11-26T14:39:45.681957798Z Using FAST training configuration (20-30 minutes)
2025-11-26T14:39:45.686754778Z ==========================================
2025-11-26T14:39:45.686763738Z Starting GRPO Training (Cloud)
2025-11-26T14:39:45.686770978Z ==========================================
2025-11-26T14:39:45.686777918Z Config: fast
2025-11-26T14:39:45.686784918Z Config file: examples/cloud_gsm8k/gsm8k_grpo_fast.yaml
2025-11-26T14:39:45.686791028Z Training script: examples/cloud_gsm8k/gsm8k_grpo_train.py
2025-11-26T14:39:45.686796988Z Experiment: gsm8k-grpo-cloud-fast
2025-11-26T14:39:45.686802418Z Trial: trial_20251126_143945
2025-11-26T14:39:45.686807638Z GPU: NVIDIA A100 80GB PCIe (81920 MB)
2025-11-26T14:39:45.686812988Z WandB API key: e1adc5be02...
2025-11-26T14:39:45.686818498Z ==========================================
2025-11-26T14:39:46.713685030Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:39:46.713731600Z   import pynvml  # type: ignore[import]
2025-11-26T14:39:51.752793171Z [37m20251126-14:39:51.752 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:39:51.752844751Z [37m20251126-14:39:51.752 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:39:51.753393290Z [37m20251126-14:39:51.752 name-resolve INFO: No such name resolve path: ./tmp/areal/name_resolve/root/gsm8k-grpo-cloud-fast/trial_20251126_143945[0m
2025-11-26T14:39:51.925496090Z [37m20251126-14:39:51.925 Local Scheduler INFO: LocalLauncher: experiment_name=gsm8k-grpo-cloud-fast, trial_name=trial_20251126_143945, fileroot=/workspace/outputs/grpo, run_id=0, is_recover_run=False[0m
2025-11-26T14:39:51.934696091Z [37m20251126-14:39:51.934 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL CUDA_VISIBLE_DEVICES=0 stdbuf -oL python3 -m areal.launcher.sglang_server examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_143945 sglang.random_seed=1 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143945/llm_server.log[0m
2025-11-26T14:39:53.022779615Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:39:53.022818885Z   import pynvml  # type: ignore[import]
2025-11-26T14:39:55.004730410Z [37m20251126-14:39:55.004 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:39:55.004766340Z [37m20251126-14:39:55.004 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:39:55.161597412Z [37m20251126-14:39:55.160 SGLangServer Wrapper INFO: Launch command: python3 -m sglang.launch_server --host 172.18.0.2 --port 14081 --tokenizer-path Qwen/Qwen2.5-0.5B-Instruct --tokenizer-mode auto --load-format auto --trust-remote-code --device cuda --tp-size 1 --base-gpu-id 0 --nnodes 1 --node-rank 0 --dist-init-addr localhost:25870 --model-path Qwen/Qwen2.5-0.5B-Instruct --random-seed 1 --skip-tokenizer-init --disable-radix-cache --torch-compile-max-bs 32 --triton-attention-num-kv-splits 8 --num-continuous-decode-steps 1 --attention-backend flashinfer --context-length 4096 --mem-fraction-static 0.8 --chunked-prefill-size -1 --max-prefill-tokens 32768 --schedule-policy lpm --schedule-conservativeness 1.0 --cpu-offload-gb 0 --dtype bfloat16 --kv-cache-dtype auto --dp-size 1 --ep-size 1 --max-loaded-loras 1 --max-loras-per-batch 1 --lora-backend triton --log-level warning --log-level-http warning --log-requests-level 0 --enable-metrics --decode-log-interval 1[0m
2025-11-26T14:39:56.578484889Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:39:56.578548829Z   import pynvml  # type: ignore[import]
2025-11-26T14:40:03.813367578Z INFO 11-26 14:40:03 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:40:04.654119570Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:40:06.044071863Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:40:06.044098123Z   import pynvml  # type: ignore[import]
2025-11-26T14:40:06.050140250Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:40:06.050165560Z   import pynvml  # type: ignore[import]
2025-11-26T14:40:12.800524553Z INFO 11-26 14:40:12 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:40:12.801397621Z INFO 11-26 14:40:12 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:40:13.578962175Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:40:13.843637291Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:40:13.848105702Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:40:13.849055660Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:40:13.850116588Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:40:13.896980250Z [2025-11-26 14:40:13] MOE_RUNNER_BACKEND is not initialized, using triton backend
2025-11-26T14:40:14.553656036Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:40:14.553702036Z   warnings.warn(
2025-11-26T14:40:14.553707176Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:40:14.553711386Z   warnings.warn(
2025-11-26T14:40:15.937056763Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:40:16.107034458Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.85it/s]
2025-11-26T14:40:16.107065548Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.85it/s]
2025-11-26T14:40:19.895237775Z   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   4%|â–         | 1/23 [00:00<00:16,  1.32it/s]Capturing batches (bs=152 avail_mem=14.84 GB):   4%|â–         | 1/23 [00:00<00:16,  1.32it/s]Capturing batches (bs=152 avail_mem=14.84 GB):   9%|â–Š         | 2/23 [00:00<00:07,  2.67it/s]Capturing batches (bs=144 avail_mem=14.82 GB):   9%|â–Š         | 2/23 [00:00<00:07,  2.67it/s]Capturing batches (bs=136 avail_mem=14.81 GB):   9%|â–Š         | 2/23 [00:00<00:07,  2.67it/s]Capturing batches (bs=136 avail_mem=14.81 GB):  17%|â–ˆâ–‹        | 4/23 [00:00<00:03,  5.57it/s]Capturing batches (bs=128 avail_mem=14.79 GB):  17%|â–ˆâ–‹        | 4/23 [00:00<00:03,  5.57it/s]Capturing batches (bs=120 avail_mem=14.79 GB):  17%|â–ˆâ–‹        | 4/23 [00:01<00:03,  5.57it/s]Capturing batches (bs=120 avail_mem=14.79 GB):  26%|â–ˆâ–ˆâ–Œ       | 6/23 [00:01<00:02,  7.97it/s]Capturing batches (bs=112 avail_mem=14.78 GB):  26%|â–ˆâ–ˆâ–Œ       | 6/23 [00:01<00:02,  7.97it/s]Capturing batches (bs=104 avail_mem=14.76 GB):  26%|â–ˆâ–ˆâ–Œ       | 6/23 [00:01<00:02,  7.97it/s]Capturing batches (bs=104 avail_mem=14.76 GB):  35%|â–ˆâ–ˆâ–ˆâ–      | 8/23 [00:01<00:01,  9.68it/s]Capturing batches (bs=96 avail_mem=14.75 GB):  35%|â–ˆâ–ˆâ–ˆâ–      | 8/23 [00:01<00:01,  9.68it/s] Capturing batches (bs=88 avail_mem=14.73 GB):  35%|â–ˆâ–ˆâ–ˆâ–      | 8/23 [00:01<00:01,  9.68it/s]Capturing batches (bs=88 avail_mem=14.73 GB):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10/23 [00:01<00:01, 11.05it/s]Capturing batches (bs=80 avail_mem=14.72 GB):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10/23 [00:01<00:01, 11.05it/s]Capturing batches (bs=72 avail_mem=14.71 GB):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 10/23 [00:01<00:01, 11.05it/s]Capturing batches (bs=72 avail_mem=14.71 GB):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/23 [00:01<00:00, 12.01it/s]Capturing batches (bs=64 avail_mem=14.69 GB):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/23 [00:01<00:00, 12.01it/s]Capturing batches (bs=56 avail_mem=14.69 GB):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/23 [00:01<00:00, 12.01it/s]Capturing batches (bs=56 avail_mem=14.69 GB):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14/23 [00:01<00:00, 12.73it/s]Capturing batches (bs=48 avail_mem=14.66 GB):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14/23 [00:01<00:00, 12.73it/s]Capturing batches (bs=40 avail_mem=14.66 GB):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 14/23 [00:01<00:00, 12.73it/s]Capturing batches (bs=40 avail_mem=14.66 GB):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16/23 [00:01<00:00, 13.18it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16/23 [00:01<00:00, 13.18it/s]Capturing batches (bs=24 avail_mem=14.63 GB):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 16/23 [00:01<00:00, 13.18it/s]Capturing batches (bs=24 avail_mem=14.63 GB):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18/23 [00:01<00:00, 12.99it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18/23 [00:01<00:00, 12.99it/s]Capturing batches (bs=8 avail_mem=14.60 GB):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 18/23 [00:02<00:00, 12.99it/s] Capturing batches (bs=8 avail_mem=14.60 GB):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20/23 [00:02<00:00, 11.93it/s]Capturing batches (bs=4 avail_mem=14.60 GB):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20/23 [00:02<00:00, 11.93it/s]Capturing batches (bs=2 avail_mem=14.59 GB):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 20/23 [00:02<00:00, 11.93it/s]Capturing batches (bs=2 avail_mem=14.59 GB):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22/23 [00:02<00:00, 12.95it/s]Capturing batches (bs=1 avail_mem=14.57 GB):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 22/23 [00:02<00:00, 12.95it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00,  9.73it/s]
2025-11-26T14:40:25.280974731Z [37m20251126-14:40:25.280 SGLangServer Wrapper INFO: SGLang server launched at: http://172.18.0.2:14081[0m
2025-11-26T14:40:25.948733975Z [37m20251126-14:40:25.948 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:14081[0m
2025-11-26T14:40:25.948796145Z [37m20251126-14:40:25.948 Local Scheduler INFO: LLM inference server launched at: AREAL_LLM_SERVER_ADDRS=172.18.0.2:14081[0m
2025-11-26T14:40:25.949648173Z [37m20251126-14:40:25.949 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL AREAL_LLM_SERVER_ADDRS=172.18.0.2:14081 AREAL_RECOVER_RUN=0 NCCL_CUMEM_ENABLE=0 NCCL_NVLS_ENABLE=0 CUDA_VISIBLE_DEVICES=0 stdbuf -oL torchrun --nnodes 1 --nproc-per-node 1 --master-addr localhost --master-port 15428 examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_143945 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143945/trainer.log[0m
2025-11-26T14:40:25.950624331Z [37m20251126-14:40:25.950 Local Scheduler INFO: Waiting for 2 local running processes, pids: 387 1037[0m
2025-11-26T14:40:26.707730158Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:40:26.707759538Z   import pynvml  # type: ignore[import]
2025-11-26T14:40:28.403839551Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:40:28.403869830Z   import pynvml  # type: ignore[import]
2025-11-26T14:40:37.859791465Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:40:37.859848265Z   warnings.warn(
2025-11-26T14:40:37.859855305Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:40:37.859930765Z   warnings.warn(
2025-11-26T14:40:39.517967757Z [37m20251126-14:40:39.517 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:40:39.518002677Z [37m20251126-14:40:39.517 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:40:39.736038531Z [37m20251126-14:40:39.735 [FSDP Engine Rank 0] INFO: Initializing device mesh with parallel dims (dp=1, sp=1, tp=1, ep=1, etp=1, world_size=1).[0m
2025-11-26T14:40:39.738861905Z [37m20251126-14:40:39.738 [FSDP Engine Rank 0] INFO: Data parallel head 0 and rank 0[0m
2025-11-26T14:40:41.044066145Z [FAST] Limiting dataset from 7473 to 200 samples
2025-11-26T14:40:41.048125137Z [37m20251126-14:40:41.047 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:14081[0m
2025-11-26T14:40:41.048204647Z [37m20251126-14:40:41.047 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:40:41.048212407Z [37m20251126-14:40:41.048 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:40:42.052277177Z [37m20251126-14:40:42.051 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:40:42.055573880Z [37m20251126-14:40:42.055 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:14081[0m
2025-11-26T14:40:42.055604840Z [37m20251126-14:40:42.055 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:40:42.055627820Z [37m20251126-14:40:42.055 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:40:43.059506221Z [37m20251126-14:40:43.059 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:40:43.782676948Z [37m20251126-14:40:43.782 [FSDP Engine Rank 0] INFO: Model creation and loading time: 0.5675919728819281[0m
2025-11-26T14:40:44.310765944Z [37m20251126-14:40:44.310 [FSDP Engine Rank 0] INFO: Applying FSDP2 with N-D parallelism for 0.53 seconds[0m
2025-11-26T14:40:44.311659162Z [37m20251126-14:40:44.311 [FSDP Engine Rank 0] INFO: Create optimizer time: 0.0009058879222720861[0m
2025-11-26T14:40:44.952002943Z wandb: Currently logged in as: tong-zhao (tong-zhao-georgia-institute-of-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-11-26T14:40:44.955169856Z wandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.
2025-11-26T14:40:45.696669935Z wandb: setting up run gsm8k-grpo-cloud-fast_trial_20251126_143945_train
2025-11-26T14:40:45.890197871Z wandb: Tracking run with wandb version 0.22.2
2025-11-26T14:40:45.890246091Z wandb: Run data is saved locally in /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143945/wandb/run-20251126_144044-gsm8k-grpo-cloud-fast_trial_20251126_143945_train
2025-11-26T14:40:45.890255630Z wandb: Run `wandb offline` to turn off syncing.
2025-11-26T14:40:45.890322650Z wandb: Syncing run trial_20251126_143945
2025-11-26T14:40:45.890336220Z wandb: â­ï¸ View project at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local
2025-11-26T14:40:45.890433920Z wandb: ðŸš€ View run at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local/runs/gsm8k-grpo-cloud-fast_trial_20251126_143945_train
2025-11-26T14:40:46.237439884Z wandb: Detected [openai] in use.
2025-11-26T14:40:46.237725154Z wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-11-26T14:40:46.238000033Z wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-11-26T14:40:46.243408912Z swanlab: SwanLab run disabled, the data will not be saved or uploaded.
2025-11-26T14:40:46.854236674Z ================================================================================
2025-11-26T14:40:46.855188192Z [FAST MODE]
2025-11-26T14:40:46.855946981Z   Dataset size: 200 samples (limited from 7473)
2025-11-26T14:40:46.856552250Z   Batch size: 8
2025-11-26T14:40:46.856948979Z   Steps per epoch: 25
2025-11-26T14:40:46.857417068Z   Total epochs: 1
2025-11-26T14:40:46.857832577Z   Total steps: 25
2025-11-26T14:40:46.858267186Z   Estimated time: ~25 minutes (~0.4 hours) at ~1 step/min
2025-11-26T14:40:46.858681735Z   Circuit breaker: Enabled (threshold: 50 consecutive zero rewards)
2025-11-26T14:40:46.859328554Z ================================================================================
2025-11-26T14:40:55.629676802Z [37m20251126-14:40:55.628 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [5093, 262], padded to: [5120, 512], padding lengths: [27, 250][0m
2025-11-26T14:40:57.101196514Z [37m20251126-14:40:57.100 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 0.93, memory reserved (GB): 3.14, device memory used/total (GB): 69.61/79.25[0m
2025-11-26T14:40:57.265197811Z [37m20251126-14:40:57.264 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 0.93, memory reserved (GB): 3.14, device memory used/total (GB): 69.61/79.25[0m
2025-11-26T14:40:57.300067768Z [37m20251126-14:40:57.299 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [5093, 262], padded to: [5120, 512], padding lengths: [27, 250][0m
2025-11-26T14:40:59.046657856Z [37m20251126-14:40:59.046 /workspace/AReaL/areal/utils/device.py INFO: ppo update, memory allocated (GB): 3.70, memory reserved (GB): 10.29, device memory used/total (GB): 77.26/79.25[0m
2025-11-26T14:41:02.186326060Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:41:02.333238482Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.82it/s]
2025-11-26T14:41:02.333274792Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.81it/s]
2025-11-26T14:41:02.394414054Z [37m20251126-14:41:02.393 [Remote Inference Engine Rank 0] INFO: Loading weights from disk done in 3.35s. Respond time: 0.06s.[0m
2025-11-26T14:41:02.404622593Z [92m20251126-14:41:02.403 StatsLogger INFO: Epoch 1/1 Step 1/25 Train step 1/25 done.[0m
2025-11-26T14:41:02.405545431Z [92m20251126-14:41:02.404 StatsLogger INFO: Stats (1/1):[0m
2025-11-26T14:41:02.417261837Z [92m20251126-14:41:02.416 StatsLogger INFO:
2025-11-26T14:41:02.417287897Z â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â••
2025-11-26T14:41:02.417294177Z â”‚ grpo_actor/actor_loss/avg        â”‚  0.0000e+00 â”‚ grpo_actor/entropy/min             â”‚  4.0641e-08 â”‚ grpo_actor/vocab_min_logits/min  â”‚ -3.2500e+01 â”‚ grpo_actor/no_eos_ratios/min    â”‚ 0.0000e+00 â”‚
2025-11-26T14:41:02.417300727Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417306597Z â”‚ grpo_actor/actor_loss/min        â”‚ -0.0000e+00 â”‚ grpo_actor/entropy/max             â”‚  4.4117e+00 â”‚ grpo_actor/vocab_min_logits/max  â”‚ -7.4688e+00 â”‚ grpo_actor/no_eos_ratios/max    â”‚ 1.0000e+00 â”‚
2025-11-26T14:41:02.417328847Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417334517Z â”‚ grpo_actor/actor_loss/max        â”‚ -0.0000e+00 â”‚ grpo_actor/grad_norm               â”‚  0.0000e+00 â”‚ grpo_actor/advantages/avg        â”‚  0.0000e+00 â”‚ grpo_actor/prompt_len/avg       â”‚ 9.8875e+01 â”‚
2025-11-26T14:41:02.417340077Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417345547Z â”‚ grpo_actor/approx_kl/avg         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/avg   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/min        â”‚  0.0000e+00 â”‚ grpo_actor/prompt_len/min       â”‚ 8.4000e+01 â”‚
2025-11-26T14:41:02.417351257Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417356687Z â”‚ grpo_actor/approx_kl/min         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/min   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/max        â”‚  0.0000e+00 â”‚ grpo_actor/prompt_len/max       â”‚ 1.1400e+02 â”‚
2025-11-26T14:41:02.417366947Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417372537Z â”‚ grpo_actor/approx_kl/max         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/max   â”‚  1.0000e+00 â”‚ grpo_actor/behav_imp_weight_cap  â”‚  5.0000e+00 â”‚ grpo_actor/seq_len/avg          â”‚ 3.3469e+02 â”‚
2025-11-26T14:41:02.417378087Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417389166Z â”‚ grpo_actor/behave_approx_kl/avg  â”‚ -9.0847e-04 â”‚ grpo_actor/lr                      â”‚  1.7000e-05 â”‚ grpo_actor/correct_seq_len/avg   â”‚  3.2200e+02 â”‚ grpo_actor/seq_len/min          â”‚ 2.6200e+02 â”‚
2025-11-26T14:41:02.417395346Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417400796Z â”‚ grpo_actor/behave_approx_kl/min  â”‚ -4.4094e-01 â”‚ grpo_actor/n_tokens                â”‚  5.3550e+03 â”‚ grpo_actor/correct_seq_len/min   â”‚  3.0400e+02 â”‚ grpo_actor/seq_len/max          â”‚ 3.7000e+02 â”‚
2025-11-26T14:41:02.417406636Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417411986Z â”‚ grpo_actor/behave_approx_kl/max  â”‚  2.5725e-01 â”‚ grpo_actor/n_valid_tokens          â”‚  3.7730e+03 â”‚ grpo_actor/correct_seq_len/max   â”‚  3.4000e+02 â”‚ grpo_actor/task_reward/avg      â”‚ 1.2500e-01 â”‚
2025-11-26T14:41:02.417417426Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417422846Z â”‚ grpo_actor/behave_imp_weight/avg â”‚  9.9961e-01 â”‚ grpo_actor/new_logp/avg            â”‚ -3.7095e-01 â”‚ grpo_actor/eps_clip              â”‚  4.0000e-01 â”‚ grpo_actor/task_reward/min      â”‚ 0.0000e+00 â”‚
2025-11-26T14:41:02.417428566Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417433846Z â”‚ grpo_actor/behave_imp_weight/min â”‚  6.4343e-01 â”‚ grpo_actor/new_logp/min            â”‚ -1.6320e+01 â”‚ grpo_actor/final_reward/avg      â”‚  0.0000e+00 â”‚ grpo_actor/task_reward/max      â”‚ 1.0000e+00 â”‚
2025-11-26T14:41:02.417439246Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417451846Z â”‚ grpo_actor/behave_imp_weight/max â”‚  1.2934e+00 â”‚ grpo_actor/new_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/final_reward/min      â”‚  0.0000e+00 â”‚ grpo_actor/use_dual_clip        â”‚ 0.0000e+00 â”‚
2025-11-26T14:41:02.417457426Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417462686Z â”‚ grpo_actor/clip_ratio/avg        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/avg            â”‚ -3.7005e-01 â”‚ grpo_actor/final_reward/max      â”‚  0.0000e+00 â”‚ timeperf/compute_advantage      â”‚ 1.6383e-01 â”‚
2025-11-26T14:41:02.417468406Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417473626Z â”‚ grpo_actor/clip_ratio/min        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/min            â”‚ -1.6258e+01 â”‚ grpo_actor/incorrect_seq_len/avg â”‚  3.3650e+02 â”‚ timeperf/recompute_logp         â”‚ 1.6106e+00 â”‚
2025-11-26T14:41:02.417479116Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417484336Z â”‚ grpo_actor/clip_ratio/max        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/incorrect_seq_len/min â”‚  2.6200e+02 â”‚ timeperf/rollout                â”‚ 8.6311e+00 â”‚
2025-11-26T14:41:02.417489826Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417495086Z â”‚ grpo_actor/clipped_tokens        â”‚  0.0000e+00 â”‚ grpo_actor/unclipped_behave_tokens â”‚  3.7730e+03 â”‚ grpo_actor/incorrect_seq_len/max â”‚  3.7000e+02 â”‚ timeperf/checkpoint_for_recover â”‚ 6.2460e-05 â”‚
2025-11-26T14:41:02.417506326Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417511646Z â”‚ grpo_actor/dual_clip_ratio/avg   â”‚  0.0000e+00 â”‚ grpo_actor/update_successful       â”‚  1.0000e+00 â”‚ grpo_actor/kl_rewards/avg        â”‚  0.0000e+00 â”‚ timeperf/eval                   â”‚ 1.0616e-04 â”‚
2025-11-26T14:41:02.417517466Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417522936Z â”‚ grpo_actor/dual_clip_ratio/min   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/avg    â”‚  2.5654e+01 â”‚ grpo_actor/kl_rewards/min        â”‚  0.0000e+00 â”‚ timeperf/save                   â”‚ 1.8934e-04 â”‚
2025-11-26T14:41:02.417528336Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417533516Z â”‚ grpo_actor/dual_clip_ratio/max   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/min    â”‚  1.2938e+01 â”‚ grpo_actor/kl_rewards/max        â”‚  0.0000e+00 â”‚ timeperf/train_step             â”‚ 1.7820e+00 â”‚
2025-11-26T14:41:02.417538726Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417543956Z â”‚ grpo_actor/dual_clipped_tokens   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/max    â”‚  3.9750e+01 â”‚ grpo_actor/mask_no_eos_with_zero â”‚  0.0000e+00 â”‚ timeperf/update_weights         â”‚ 3.3492e+00 â”‚
2025-11-26T14:41:02.417552996Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:41:02.417564046Z â”‚ grpo_actor/entropy/avg           â”‚  3.7503e-01 â”‚ grpo_actor/vocab_min_logits/avg    â”‚ -1.5622e+01 â”‚ grpo_actor/no_eos_ratios/avg     â”‚  6.2500e-02 â”‚ rollout/reward                  â”‚ 1.8750e-01 â”‚
2025-11-26T14:41:02.417569616Z â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•›[0m
2025-11-26T14:41:02.468835339Z [37m20251126-14:41:02.468 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [5007, 419], padded to: [5120, 512], padding lengths: [113, 93][0m
2025-11-26T14:41:02.651253037Z [37m20251126-14:41:02.650 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 3.70, memory reserved (GB): 10.29, device memory used/total (GB): 77.14/79.25[0m
2025-11-26T14:41:02.764581120Z [37m20251126-14:41:02.764 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 3.70, memory reserved (GB): 10.29, device memory used/total (GB): 77.14/79.25[0m
2025-11-26T14:41:02.788784980Z [37m20251126-14:41:02.788 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [5007, 419], padded to: [5120, 512], padding lengths: [113, 93][0m
2025-11-26T14:41:03.145320934Z Traceback (most recent call last):
2025-11-26T14:41:03.147027590Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:41:03.147044500Z     main(sys.argv[1:])
2025-11-26T14:41:03.147609419Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:41:03.147625909Z     stats = actor.ppo_update(batch)
2025-11-26T14:41:03.147633439Z             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:03.148006898Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:41:03.148032978Z     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:41:03.148041698Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:03.148466227Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:41:03.148484057Z     train_stat = self.engine.train_batch(
2025-11-26T14:41:03.148492337Z                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:03.148930036Z   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:41:03.148946116Z     loss.backward()
2025-11-26T14:41:03.149395135Z   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:41:03.149406965Z     torch.autograd.backward(
2025-11-26T14:41:03.149821305Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:41:03.149836585Z     _engine_run_backward(
2025-11-26T14:41:03.150232434Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:41:03.150250624Z     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:41:03.150257864Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:03.150649323Z torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.16 GiB is free. Process 1222136 has 64.85 GiB memory in use. Process 1223118 has 13.23 GiB memory in use. Of the allocated memory 11.09 GiB is allocated by PyTorch, and 67.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:41:03.153821496Z [rank0]: Traceback (most recent call last):
2025-11-26T14:41:03.153839566Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:41:03.153845776Z [rank0]:     main(sys.argv[1:])
2025-11-26T14:41:03.153851826Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:41:03.153857806Z [rank0]:     stats = actor.ppo_update(batch)
2025-11-26T14:41:03.153863446Z [rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:03.153869626Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:41:03.153889516Z [rank0]:     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:41:03.153901286Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:03.153907146Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:41:03.153912926Z [rank0]:     train_stat = self.engine.train_batch(
2025-11-26T14:41:03.153918596Z [rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:03.153924396Z [rank0]:   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:41:03.153929946Z [rank0]:     loss.backward()
2025-11-26T14:41:03.153935846Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:41:03.153941596Z [rank0]:     torch.autograd.backward(
2025-11-26T14:41:03.153947266Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:41:03.153952856Z [rank0]:     _engine_run_backward(
2025-11-26T14:41:03.153958556Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:41:03.153966706Z [rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:41:03.153973456Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:03.153981816Z [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.16 GiB is free. Process 1222136 has 64.85 GiB memory in use. Process 1223118 has 13.23 GiB memory in use. Of the allocated memory 11.09 GiB is allocated by PyTorch, and 67.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:41:04.883504419Z [1;34mwandb[0m:
2025-11-26T14:41:04.883551928Z [1;34mwandb[0m: ðŸš€ View run [33mtrial_20251126_143945[0m at: [34m[0m
2025-11-26T14:41:04.883560538Z [1;34mwandb[0m: Find logs at: [1;35m../outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_143945/wandb/run-20251126_144044-gsm8k-grpo-cloud-fast_trial_20251126_143945_train/logs[0m
2025-11-26T14:41:05.120863422Z [31m20251126-14:41:05.120 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.127435388Z [37m20251126-14:41:05.127 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:41:05.129463854Z [31m20251126-14:41:05.129 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.131403510Z [37m20251126-14:41:05.131 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:41:05.133330406Z [31m20251126-14:41:05.133 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.135324572Z [37m20251126-14:41:05.135 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:41:05.137193478Z [31m20251126-14:41:05.136 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.139074954Z [31m20251126-14:41:05.138 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:41:05.141000040Z Traceback (most recent call last):
2025-11-26T14:41:05.145370061Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.145393371Z     future = loop.run_in_executor(
2025-11-26T14:41:05.145401081Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.148863624Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.152271927Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.152295587Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.155766509Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:05.162739735Z [31m20251126-14:41:05.161 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 24 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:41:05.162764425Z Traceback (most recent call last):
2025-11-26T14:41:05.162772005Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:41:05.162779395Z     result = await async_task
2025-11-26T14:41:05.162787094Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.162793644Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:41:05.162800314Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:41:05.162807014Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.162813774Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:41:05.162820874Z     reward = await self.async_reward_fn(
2025-11-26T14:41:05.162827544Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.162834424Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:41:05.162841474Z     raise e
2025-11-26T14:41:05.162849244Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.162856464Z     future = loop.run_in_executor(
2025-11-26T14:41:05.162863564Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.162870854Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.162898164Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.162912174Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.162918704Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:05.165602709Z [31m20251126-14:41:05.165 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.167482165Z [37m20251126-14:41:05.167 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:41:05.169338401Z [31m20251126-14:41:05.169 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.171145757Z [37m20251126-14:41:05.170 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:41:05.172960733Z [31m20251126-14:41:05.172 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.174767539Z [37m20251126-14:41:05.174 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:41:05.176589826Z [31m20251126-14:41:05.176 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.178428152Z [31m20251126-14:41:05.178 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:41:05.180756247Z Traceback (most recent call last):
2025-11-26T14:41:05.184324500Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.184337860Z     future = loop.run_in_executor(
2025-11-26T14:41:05.184344289Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.187574953Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.190826586Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.190840886Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.194120429Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:05.203924178Z [31m20251126-14:41:05.203 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.205359616Z [37m20251126-14:41:05.205 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:41:05.206754913Z [31m20251126-14:41:05.206 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.208069430Z [37m20251126-14:41:05.207 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:41:05.209467697Z [31m20251126-14:41:05.209 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.210787524Z [37m20251126-14:41:05.210 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:41:05.212166181Z [31m20251126-14:41:05.211 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.213519518Z [31m20251126-14:41:05.213 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:41:05.214827426Z Traceback (most recent call last):
2025-11-26T14:41:05.217504230Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.217519650Z     future = loop.run_in_executor(
2025-11-26T14:41:05.217524180Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.219888465Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.222286260Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.222302140Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.224638315Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:05.227399639Z [31m20251126-14:41:05.227 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.228694537Z [37m20251126-14:41:05.228 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:41:05.230418473Z [31m20251126-14:41:05.230 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.231741170Z [37m20251126-14:41:05.231 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:41:05.233190337Z [31m20251126-14:41:05.232 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.234497265Z [37m20251126-14:41:05.234 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:41:05.235934992Z [31m20251126-14:41:05.235 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.237247989Z [31m20251126-14:41:05.237 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:41:05.238536876Z Traceback (most recent call last):
2025-11-26T14:41:05.241009791Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.241026851Z     future = loop.run_in_executor(
2025-11-26T14:41:05.241031701Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.243613516Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.246010931Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.246029711Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.247985696Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:05.250091052Z [31m20251126-14:41:05.249 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.250956520Z [37m20251126-14:41:05.250 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:41:05.251929028Z [31m20251126-14:41:05.251 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.253261655Z [37m20251126-14:41:05.253 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:41:05.254201773Z [31m20251126-14:41:05.254 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.255087272Z [37m20251126-14:41:05.254 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:41:05.255988790Z [31m20251126-14:41:05.255 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.258148945Z [31m20251126-14:41:05.258 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:41:05.259343333Z Traceback (most recent call last):
2025-11-26T14:41:05.261097599Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.261123769Z     future = loop.run_in_executor(
2025-11-26T14:41:05.261130939Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.262799775Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.264913971Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.264932501Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.267108466Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:05.269474461Z [31m20251126-14:41:05.269 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.270492069Z [37m20251126-14:41:05.270 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:41:05.271392687Z [31m20251126-14:41:05.271 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.272283956Z [37m20251126-14:41:05.272 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:41:05.273191554Z [31m20251126-14:41:05.272 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.274313091Z [37m20251126-14:41:05.274 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:41:05.275171630Z [31m20251126-14:41:05.274 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.276084478Z [31m20251126-14:41:05.275 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:41:05.276920106Z Traceback (most recent call last):
2025-11-26T14:41:05.278623832Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.278635252Z     future = loop.run_in_executor(
2025-11-26T14:41:05.278641302Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.280293659Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.281941245Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.281952115Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.283765122Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:05.285756717Z [31m20251126-14:41:05.285 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.286698375Z [37m20251126-14:41:05.286 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:41:05.287668313Z [31m20251126-14:41:05.287 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.288644361Z [37m20251126-14:41:05.288 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:41:05.289595909Z [31m20251126-14:41:05.289 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.290530547Z [37m20251126-14:41:05.290 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:41:05.291498935Z [31m20251126-14:41:05.291 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.292616103Z [31m20251126-14:41:05.292 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:41:05.293575301Z Traceback (most recent call last):
2025-11-26T14:41:05.295238338Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.295261327Z     future = loop.run_in_executor(
2025-11-26T14:41:05.295267347Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.296827874Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.298452401Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.298474811Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.300079667Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:05.302077683Z [31m20251126-14:41:05.301 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.303016621Z [37m20251126-14:41:05.302 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:41:05.303915149Z [31m20251126-14:41:05.303 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.304797178Z [37m20251126-14:41:05.304 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:41:05.305702976Z [31m20251126-14:41:05.305 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.306592504Z [37m20251126-14:41:05.306 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:41:05.307487712Z [31m20251126-14:41:05.307 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:41:05.308392820Z [31m20251126-14:41:05.308 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:41:05.309491598Z Traceback (most recent call last):
2025-11-26T14:41:05.311108544Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.311123024Z     future = loop.run_in_executor(
2025-11-26T14:41:05.311128624Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.312697631Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.314310118Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.314323268Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.315907454Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:05.782072309Z [31m20251126-14:41:05.781 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 29 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:41:05.782095549Z Traceback (most recent call last):
2025-11-26T14:41:05.782101869Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:41:05.782107379Z     result = await async_task
2025-11-26T14:41:05.782113339Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.782118679Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:41:05.782124499Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:41:05.782130389Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.782135769Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:41:05.782140969Z     reward = await self.async_reward_fn(
2025-11-26T14:41:05.782146159Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.782151379Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:41:05.782161609Z     raise e
2025-11-26T14:41:05.782167669Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.782173109Z     future = loop.run_in_executor(
2025-11-26T14:41:05.782178359Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.782183629Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.782188829Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.782194069Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.782199309Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:05.787247189Z [31m20251126-14:41:05.785 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 25 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:41:05.787272619Z Traceback (most recent call last):
2025-11-26T14:41:05.787278588Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:41:05.787283858Z     result = await async_task
2025-11-26T14:41:05.787289068Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.787294238Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:41:05.787299528Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:41:05.787305058Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.787310278Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:41:05.787315678Z     reward = await self.async_reward_fn(
2025-11-26T14:41:05.787320838Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.787325988Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:41:05.787331318Z     raise e
2025-11-26T14:41:05.787336578Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.787341688Z     future = loop.run_in_executor(
2025-11-26T14:41:05.787346858Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.787351978Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.787357168Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.787362348Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.787367568Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:05.793040096Z [31m20251126-14:41:05.791 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 27 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:41:05.793062436Z Traceback (most recent call last):
2025-11-26T14:41:05.793068356Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:41:05.793073656Z     result = await async_task
2025-11-26T14:41:05.793079236Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.793084546Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:41:05.793089916Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:41:05.793095206Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.793100416Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:41:05.793105616Z     reward = await self.async_reward_fn(
2025-11-26T14:41:05.793114756Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.793120046Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:41:05.793125326Z     raise e
2025-11-26T14:41:05.793130856Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.793136046Z     future = loop.run_in_executor(
2025-11-26T14:41:05.793141286Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.793146516Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.793151706Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.793156986Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.793162216Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:05.801856048Z [31m20251126-14:41:05.795 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 30 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:41:05.801897308Z Traceback (most recent call last):
2025-11-26T14:41:05.801909908Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:41:05.801915468Z     result = await async_task
2025-11-26T14:41:05.801920688Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.801925868Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:41:05.801943568Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:41:05.801949248Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.801954478Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:41:05.801959648Z     reward = await self.async_reward_fn(
2025-11-26T14:41:05.801964758Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.801969898Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:41:05.801978698Z     raise e
2025-11-26T14:41:05.801983938Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.801989038Z     future = loop.run_in_executor(
2025-11-26T14:41:05.801994198Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.801999488Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.802004608Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.802009738Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.802014878Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:05.806224249Z [31m20251126-14:41:05.804 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 28 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:41:05.806246699Z Traceback (most recent call last):
2025-11-26T14:41:05.806253129Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:41:05.806258569Z     result = await async_task
2025-11-26T14:41:05.806263909Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.806269229Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:41:05.806274539Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:41:05.806279989Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.806285159Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:41:05.806290419Z     reward = await self.async_reward_fn(
2025-11-26T14:41:05.806295769Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.806300979Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:41:05.806306159Z     raise e
2025-11-26T14:41:05.806311509Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.806316739Z     future = loop.run_in_executor(
2025-11-26T14:41:05.806322099Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.806327449Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.806332649Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.806337969Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.806343159Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:05.810894289Z [31m20251126-14:41:05.809 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 31 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:41:05.810916789Z Traceback (most recent call last):
2025-11-26T14:41:05.810922809Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:41:05.810928299Z     result = await async_task
2025-11-26T14:41:05.810933539Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.810938909Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:41:05.810944399Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:41:05.810949639Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.810954889Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:41:05.810960159Z     reward = await self.async_reward_fn(
2025-11-26T14:41:05.810965329Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.810970569Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:41:05.810975759Z     raise e
2025-11-26T14:41:05.810981079Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.810996419Z     future = loop.run_in_executor(
2025-11-26T14:41:05.811204398Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.811214528Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.811221588Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.811227448Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.811232858Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:05.816196988Z [31m20251126-14:41:05.814 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 26 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:41:05.816224358Z Traceback (most recent call last):
2025-11-26T14:41:05.816230888Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:41:05.816236888Z     result = await async_task
2025-11-26T14:41:05.816242918Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.816248338Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:41:05.816254048Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:41:05.816259738Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.816265498Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:41:05.816271178Z     reward = await self.async_reward_fn(
2025-11-26T14:41:05.816276778Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.816282368Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:41:05.816287798Z     raise e
2025-11-26T14:41:05.816293418Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:41:05.816298868Z     future = loop.run_in_executor(
2025-11-26T14:41:05.816304048Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:05.816309248Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:41:05.816314538Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:41:05.816319748Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:41:05.816324888Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:41:06.914677581Z [rank0]:[W1126 14:41:06.040531579 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
2025-11-26T14:41:09.258450309Z E1126 14:41:09.256000 1038 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1104) of binary: /usr/bin/python3
2025-11-26T14:41:09.259420577Z Traceback (most recent call last):
2025-11-26T14:41:09.259453207Z   File "/usr/local/bin/torchrun", line 7, in <module>
2025-11-26T14:41:09.259579006Z     sys.exit(main())
2025-11-26T14:41:09.259675206Z              ^^^^^^
2025-11-26T14:41:09.259690296Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
2025-11-26T14:41:09.259990875Z     return f(*args, **kwargs)
2025-11-26T14:41:09.260015625Z            ^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:09.260023165Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 901, in main
2025-11-26T14:41:09.260670594Z     run(args)
2025-11-26T14:41:09.260688244Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 892, in run
2025-11-26T14:41:09.261424502Z     elastic_launch(
2025-11-26T14:41:09.261447932Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 143, in __call__
2025-11-26T14:41:09.261531662Z     return launch_agent(self._config, self._entrypoint, list(args))
2025-11-26T14:41:09.261603562Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:41:09.261615172Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
2025-11-26T14:41:09.261873991Z     raise ChildFailedError(
2025-11-26T14:41:09.261966611Z torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
2025-11-26T14:41:09.261973061Z ============================================================
2025-11-26T14:41:09.261979891Z examples/cloud_gsm8k/gsm8k_grpo_train.py FAILED
2025-11-26T14:41:09.261986431Z ------------------------------------------------------------
2025-11-26T14:41:09.261992241Z Failures:
2025-11-26T14:41:09.261997881Z   <NO_OTHER_FAILURES>
2025-11-26T14:41:09.262003781Z ------------------------------------------------------------
2025-11-26T14:41:09.262010321Z Root Cause (first observed failure):
2025-11-26T14:41:09.262015971Z [0]:
2025-11-26T14:41:09.262021931Z   time      : 2025-11-26_14:41:09
2025-11-26T14:41:09.262027521Z   host      : 01af0063c46e
2025-11-26T14:41:09.262033041Z   rank      : 0 (local_rank: 0)
2025-11-26T14:41:09.262038551Z   exitcode  : 1 (pid: 1104)
2025-11-26T14:41:09.262044031Z   error_file: <N/A>
2025-11-26T14:41:09.262049581Z   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
2025-11-26T14:41:09.262055301Z ============================================================
2025-11-26T14:41:10.046427341Z [37m20251126-14:41:10.045 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [387][0m
2025-11-26T14:41:10.323624881Z Killed
2025-11-26T14:41:10.334969507Z [37m20251126-14:41:10.334 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [1037][0m
2025-11-26T14:41:10.336086195Z Traceback (most recent call last):
2025-11-26T14:41:10.336092855Z   File "<frozen runpy>", line 198, in _run_module_as_main
2025-11-26T14:41:10.336097885Z   File "<frozen runpy>", line 88, in _run_code
2025-11-26T14:41:10.336101925Z   File "/workspace/AReaL/areal/launcher/local.py", line 405, in <module>
2025-11-26T14:41:10.336251615Z     main()
2025-11-26T14:41:10.336270794Z   File "/workspace/AReaL/areal/launcher/local.py", line 260, in main
2025-11-26T14:41:10.336358764Z     local_main(config, run_id=0)
2025-11-26T14:41:10.336410864Z   File "/workspace/AReaL/areal/launcher/local.py", line 399, in local_main
2025-11-26T14:41:10.336473584Z     raise e
2025-11-26T14:41:10.336481424Z   File "/workspace/AReaL/areal/launcher/local.py", line 375, in local_main
2025-11-26T14:41:10.336577424Z     launcher.wait(
2025-11-26T14:41:10.336584654Z   File "/workspace/AReaL/areal/launcher/local.py", line 235, in wait
2025-11-26T14:41:10.336618654Z     raise JobException(
2025-11-26T14:41:10.336622064Z areal.utils.launcher.JobException: Job gsm8k-grpo-cloud-fast_trial_20251126_143945:trainer JobState.COMPLETED at node local
2025-11-26T14:41:10.546501085Z [37m20251126-14:41:10.545 Local Scheduler INFO: Waiting for 0 local running processes, pids: [0m
2025-11-26T14:41:25.317683843Z ==========
2025-11-26T14:41:25.317728323Z == CUDA ==
2025-11-26T14:41:25.317806823Z ==========
2025-11-26T14:41:25.325506617Z CUDA Version 12.9.1
2025-11-26T14:41:25.328794400Z Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2025-11-26T14:41:25.331615734Z This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2025-11-26T14:41:25.331622504Z By pulling and using the container, you accept the terms and conditions of this license:
2025-11-26T14:41:25.331628494Z https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2025-11-26T14:41:25.331639944Z A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
2025-11-26T14:41:25.550489896Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:41:25.762505653Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:41:25.991675473Z Branch 'feature/reasoning_model' set up to track remote branch 'feature/reasoning_model' from 'origin'.
2025-11-26T14:41:25.991719373Z Your branch is up to date with 'origin/feature/reasoning_model'.
2025-11-26T14:41:26.106081924Z Checking AReaL installation...
2025-11-26T14:41:26.199112880Z AReaL already installed. Skipping installation.
2025-11-26T14:41:26.199137679Z Cleaning up any leftover GPU processes...
2025-11-26T14:41:29.220451891Z Checking for processes holding GPU device files...
2025-11-26T14:41:29.901625796Z Found processes holding GPU devices: 1
2025-11-26T14:41:29.901659526Z 144
2025-11-26T14:41:29.901662926Z 145
2025-11-26T14:41:29.901665836Z 93
2025-11-26T14:41:29.901668956Z Killing process 1...
2025-11-26T14:41:29.901673046Z Killing process 144...
2025-11-26T14:41:29.901753106Z Killing process 145...
2025-11-26T14:41:31.905755534Z Using fuser to kill processes on GPU devices...
2025-11-26T14:41:33.933945003Z Checking GPU...
2025-11-26T14:41:33.973214631Z NVIDIA A100 80GB PCIe, 81920 MiB
2025-11-26T14:41:34.002437250Z Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True for better memory management
2025-11-26T14:41:34.002494359Z Detected 1 GPU(s)
2025-11-26T14:41:34.002503589Z Checking GPU status...
2025-11-26T14:41:34.034243113Z index, name, utilization.gpu [%], memory.used [MiB], compute_mode
2025-11-26T14:41:34.040746529Z 0, NVIDIA A100 80GB PCIe, 0, 0, Default
2025-11-26T14:41:34.092750561Z Verifying GPU accessibility...
2025-11-26T14:41:34.827539324Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:41:34.827592214Z   import pynvml  # type: ignore[import]
2025-11-26T14:41:36.043967680Z GPU accessibility verified on attempt 1
2025-11-26T14:41:36.617075751Z Starting training...
2025-11-26T14:41:39.620851019Z Using FAST training configuration (20-30 minutes)
2025-11-26T14:41:39.625671909Z ==========================================
2025-11-26T14:41:39.625679019Z Starting GRPO Training (Cloud)
2025-11-26T14:41:39.625685219Z ==========================================
2025-11-26T14:41:39.625690579Z Config: fast
2025-11-26T14:41:39.625696979Z Config file: examples/cloud_gsm8k/gsm8k_grpo_fast.yaml
2025-11-26T14:41:39.625703249Z Training script: examples/cloud_gsm8k/gsm8k_grpo_train.py
2025-11-26T14:41:39.625708659Z Experiment: gsm8k-grpo-cloud-fast
2025-11-26T14:41:39.625714379Z Trial: trial_20251126_144139
2025-11-26T14:41:39.625719829Z GPU: NVIDIA A100 80GB PCIe (81920 MB)
2025-11-26T14:41:39.625725129Z WandB API key: e1adc5be02...
2025-11-26T14:41:39.625730399Z ==========================================
2025-11-26T14:41:40.622818524Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:41:40.622871794Z   import pynvml  # type: ignore[import]
2025-11-26T14:41:45.658508352Z [37m20251126-14:41:45.658 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:41:45.658545942Z [37m20251126-14:41:45.658 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:41:45.659032841Z [37m20251126-14:41:45.658 name-resolve INFO: No such name resolve path: ./tmp/areal/name_resolve/root/gsm8k-grpo-cloud-fast/trial_20251126_144139[0m
2025-11-26T14:41:45.825058124Z [37m20251126-14:41:45.824 Local Scheduler INFO: LocalLauncher: experiment_name=gsm8k-grpo-cloud-fast, trial_name=trial_20251126_144139, fileroot=/workspace/outputs/grpo, run_id=0, is_recover_run=False[0m
2025-11-26T14:41:45.833017867Z [37m20251126-14:41:45.832 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL CUDA_VISIBLE_DEVICES=0 stdbuf -oL python3 -m areal.launcher.sglang_server examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_144139 sglang.random_seed=1 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144139/llm_server.log[0m
2025-11-26T14:41:46.817060729Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:41:46.817101399Z   import pynvml  # type: ignore[import]
2025-11-26T14:41:48.764169667Z [37m20251126-14:41:48.763 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:41:48.764234487Z [37m20251126-14:41:48.763 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:41:48.912567997Z [37m20251126-14:41:48.912 SGLangServer Wrapper INFO: Launch command: python3 -m sglang.launch_server --host 172.18.0.2 --port 10766 --tokenizer-path Qwen/Qwen2.5-0.5B-Instruct --tokenizer-mode auto --load-format auto --trust-remote-code --device cuda --tp-size 1 --base-gpu-id 0 --nnodes 1 --node-rank 0 --dist-init-addr localhost:24255 --model-path Qwen/Qwen2.5-0.5B-Instruct --random-seed 1 --skip-tokenizer-init --disable-radix-cache --torch-compile-max-bs 32 --triton-attention-num-kv-splits 8 --num-continuous-decode-steps 1 --attention-backend flashinfer --context-length 4096 --mem-fraction-static 0.8 --chunked-prefill-size -1 --max-prefill-tokens 32768 --schedule-policy lpm --schedule-conservativeness 1.0 --cpu-offload-gb 0 --dtype bfloat16 --kv-cache-dtype auto --dp-size 1 --ep-size 1 --max-loaded-loras 1 --max-loras-per-batch 1 --lora-backend triton --log-level warning --log-level-http warning --log-requests-level 0 --enable-metrics --decode-log-interval 1[0m
2025-11-26T14:41:50.378758291Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:41:50.378801550Z   import pynvml  # type: ignore[import]
2025-11-26T14:41:57.948571429Z INFO 11-26 14:41:57 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:41:58.780981338Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:42:00.153797957Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:42:00.153826387Z   import pynvml  # type: ignore[import]
2025-11-26T14:42:00.201460178Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:42:00.201494127Z   import pynvml  # type: ignore[import]
2025-11-26T14:42:07.056488731Z INFO 11-26 14:42:07 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:42:07.084714852Z INFO 11-26 14:42:07 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:42:07.848714924Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:42:08.120678195Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:42:08.124854067Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:42:08.125731425Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:42:08.126628743Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:42:08.172715507Z [2025-11-26 14:42:08] MOE_RUNNER_BACKEND is not initialized, using triton backend
2025-11-26T14:42:08.810096344Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:42:08.810152513Z   warnings.warn(
2025-11-26T14:42:08.810157383Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:42:08.810161213Z   warnings.warn(
2025-11-26T14:42:10.225485243Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:42:10.405819506Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.55it/s]
2025-11-26T14:42:10.405854266Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.55it/s]
2025-11-26T14:42:13.858131746Z   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   4%|â–         | 1/23 [00:00<00:16,  1.35it/s]Capturing batches (bs=152 avail_mem=14.84 GB):   4%|â–         | 1/23 [00:00<00:16,  1.35it/s]Capturing batches (bs=144 avail_mem=14.82 GB):   4%|â–         | 1/23 [00:00<00:16,  1.35it/s]Capturing batches (bs=144 avail_mem=14.82 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.23it/s]Capturing batches (bs=136 avail_mem=14.81 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.23it/s]Capturing batches (bs=128 avail_mem=14.79 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.23it/s]Capturing batches (bs=128 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:00<00:02,  6.88it/s]Capturing batches (bs=120 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:00<00:02,  6.88it/s]Capturing batches (bs=112 avail_mem=14.78 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:01<00:02,  6.88it/s]Capturing batches (bs=112 avail_mem=14.78 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.17it/s]Capturing batches (bs=104 avail_mem=14.76 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.17it/s]Capturing batches (bs=96 avail_mem=14.75 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.17it/s] Capturing batches (bs=96 avail_mem=14.75 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.09it/s]Capturing batches (bs=88 avail_mem=14.73 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.09it/s]Capturing batches (bs=80 avail_mem=14.72 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.09it/s]Capturing batches (bs=80 avail_mem=14.72 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.64it/s]Capturing batches (bs=72 avail_mem=14.71 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.64it/s]Capturing batches (bs=64 avail_mem=14.69 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.64it/s]Capturing batches (bs=64 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 13.86it/s]Capturing batches (bs=56 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 13.86it/s]Capturing batches (bs=48 avail_mem=14.66 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 13.86it/s]Capturing batches (bs=48 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.80it/s]Capturing batches (bs=40 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.80it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.80it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.73it/s]Capturing batches (bs=24 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.73it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.73it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.27it/s]Capturing batches (bs=8 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.27it/s] Capturing batches (bs=4 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.27it/s]Capturing batches (bs=4 avail_mem=14.60 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:01<00:00, 14.41it/s]Capturing batches (bs=2 avail_mem=14.59 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:01<00:00, 14.41it/s]Capturing batches (bs=1 avail_mem=14.57 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:02<00:00, 14.41it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 14.95it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 10.85it/s]
2025-11-26T14:42:19.982162559Z [37m20251126-14:42:19.981 SGLangServer Wrapper INFO: SGLang server launched at: http://172.18.0.2:10766[0m
2025-11-26T14:42:20.855513712Z [37m20251126-14:42:20.855 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:10766[0m
2025-11-26T14:42:20.855560032Z [37m20251126-14:42:20.855 Local Scheduler INFO: LLM inference server launched at: AREAL_LLM_SERVER_ADDRS=172.18.0.2:10766[0m
2025-11-26T14:42:20.856331430Z [37m20251126-14:42:20.856 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL AREAL_LLM_SERVER_ADDRS=172.18.0.2:10766 AREAL_RECOVER_RUN=0 NCCL_CUMEM_ENABLE=0 NCCL_NVLS_ENABLE=0 CUDA_VISIBLE_DEVICES=0 stdbuf -oL torchrun --nnodes 1 --nproc-per-node 1 --master-addr localhost --master-port 49161 examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_144139 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144139/trainer.log[0m
2025-11-26T14:42:20.857312588Z [37m20251126-14:42:20.857 Local Scheduler INFO: Waiting for 2 local running processes, pids: 387 1037[0m
2025-11-26T14:42:21.614400855Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:42:21.614436935Z   import pynvml  # type: ignore[import]
2025-11-26T14:42:23.415462628Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:42:23.415499768Z   import pynvml  # type: ignore[import]
2025-11-26T14:42:32.806588288Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:42:32.806629358Z   warnings.warn(
2025-11-26T14:42:32.806636008Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:42:32.806673117Z   warnings.warn(
2025-11-26T14:42:34.478035962Z [37m20251126-14:42:34.477 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:42:34.478083712Z [37m20251126-14:42:34.477 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:42:34.688763451Z [37m20251126-14:42:34.688 [FSDP Engine Rank 0] INFO: Initializing device mesh with parallel dims (dp=1, sp=1, tp=1, ep=1, etp=1, world_size=1).[0m
2025-11-26T14:42:34.691700225Z [37m20251126-14:42:34.691 [FSDP Engine Rank 0] INFO: Data parallel head 0 and rank 0[0m
2025-11-26T14:42:35.872119176Z [FAST] Limiting dataset from 7473 to 200 samples
2025-11-26T14:42:35.875655959Z [37m20251126-14:42:35.875 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:10766[0m
2025-11-26T14:42:35.875681099Z [37m20251126-14:42:35.875 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:42:35.875699229Z [37m20251126-14:42:35.875 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:42:36.879187750Z [37m20251126-14:42:36.878 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:42:36.882254344Z [37m20251126-14:42:36.882 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:10766[0m
2025-11-26T14:42:36.882305854Z [37m20251126-14:42:36.882 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:42:36.882385244Z [37m20251126-14:42:36.882 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:42:37.885677875Z [37m20251126-14:42:37.885 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:42:38.644639788Z [37m20251126-14:42:38.644 [FSDP Engine Rank 0] INFO: Model creation and loading time: 0.5929483200889081[0m
2025-11-26T14:42:39.135497992Z [37m20251126-14:42:39.135 [FSDP Engine Rank 0] INFO: Applying FSDP2 with N-D parallelism for 0.49 seconds[0m
2025-11-26T14:42:39.136370760Z [37m20251126-14:42:39.136 [FSDP Engine Rank 0] INFO: Create optimizer time: 0.0008864381816238165[0m
2025-11-26T14:42:39.674419144Z wandb: Currently logged in as: tong-zhao (tong-zhao-georgia-institute-of-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-11-26T14:42:39.677355948Z wandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.
2025-11-26T14:42:40.520816604Z wandb: setting up run gsm8k-grpo-cloud-fast_trial_20251126_144139_train
2025-11-26T14:42:40.657364379Z wandb: Tracking run with wandb version 0.22.2
2025-11-26T14:42:40.657397249Z wandb: Run data is saved locally in /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144139/wandb/run-20251126_144239-gsm8k-grpo-cloud-fast_trial_20251126_144139_train
2025-11-26T14:42:40.657407129Z wandb: Run `wandb offline` to turn off syncing.
2025-11-26T14:42:40.657412949Z wandb: Syncing run trial_20251126_144139
2025-11-26T14:42:40.657418729Z wandb: â­ï¸ View project at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local
2025-11-26T14:42:40.657424969Z wandb: ðŸš€ View run at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local/runs/gsm8k-grpo-cloud-fast_trial_20251126_144139_train
2025-11-26T14:42:40.995046632Z wandb: Detected [openai] in use.
2025-11-26T14:42:40.995228372Z wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-11-26T14:42:40.995475322Z wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-11-26T14:42:41.000400881Z swanlab: SwanLab run disabled, the data will not be saved or uploaded.
2025-11-26T14:42:41.595537017Z ================================================================================
2025-11-26T14:42:41.596061246Z [FAST MODE]
2025-11-26T14:42:41.596556635Z   Dataset size: 200 samples (limited from 7473)
2025-11-26T14:42:41.596952244Z   Batch size: 8
2025-11-26T14:42:41.597459593Z   Steps per epoch: 25
2025-11-26T14:42:41.597808402Z   Total epochs: 1
2025-11-26T14:42:41.598232551Z   Total steps: 25
2025-11-26T14:42:41.598611260Z   Estimated time: ~25 minutes (~0.4 hours) at ~1 step/min
2025-11-26T14:42:41.598996059Z   Circuit breaker: Enabled (threshold: 50 consecutive zero rewards)
2025-11-26T14:42:41.599205079Z ================================================================================
2025-11-26T14:42:49.708761319Z [37m20251126-14:42:49.707 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4877, 553], padded to: [5120, 768], padding lengths: [243, 215][0m
2025-11-26T14:42:51.145296024Z [37m20251126-14:42:51.144 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 0.93, memory reserved (GB): 3.22, device memory used/total (GB): 69.68/79.25[0m
2025-11-26T14:42:51.306032978Z [37m20251126-14:42:51.305 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 0.93, memory reserved (GB): 3.22, device memory used/total (GB): 69.68/79.25[0m
2025-11-26T14:42:51.338086931Z [37m20251126-14:42:51.337 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4877, 553], padded to: [5120, 768], padding lengths: [243, 215][0m
2025-11-26T14:42:53.021181181Z [37m20251126-14:42:53.020 /workspace/AReaL/areal/utils/device.py INFO: ppo update, memory allocated (GB): 3.70, memory reserved (GB): 9.08, device memory used/total (GB): 76.05/79.25[0m
2025-11-26T14:42:56.214420553Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:42:56.359399580Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.91it/s]
2025-11-26T14:42:56.359437270Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.90it/s]
2025-11-26T14:42:56.380723135Z [37m20251126-14:42:56.379 [Remote Inference Engine Rank 0] INFO: Loading weights from disk done in 3.36s. Respond time: 0.07s.[0m
2025-11-26T14:42:56.389088788Z [92m20251126-14:42:56.388 StatsLogger INFO: Epoch 1/1 Step 1/25 Train step 1/25 done.[0m
2025-11-26T14:42:56.389577667Z [92m20251126-14:42:56.389 StatsLogger INFO: Stats (1/1):[0m
2025-11-26T14:42:56.401815491Z [92m20251126-14:42:56.401 StatsLogger INFO:
2025-11-26T14:42:56.401839721Z â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â••
2025-11-26T14:42:56.401846471Z â”‚ grpo_actor/actor_loss/avg        â”‚ -5.4797e-04 â”‚ grpo_actor/entropy/min             â”‚  3.4554e-07 â”‚ grpo_actor/vocab_min_logits/min  â”‚ -3.2250e+01 â”‚ grpo_actor/no_eos_ratios/min    â”‚ 0.0000e+00 â”‚
2025-11-26T14:42:56.401853491Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.401859551Z â”‚ grpo_actor/actor_loss/min        â”‚ -3.7367e+00 â”‚ grpo_actor/entropy/max             â”‚  4.0370e+00 â”‚ grpo_actor/vocab_min_logits/max  â”‚ -6.9688e+00 â”‚ grpo_actor/no_eos_ratios/max    â”‚ 1.0000e+00 â”‚
2025-11-26T14:42:56.401967731Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.401978781Z â”‚ grpo_actor/actor_loss/max        â”‚  3.5840e+00 â”‚ grpo_actor/grad_norm               â”‚  4.4070e+00 â”‚ grpo_actor/advantages/avg        â”‚ -1.2887e-08 â”‚ grpo_actor/prompt_len/avg       â”‚ 9.8875e+01 â”‚
2025-11-26T14:42:56.401984511Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.401989911Z â”‚ grpo_actor/approx_kl/avg         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/avg   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/min        â”‚ -3.0786e+00 â”‚ grpo_actor/prompt_len/min       â”‚ 8.4000e+01 â”‚
2025-11-26T14:42:56.401995311Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402000611Z â”‚ grpo_actor/approx_kl/min         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/min   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/max        â”‚  3.0626e+00 â”‚ grpo_actor/prompt_len/max       â”‚ 1.1400e+02 â”‚
2025-11-26T14:42:56.402006851Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402012301Z â”‚ grpo_actor/approx_kl/max         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/max   â”‚  1.0000e+00 â”‚ grpo_actor/behav_imp_weight_cap  â”‚  5.0000e+00 â”‚ grpo_actor/seq_len/avg          â”‚ 3.3938e+02 â”‚
2025-11-26T14:42:56.402018141Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402035661Z â”‚ grpo_actor/behave_approx_kl/avg  â”‚ -3.6782e-04 â”‚ grpo_actor/lr                      â”‚  1.7000e-05 â”‚ grpo_actor/correct_seq_len/avg   â”‚  2.9300e+02 â”‚ grpo_actor/seq_len/min          â”‚ 2.7000e+02 â”‚
2025-11-26T14:42:56.402041861Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402047291Z â”‚ grpo_actor/behave_approx_kl/min  â”‚ -4.4094e-01 â”‚ grpo_actor/n_tokens                â”‚  5.4300e+03 â”‚ grpo_actor/correct_seq_len/min   â”‚  2.9300e+02 â”‚ grpo_actor/seq_len/max          â”‚ 3.7000e+02 â”‚
2025-11-26T14:42:56.402052561Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402057851Z â”‚ grpo_actor/behave_approx_kl/max  â”‚  2.4609e-01 â”‚ grpo_actor/n_valid_tokens          â”‚  3.8480e+03 â”‚ grpo_actor/correct_seq_len/max   â”‚  2.9300e+02 â”‚ grpo_actor/task_reward/avg      â”‚ 6.2500e-02 â”‚
2025-11-26T14:42:56.402063351Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402068761Z â”‚ grpo_actor/behave_imp_weight/avg â”‚  1.0002e+00 â”‚ grpo_actor/new_logp/avg            â”‚ -3.9115e-01 â”‚ grpo_actor/eps_clip              â”‚  4.0000e-01 â”‚ grpo_actor/task_reward/min      â”‚ 0.0000e+00 â”‚
2025-11-26T14:42:56.402074431Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402079721Z â”‚ grpo_actor/behave_imp_weight/min â”‚  6.4343e-01 â”‚ grpo_actor/new_logp/min            â”‚ -1.5728e+01 â”‚ grpo_actor/final_reward/avg      â”‚  0.0000e+00 â”‚ grpo_actor/task_reward/max      â”‚ 1.0000e+00 â”‚
2025-11-26T14:42:56.402085041Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402099170Z â”‚ grpo_actor/behave_imp_weight/max â”‚  1.2790e+00 â”‚ grpo_actor/new_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/final_reward/min      â”‚ -7.0711e-01 â”‚ grpo_actor/use_dual_clip        â”‚ 0.0000e+00 â”‚
2025-11-26T14:42:56.402104780Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402110150Z â”‚ grpo_actor/clip_ratio/avg        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/avg            â”‚ -3.9078e-01 â”‚ grpo_actor/final_reward/max      â”‚  7.0711e-01 â”‚ timeperf/compute_advantage      â”‚ 1.6084e-01 â”‚
2025-11-26T14:42:56.402115880Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402121200Z â”‚ grpo_actor/clip_ratio/min        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/min            â”‚ -1.5708e+01 â”‚ grpo_actor/incorrect_seq_len/avg â”‚  3.4247e+02 â”‚ timeperf/recompute_logp         â”‚ 1.5446e+00 â”‚
2025-11-26T14:42:56.402126480Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402131690Z â”‚ grpo_actor/clip_ratio/max        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/incorrect_seq_len/min â”‚  2.7000e+02 â”‚ timeperf/rollout                â”‚ 8.0011e+00 â”‚
2025-11-26T14:42:56.402137280Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402142740Z â”‚ grpo_actor/clipped_tokens        â”‚  0.0000e+00 â”‚ grpo_actor/unclipped_behave_tokens â”‚  3.8480e+03 â”‚ grpo_actor/incorrect_seq_len/max â”‚  3.7000e+02 â”‚ timeperf/checkpoint_for_recover â”‚ 3.8780e-05 â”‚
2025-11-26T14:42:56.402148000Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402158770Z â”‚ grpo_actor/dual_clip_ratio/avg   â”‚  0.0000e+00 â”‚ grpo_actor/update_successful       â”‚  1.0000e+00 â”‚ grpo_actor/kl_rewards/avg        â”‚  0.0000e+00 â”‚ timeperf/eval                   â”‚ 6.8680e-05 â”‚
2025-11-26T14:42:56.402164700Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402170160Z â”‚ grpo_actor/dual_clip_ratio/min   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/avg    â”‚  2.5616e+01 â”‚ grpo_actor/kl_rewards/min        â”‚  0.0000e+00 â”‚ timeperf/save                   â”‚ 1.3418e-04 â”‚
2025-11-26T14:42:56.402175360Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402181460Z â”‚ grpo_actor/dual_clip_ratio/max   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/min    â”‚  1.2938e+01 â”‚ grpo_actor/kl_rewards/max        â”‚  0.0000e+00 â”‚ timeperf/train_step             â”‚ 1.7152e+00 â”‚
2025-11-26T14:42:56.402186720Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402191990Z â”‚ grpo_actor/dual_clipped_tokens   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/max    â”‚  4.1250e+01 â”‚ grpo_actor/mask_no_eos_with_zero â”‚  0.0000e+00 â”‚ timeperf/update_weights         â”‚ 3.3607e+00 â”‚
2025-11-26T14:42:56.402197630Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:42:56.402202950Z â”‚ grpo_actor/entropy/avg           â”‚  3.8637e-01 â”‚ grpo_actor/vocab_min_logits/avg    â”‚ -1.5612e+01 â”‚ grpo_actor/no_eos_ratios/avg     â”‚  1.2500e-01 â”‚ rollout/reward                  â”‚ 1.0417e-01 â”‚
2025-11-26T14:42:56.402213710Z â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•›[0m
2025-11-26T14:42:56.513830647Z [37m20251126-14:42:56.513 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [5022, 120], padded to: [5120, 256], padding lengths: [98, 136][0m
2025-11-26T14:42:57.422320067Z [37m20251126-14:42:57.421 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 3.70, memory reserved (GB): 9.08, device memory used/total (GB): 75.97/79.25[0m
2025-11-26T14:42:59.120808815Z [37m20251126-14:42:59.120 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 3.70, memory reserved (GB): 9.08, device memory used/total (GB): 76.03/79.25[0m
2025-11-26T14:42:59.281749278Z [37m20251126-14:42:59.281 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [5022, 120], padded to: [5120, 256], padding lengths: [98, 136][0m
2025-11-26T14:42:59.957032476Z Traceback (most recent call last):
2025-11-26T14:42:59.958374153Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:42:59.958402663Z     main(sys.argv[1:])
2025-11-26T14:42:59.958576123Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:42:59.958584583Z     stats = actor.ppo_update(batch)
2025-11-26T14:42:59.958588233Z             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:42:59.958815442Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:42:59.958822522Z     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:42:59.958829512Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:42:59.959307541Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:42:59.959331661Z     train_stat = self.engine.train_batch(
2025-11-26T14:42:59.959338091Z                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:42:59.959780520Z   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:42:59.959792550Z     loss.backward()
2025-11-26T14:42:59.960308529Z   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:42:59.960328709Z     torch.autograd.backward(
2025-11-26T14:42:59.960791408Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:42:59.960803238Z     _engine_run_backward(
2025-11-26T14:42:59.961172017Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:42:59.961189497Z     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:42:59.961194567Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:42:59.961653356Z torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.07 GiB is free. Process 1225950 has 64.87 GiB memory in use. Process 1226594 has 13.30 GiB memory in use. Of the allocated memory 11.09 GiB is allocated by PyTorch, and 143.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:42:59.965345718Z [rank0]: Traceback (most recent call last):
2025-11-26T14:42:59.965354218Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:42:59.965357018Z [rank0]:     main(sys.argv[1:])
2025-11-26T14:42:59.965359788Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:42:59.965362518Z [rank0]:     stats = actor.ppo_update(batch)
2025-11-26T14:42:59.965365348Z [rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:42:59.965368738Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:42:59.965371388Z [rank0]:     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:42:59.965374478Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:42:59.965377298Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:42:59.965379968Z [rank0]:     train_stat = self.engine.train_batch(
2025-11-26T14:42:59.965382598Z [rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:42:59.965385128Z [rank0]:   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:42:59.965388048Z [rank0]:     loss.backward()
2025-11-26T14:42:59.965390838Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:42:59.965393508Z [rank0]:     torch.autograd.backward(
2025-11-26T14:42:59.965397348Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:42:59.965399898Z [rank0]:     _engine_run_backward(
2025-11-26T14:42:59.965402748Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:42:59.965405998Z [rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:42:59.965408708Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:42:59.965411798Z [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.07 GiB is free. Process 1225950 has 64.87 GiB memory in use. Process 1226594 has 13.30 GiB memory in use. Of the allocated memory 11.09 GiB is allocated by PyTorch, and 143.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:43:00.101728123Z [31m20251126-14:43:00.101 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.102409692Z [37m20251126-14:43:00.102 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:43:00.103474620Z [31m20251126-14:43:00.103 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.103977638Z [37m20251126-14:43:00.103 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:43:00.104441858Z [31m20251126-14:43:00.104 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.104864657Z [37m20251126-14:43:00.104 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:43:00.105307966Z [31m20251126-14:43:00.105 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.105834185Z [31m20251126-14:43:00.105 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:43:00.106965182Z Traceback (most recent call last):
2025-11-26T14:43:00.107579831Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:43:00.107589871Z     future = loop.run_in_executor(
2025-11-26T14:43:00.107596901Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.108115200Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:43:00.108575779Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:43:00.108590579Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:43:00.109067738Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:43:00.110821004Z [31m20251126-14:43:00.110 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.111272963Z [37m20251126-14:43:00.111 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:43:00.111691692Z [31m20251126-14:43:00.111 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.112108031Z [37m20251126-14:43:00.111 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:43:00.112554901Z [31m20251126-14:43:00.112 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.113012840Z [37m20251126-14:43:00.112 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:43:00.113433749Z [31m20251126-14:43:00.113 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.113828228Z [31m20251126-14:43:00.113 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:43:00.115215105Z Traceback (most recent call last):
2025-11-26T14:43:00.116499112Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:43:00.116518492Z     future = loop.run_in_executor(
2025-11-26T14:43:00.116525482Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.116977521Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:43:00.117408660Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:43:00.117420750Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:43:00.117839249Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:43:00.119489616Z [31m20251126-14:43:00.119 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.119938415Z [37m20251126-14:43:00.119 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:43:00.120359634Z [31m20251126-14:43:00.120 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.120757723Z [37m20251126-14:43:00.120 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:43:00.121144483Z [31m20251126-14:43:00.120 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.121518102Z [37m20251126-14:43:00.121 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:43:00.121942281Z [31m20251126-14:43:00.121 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.122353500Z [31m20251126-14:43:00.122 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:43:00.122816199Z Traceback (most recent call last):
2025-11-26T14:43:00.123431568Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:43:00.123453638Z     future = loop.run_in_executor(
2025-11-26T14:43:00.123460888Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.123920587Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:43:00.124409226Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:43:00.124423746Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:43:00.124911075Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:43:00.126514871Z [31m20251126-14:43:00.126 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.126943140Z [37m20251126-14:43:00.126 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:43:00.127380440Z [31m20251126-14:43:00.127 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.127784999Z [37m20251126-14:43:00.127 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:43:00.128230308Z [31m20251126-14:43:00.128 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.128629237Z [37m20251126-14:43:00.128 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:43:00.129066636Z [31m20251126-14:43:00.128 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.129468265Z [31m20251126-14:43:00.129 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:43:00.129911664Z Traceback (most recent call last):
2025-11-26T14:43:00.130499343Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:43:00.130513593Z     future = loop.run_in_executor(
2025-11-26T14:43:00.130520353Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.130979432Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:43:00.131451401Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:43:00.131466021Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:43:00.131935200Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:43:00.133586737Z [31m20251126-14:43:00.133 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.134032916Z [37m20251126-14:43:00.133 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:43:00.134456705Z [31m20251126-14:43:00.134 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.134872004Z [37m20251126-14:43:00.134 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:43:00.135318323Z [31m20251126-14:43:00.135 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.135696842Z [37m20251126-14:43:00.135 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:43:00.136137591Z [31m20251126-14:43:00.135 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.136504650Z [31m20251126-14:43:00.136 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:43:00.136896410Z Traceback (most recent call last):
2025-11-26T14:43:00.137434399Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:43:00.137448569Z     future = loop.run_in_executor(
2025-11-26T14:43:00.137455928Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.137854428Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:43:00.138296827Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:43:00.138314217Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:43:00.138730826Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:43:00.140245363Z [31m20251126-14:43:00.139 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.140662062Z [37m20251126-14:43:00.140 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:43:00.141090131Z [31m20251126-14:43:00.140 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.141489420Z [37m20251126-14:43:00.141 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:43:00.141843899Z [31m20251126-14:43:00.141 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.142295058Z [37m20251126-14:43:00.142 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:43:00.142776957Z [31m20251126-14:43:00.142 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:43:00.143208416Z [31m20251126-14:43:00.142 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:43:00.143662186Z Traceback (most recent call last):
2025-11-26T14:43:00.144195254Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:43:00.144210674Z     future = loop.run_in_executor(
2025-11-26T14:43:00.144217374Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.144607844Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:43:00.145127652Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:43:00.145141472Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:43:00.145604261Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:43:00.226394322Z [31m20251126-14:43:00.223 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 27 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:43:00.226418002Z Traceback (most recent call last):
2025-11-26T14:43:00.226425052Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:43:00.226432062Z     result = await async_task
2025-11-26T14:43:00.226437572Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.226442792Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:43:00.226448102Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:43:00.226453492Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.226458842Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:43:00.226464172Z     reward = await self.async_reward_fn(
2025-11-26T14:43:00.226469402Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.226474602Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:43:00.226479882Z     raise e
2025-11-26T14:43:00.226485642Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:43:00.226490962Z     future = loop.run_in_executor(
2025-11-26T14:43:00.226496232Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.226501432Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:43:00.226506722Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:43:00.226511982Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:43:00.226517302Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:43:00.227805859Z [31m20251126-14:43:00.226 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 30 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:43:00.227822209Z Traceback (most recent call last):
2025-11-26T14:43:00.227829329Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:43:00.227835969Z     result = await async_task
2025-11-26T14:43:00.227842239Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.227847599Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:43:00.227852859Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:43:00.227858189Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.227863449Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:43:00.227868839Z     reward = await self.async_reward_fn(
2025-11-26T14:43:00.227874099Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.227922629Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:43:00.227928099Z     raise e
2025-11-26T14:43:00.227933379Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:43:00.227938589Z     future = loop.run_in_executor(
2025-11-26T14:43:00.227943769Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.227948929Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:43:00.227954279Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:43:00.227973169Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:43:00.227979739Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:43:00.234641255Z [31m20251126-14:43:00.228 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 29 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:43:00.234665665Z Traceback (most recent call last):
2025-11-26T14:43:00.234672615Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:43:00.234678985Z     result = await async_task
2025-11-26T14:43:00.234684375Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.234689625Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:43:00.234694905Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:43:00.234700535Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.234705825Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:43:00.234711115Z     reward = await self.async_reward_fn(
2025-11-26T14:43:00.234716535Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.234721835Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:43:00.234727155Z     raise e
2025-11-26T14:43:00.234732375Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:43:00.234737615Z     future = loop.run_in_executor(
2025-11-26T14:43:00.234742935Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.234748595Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:43:00.234753875Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:43:00.234759185Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:43:00.234764425Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:43:00.237209270Z [31m20251126-14:43:00.235 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 31 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:43:00.237225460Z Traceback (most recent call last):
2025-11-26T14:43:00.237231200Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:43:00.237236670Z     result = await async_task
2025-11-26T14:43:00.237242040Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.237247300Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:43:00.237252500Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:43:00.237257850Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.237263030Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:43:00.237268220Z     reward = await self.async_reward_fn(
2025-11-26T14:43:00.237273540Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.237278880Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:43:00.237284350Z     raise e
2025-11-26T14:43:00.237289640Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:43:00.237294880Z     future = loop.run_in_executor(
2025-11-26T14:43:00.237300090Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.237305270Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:43:00.237310610Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:43:00.237315920Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:43:00.237321150Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:43:00.238760467Z [31m20251126-14:43:00.237 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 28 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:43:00.238782196Z Traceback (most recent call last):
2025-11-26T14:43:00.238787956Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:43:00.238793316Z     result = await async_task
2025-11-26T14:43:00.238808926Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.238814826Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:43:00.238820226Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:43:00.238825576Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.238831176Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:43:00.238836406Z     reward = await self.async_reward_fn(
2025-11-26T14:43:00.238841706Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.238846906Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:43:00.238852196Z     raise e
2025-11-26T14:43:00.238857716Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:43:00.238862996Z     future = loop.run_in_executor(
2025-11-26T14:43:00.238868286Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.238873516Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:43:00.238901826Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:43:00.238907326Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:43:00.238912616Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:43:00.240333363Z [31m20251126-14:43:00.239 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 25 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:43:00.240346763Z Traceback (most recent call last):
2025-11-26T14:43:00.240352423Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:43:00.240357713Z     result = await async_task
2025-11-26T14:43:00.240363063Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.240368293Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:43:00.240373623Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:43:00.240378913Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.240384323Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:43:00.240393293Z     reward = await self.async_reward_fn(
2025-11-26T14:43:00.240398663Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.240404193Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:43:00.240409473Z     raise e
2025-11-26T14:43:00.240414763Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:43:00.240420063Z     future = loop.run_in_executor(
2025-11-26T14:43:00.240425283Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:00.240430593Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:43:00.240435873Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:43:00.240441093Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:43:00.240446453Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:43:02.169011719Z [1;34mwandb[0m:
2025-11-26T14:43:02.169045519Z [1;34mwandb[0m: ðŸš€ View run [33mtrial_20251126_144139[0m at: [34m[0m
2025-11-26T14:43:02.169051679Z [1;34mwandb[0m: Find logs at: [1;35m../outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144139/wandb/run-20251126_144239-gsm8k-grpo-cloud-fast_trial_20251126_144139_train/logs[0m
2025-11-26T14:43:03.887793325Z [rank0]:[W1126 14:43:03.013592904 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
2025-11-26T14:43:06.063933234Z E1126 14:43:06.062000 1038 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1104) of binary: /usr/bin/python3
2025-11-26T14:43:06.064831352Z Traceback (most recent call last):
2025-11-26T14:43:06.064845702Z   File "/usr/local/bin/torchrun", line 7, in <module>
2025-11-26T14:43:06.065010001Z     sys.exit(main())
2025-11-26T14:43:06.065047711Z              ^^^^^^
2025-11-26T14:43:06.065053801Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
2025-11-26T14:43:06.065300961Z     return f(*args, **kwargs)
2025-11-26T14:43:06.065314811Z            ^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:06.065320681Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 901, in main
2025-11-26T14:43:06.065866990Z     run(args)
2025-11-26T14:43:06.065892250Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 892, in run
2025-11-26T14:43:06.066429859Z     elastic_launch(
2025-11-26T14:43:06.066443358Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 143, in __call__
2025-11-26T14:43:06.066508038Z     return launch_agent(self._config, self._entrypoint, list(args))
2025-11-26T14:43:06.066517978Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:43:06.066558778Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
2025-11-26T14:43:06.066777278Z     raise ChildFailedError(
2025-11-26T14:43:06.066790878Z torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
2025-11-26T14:43:06.066797368Z ============================================================
2025-11-26T14:43:06.066803488Z examples/cloud_gsm8k/gsm8k_grpo_train.py FAILED
2025-11-26T14:43:06.066809698Z ------------------------------------------------------------
2025-11-26T14:43:06.066815418Z Failures:
2025-11-26T14:43:06.066821608Z   <NO_OTHER_FAILURES>
2025-11-26T14:43:06.066827748Z ------------------------------------------------------------
2025-11-26T14:43:06.066833018Z Root Cause (first observed failure):
2025-11-26T14:43:06.066838268Z [0]:
2025-11-26T14:43:06.066844438Z   time      : 2025-11-26_14:43:06
2025-11-26T14:43:06.066850248Z   host      : 01af0063c46e
2025-11-26T14:43:06.066855528Z   rank      : 0 (local_rank: 0)
2025-11-26T14:43:06.066861348Z   exitcode  : 1 (pid: 1104)
2025-11-26T14:43:06.066866548Z   error_file: <N/A>
2025-11-26T14:43:06.066871778Z   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
2025-11-26T14:43:06.066889028Z ============================================================
2025-11-26T14:43:06.953506383Z [37m20251126-14:43:06.953 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [387][0m
2025-11-26T14:43:07.238473927Z Killed
2025-11-26T14:43:07.260671151Z [37m20251126-14:43:07.260 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [1037][0m
2025-11-26T14:43:07.261748759Z Traceback (most recent call last):
2025-11-26T14:43:07.261767499Z   File "<frozen runpy>", line 198, in _run_module_as_main
2025-11-26T14:43:07.261772869Z   File "<frozen runpy>", line 88, in _run_code
2025-11-26T14:43:07.261776889Z   File "/workspace/AReaL/areal/launcher/local.py", line 405, in <module>
2025-11-26T14:43:07.261945648Z     main()
2025-11-26T14:43:07.261955508Z   File "/workspace/AReaL/areal/launcher/local.py", line 260, in main
2025-11-26T14:43:07.262009278Z     local_main(config, run_id=0)
2025-11-26T14:43:07.262039438Z   File "/workspace/AReaL/areal/launcher/local.py", line 399, in local_main
2025-11-26T14:43:07.262111408Z     raise e
2025-11-26T14:43:07.262129888Z   File "/workspace/AReaL/areal/launcher/local.py", line 375, in local_main
2025-11-26T14:43:07.262214258Z     launcher.wait(
2025-11-26T14:43:07.262220808Z   File "/workspace/AReaL/areal/launcher/local.py", line 235, in wait
2025-11-26T14:43:07.262285298Z     raise JobException(
2025-11-26T14:43:07.262291058Z areal.utils.launcher.JobException: Job gsm8k-grpo-cloud-fast_trial_20251126_144139:trainer JobState.COMPLETED at node local
2025-11-26T14:43:07.501048268Z [37m20251126-14:43:07.500 Local Scheduler INFO: Waiting for 0 local running processes, pids: [0m
2025-11-26T14:43:20.234576588Z ==========
2025-11-26T14:43:20.234586958Z == CUDA ==
2025-11-26T14:43:20.234651978Z ==========
2025-11-26T14:43:20.242662601Z CUDA Version 12.9.1
2025-11-26T14:43:20.245738305Z Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2025-11-26T14:43:20.249077468Z This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2025-11-26T14:43:20.249084528Z By pulling and using the container, you accept the terms and conditions of this license:
2025-11-26T14:43:20.249090658Z https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2025-11-26T14:43:20.249102058Z A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
2025-11-26T14:43:20.473797818Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:43:20.682718561Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:43:20.923304128Z Branch 'feature/reasoning_model' set up to track remote branch 'feature/reasoning_model' from 'origin'.
2025-11-26T14:43:20.923346798Z Your branch is up to date with 'origin/feature/reasoning_model'.
2025-11-26T14:43:21.031445491Z Checking AReaL installation...
2025-11-26T14:43:21.122412981Z AReaL already installed. Skipping installation.
2025-11-26T14:43:21.122443221Z Cleaning up any leftover GPU processes...
2025-11-26T14:43:24.150350759Z Checking for processes holding GPU device files...
2025-11-26T14:43:24.845290195Z Found processes holding GPU devices: 1
2025-11-26T14:43:24.845329715Z 144
2025-11-26T14:43:24.845333235Z 145
2025-11-26T14:43:24.845335775Z 93
2025-11-26T14:43:24.845338385Z Killing process 1...
2025-11-26T14:43:24.845368245Z Killing process 144...
2025-11-26T14:43:24.845618915Z Killing process 145...
2025-11-26T14:43:26.850075843Z Using fuser to kill processes on GPU devices...
2025-11-26T14:43:28.877812622Z Checking GPU...
2025-11-26T14:43:28.917240099Z NVIDIA A100 80GB PCIe, 81920 MiB
2025-11-26T14:43:28.942221567Z Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True for better memory management
2025-11-26T14:43:28.942266737Z Detected 1 GPU(s)
2025-11-26T14:43:28.942273327Z Checking GPU status...
2025-11-26T14:43:28.972418844Z index, name, utilization.gpu [%], memory.used [MiB], compute_mode
2025-11-26T14:43:28.979500289Z 0, NVIDIA A100 80GB PCIe, 0, 0, Default
2025-11-26T14:43:29.032729068Z Verifying GPU accessibility...
2025-11-26T14:43:29.790402263Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:43:29.790457913Z   import pynvml  # type: ignore[import]
2025-11-26T14:43:31.016147810Z GPU accessibility verified on attempt 1
2025-11-26T14:43:31.603490712Z Starting training...
2025-11-26T14:43:34.607763289Z Using FAST training configuration (20-30 minutes)
2025-11-26T14:43:34.611630521Z ==========================================
2025-11-26T14:43:34.611636371Z Starting GRPO Training (Cloud)
2025-11-26T14:43:34.611641521Z ==========================================
2025-11-26T14:43:34.611646381Z Config: fast
2025-11-26T14:43:34.611651841Z Config file: examples/cloud_gsm8k/gsm8k_grpo_fast.yaml
2025-11-26T14:43:34.611656361Z Training script: examples/cloud_gsm8k/gsm8k_grpo_train.py
2025-11-26T14:43:34.611661101Z Experiment: gsm8k-grpo-cloud-fast
2025-11-26T14:43:34.611670640Z Trial: trial_20251126_144334
2025-11-26T14:43:34.611675340Z GPU: NVIDIA A100 80GB PCIe (81920 MB)
2025-11-26T14:43:34.611719940Z WandB API key: e1adc5be02...
2025-11-26T14:43:34.611732370Z ==========================================
2025-11-26T14:43:35.600523152Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:43:35.600578722Z   import pynvml  # type: ignore[import]
2025-11-26T14:43:40.931637033Z [37m20251126-14:43:40.931 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:43:40.931709403Z [37m20251126-14:43:40.931 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:43:40.932176272Z [37m20251126-14:43:40.931 name-resolve INFO: No such name resolve path: ./tmp/areal/name_resolve/root/gsm8k-grpo-cloud-fast/trial_20251126_144334[0m
2025-11-26T14:43:41.097327447Z [37m20251126-14:43:41.097 Local Scheduler INFO: LocalLauncher: experiment_name=gsm8k-grpo-cloud-fast, trial_name=trial_20251126_144334, fileroot=/workspace/outputs/grpo, run_id=0, is_recover_run=False[0m
2025-11-26T14:43:41.105398890Z [37m20251126-14:43:41.105 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL CUDA_VISIBLE_DEVICES=0 stdbuf -oL python3 -m areal.launcher.sglang_server examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_144334 sglang.random_seed=1 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144334/llm_server.log[0m
2025-11-26T14:43:42.107812163Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:43:42.107844893Z   import pynvml  # type: ignore[import]
2025-11-26T14:43:44.038288616Z [37m20251126-14:43:44.037 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:43:44.038327656Z [37m20251126-14:43:44.038 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:43:44.184794740Z [37m20251126-14:43:44.183 SGLangServer Wrapper INFO: Launch command: python3 -m sglang.launch_server --host 172.18.0.2 --port 17195 --tokenizer-path Qwen/Qwen2.5-0.5B-Instruct --tokenizer-mode auto --load-format auto --trust-remote-code --device cuda --tp-size 1 --base-gpu-id 0 --nnodes 1 --node-rank 0 --dist-init-addr localhost:38512 --model-path Qwen/Qwen2.5-0.5B-Instruct --random-seed 1 --skip-tokenizer-init --disable-radix-cache --torch-compile-max-bs 32 --triton-attention-num-kv-splits 8 --num-continuous-decode-steps 1 --attention-backend flashinfer --context-length 4096 --mem-fraction-static 0.8 --chunked-prefill-size -1 --max-prefill-tokens 32768 --schedule-policy lpm --schedule-conservativeness 1.0 --cpu-offload-gb 0 --dtype bfloat16 --kv-cache-dtype auto --dp-size 1 --ep-size 1 --max-loaded-loras 1 --max-loras-per-batch 1 --lora-backend triton --log-level warning --log-level-http warning --log-requests-level 0 --enable-metrics --decode-log-interval 1[0m
2025-11-26T14:43:45.777083320Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:43:45.777118890Z   import pynvml  # type: ignore[import]
2025-11-26T14:43:53.046287467Z INFO 11-26 14:43:53 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:43:53.892955826Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:43:55.332468825Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:43:55.332510545Z   import pynvml  # type: ignore[import]
2025-11-26T14:43:55.348571052Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:43:55.348599562Z   import pynvml  # type: ignore[import]
2025-11-26T14:44:02.135218328Z INFO 11-26 14:44:02 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:44:02.226703267Z INFO 11-26 14:44:02 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:44:02.918131621Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:44:03.186543009Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:44:03.191088290Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:44:03.191959898Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:44:03.192856176Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:44:03.239330129Z [2025-11-26 14:44:03] MOE_RUNNER_BACKEND is not initialized, using triton backend
2025-11-26T14:44:03.864990600Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:44:03.865034580Z   warnings.warn(
2025-11-26T14:44:03.865042290Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:44:03.865048610Z   warnings.warn(
2025-11-26T14:44:05.200063448Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:44:05.379707093Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.56it/s]
2025-11-26T14:44:05.379744833Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.55it/s]
2025-11-26T14:44:08.867071089Z   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   4%|â–         | 1/23 [00:00<00:15,  1.40it/s]Capturing batches (bs=152 avail_mem=14.84 GB):   4%|â–         | 1/23 [00:00<00:15,  1.40it/s]Capturing batches (bs=144 avail_mem=14.82 GB):   4%|â–         | 1/23 [00:00<00:15,  1.40it/s]Capturing batches (bs=144 avail_mem=14.82 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.33it/s]Capturing batches (bs=136 avail_mem=14.81 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.33it/s]Capturing batches (bs=128 avail_mem=14.79 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.33it/s]Capturing batches (bs=128 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:00<00:02,  7.00it/s]Capturing batches (bs=120 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:00<00:02,  7.00it/s]Capturing batches (bs=112 avail_mem=14.78 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:01<00:02,  7.00it/s]Capturing batches (bs=112 avail_mem=14.78 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.30it/s]Capturing batches (bs=104 avail_mem=14.76 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.30it/s]Capturing batches (bs=96 avail_mem=14.75 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.30it/s] Capturing batches (bs=96 avail_mem=14.75 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.18it/s]Capturing batches (bs=88 avail_mem=14.73 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.18it/s]Capturing batches (bs=80 avail_mem=14.72 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.18it/s]Capturing batches (bs=80 avail_mem=14.72 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.72it/s]Capturing batches (bs=72 avail_mem=14.71 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.72it/s]Capturing batches (bs=64 avail_mem=14.69 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.72it/s]Capturing batches (bs=64 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 13.81it/s]Capturing batches (bs=56 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 13.81it/s]Capturing batches (bs=48 avail_mem=14.66 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 13.81it/s]Capturing batches (bs=48 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.61it/s]Capturing batches (bs=40 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.61it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.61it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.54it/s]Capturing batches (bs=24 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.54it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.54it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.20it/s]Capturing batches (bs=8 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.20it/s] Capturing batches (bs=4 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.20it/s]Capturing batches (bs=4 avail_mem=14.60 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:01<00:00, 14.19it/s]Capturing batches (bs=2 avail_mem=14.59 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:01<00:00, 14.19it/s]Capturing batches (bs=1 avail_mem=14.57 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:02<00:00, 14.19it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 14.62it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 10.85it/s]
2025-11-26T14:44:14.230432082Z [37m20251126-14:44:14.229 SGLangServer Wrapper INFO: SGLang server launched at: http://172.18.0.2:17195[0m
2025-11-26T14:44:15.131861977Z [37m20251126-14:44:15.131 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:17195[0m
2025-11-26T14:44:15.131918677Z [37m20251126-14:44:15.131 Local Scheduler INFO: LLM inference server launched at: AREAL_LLM_SERVER_ADDRS=172.18.0.2:17195[0m
2025-11-26T14:44:15.132728535Z [37m20251126-14:44:15.132 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL AREAL_LLM_SERVER_ADDRS=172.18.0.2:17195 AREAL_RECOVER_RUN=0 NCCL_CUMEM_ENABLE=0 NCCL_NVLS_ENABLE=0 CUDA_VISIBLE_DEVICES=0 stdbuf -oL torchrun --nnodes 1 --nproc-per-node 1 --master-addr localhost --master-port 39140 examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_144334 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144334/trainer.log[0m
2025-11-26T14:44:15.133687153Z [37m20251126-14:44:15.133 Local Scheduler INFO: Waiting for 2 local running processes, pids: 387 1037[0m
2025-11-26T14:44:15.995334981Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:44:15.995378211Z   import pynvml  # type: ignore[import]
2025-11-26T14:44:17.696351004Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:44:17.696404854Z   import pynvml  # type: ignore[import]
2025-11-26T14:44:26.884788287Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:44:26.884831787Z   warnings.warn(
2025-11-26T14:44:26.884838897Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:44:26.884844797Z   warnings.warn(
2025-11-26T14:44:28.344859794Z [37m20251126-14:44:28.344 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:44:28.344965764Z [37m20251126-14:44:28.344 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:44:28.553713147Z [37m20251126-14:44:28.553 [FSDP Engine Rank 0] INFO: Initializing device mesh with parallel dims (dp=1, sp=1, tp=1, ep=1, etp=1, world_size=1).[0m
2025-11-26T14:44:28.556769961Z [37m20251126-14:44:28.556 [FSDP Engine Rank 0] INFO: Data parallel head 0 and rank 0[0m
2025-11-26T14:44:29.723526480Z [FAST] Limiting dataset from 7473 to 200 samples
2025-11-26T14:44:29.727123673Z [37m20251126-14:44:29.726 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:17195[0m
2025-11-26T14:44:29.727152583Z [37m20251126-14:44:29.727 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:44:29.727240493Z [37m20251126-14:44:29.727 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:44:30.733933167Z [37m20251126-14:44:30.733 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:44:30.737257430Z [37m20251126-14:44:30.737 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:17195[0m
2025-11-26T14:44:30.737280050Z [37m20251126-14:44:30.737 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:44:30.737344160Z [37m20251126-14:44:30.737 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:44:31.741256681Z [37m20251126-14:44:31.740 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:44:32.465652706Z [37m20251126-14:44:32.465 [FSDP Engine Rank 0] INFO: Model creation and loading time: 0.5593489811290056[0m
2025-11-26T14:44:33.000511277Z [37m20251126-14:44:33.000 [FSDP Engine Rank 0] INFO: Applying FSDP2 with N-D parallelism for 0.53 seconds[0m
2025-11-26T14:44:33.001405325Z [37m20251126-14:44:33.001 [FSDP Engine Rank 0] INFO: Create optimizer time: 0.0009203888475894928[0m
2025-11-26T14:44:33.551646274Z wandb: Currently logged in as: tong-zhao (tong-zhao-georgia-institute-of-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-11-26T14:44:33.554745098Z wandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.
2025-11-26T14:44:34.298001513Z wandb: setting up run gsm8k-grpo-cloud-fast_trial_20251126_144334_train
2025-11-26T14:44:34.700806811Z wandb: Tracking run with wandb version 0.22.2
2025-11-26T14:44:34.700983251Z wandb: Run data is saved locally in /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144334/wandb/run-20251126_144433-gsm8k-grpo-cloud-fast_trial_20251126_144334_train
2025-11-26T14:44:34.701002671Z wandb: Run `wandb offline` to turn off syncing.
2025-11-26T14:44:34.701010151Z wandb: Syncing run trial_20251126_144334
2025-11-26T14:44:34.701094180Z wandb: â­ï¸ View project at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local
2025-11-26T14:44:34.701115440Z wandb: ðŸš€ View run at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local/runs/gsm8k-grpo-cloud-fast_trial_20251126_144334_train
2025-11-26T14:44:35.047622806Z wandb: Detected [openai] in use.
2025-11-26T14:44:35.047735776Z wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-11-26T14:44:35.047915115Z wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-11-26T14:44:35.052969865Z swanlab: SwanLab run disabled, the data will not be saved or uploaded.
2025-11-26T14:44:35.662974019Z ================================================================================
2025-11-26T14:44:35.663613177Z [FAST MODE]
2025-11-26T14:44:35.664099516Z   Dataset size: 200 samples (limited from 7473)
2025-11-26T14:44:35.664468276Z   Batch size: 8
2025-11-26T14:44:35.664829285Z   Steps per epoch: 25
2025-11-26T14:44:35.665352634Z   Total epochs: 1
2025-11-26T14:44:35.665697963Z   Total steps: 25
2025-11-26T14:44:35.666098892Z   Estimated time: ~25 minutes (~0.4 hours) at ~1 step/min
2025-11-26T14:44:35.666497011Z   Circuit breaker: Enabled (threshold: 50 consecutive zero rewards)
2025-11-26T14:44:35.666689461Z ================================================================================
2025-11-26T14:44:43.378462353Z [37m20251126-14:44:43.377 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4998, 669], padded to: [5120, 768], padding lengths: [122, 99][0m
2025-11-26T14:44:44.877529578Z [37m20251126-14:44:44.876 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 0.93, memory reserved (GB): 3.22, device memory used/total (GB): 69.68/79.25[0m
2025-11-26T14:44:45.043216501Z [37m20251126-14:44:45.042 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 0.93, memory reserved (GB): 3.22, device memory used/total (GB): 69.68/79.25[0m
2025-11-26T14:44:45.077700069Z [37m20251126-14:44:45.077 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4998, 669], padded to: [5120, 768], padding lengths: [122, 99][0m
2025-11-26T14:44:46.867672716Z [37m20251126-14:44:46.867 /workspace/AReaL/areal/utils/device.py INFO: ppo update, memory allocated (GB): 3.70, memory reserved (GB): 10.17, device memory used/total (GB): 77.14/79.25[0m
2025-11-26T14:44:50.083775145Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:44:50.232093112Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.76it/s]
2025-11-26T14:44:50.232130252Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.75it/s]
2025-11-26T14:44:50.254389614Z [37m20251126-14:44:50.253 [Remote Inference Engine Rank 0] INFO: Loading weights from disk done in 3.39s. Respond time: 0.17s.[0m
2025-11-26T14:44:50.262609490Z [92m20251126-14:44:50.261 StatsLogger INFO: Epoch 1/1 Step 1/25 Train step 1/25 done.[0m
2025-11-26T14:44:50.263073079Z [92m20251126-14:44:50.262 StatsLogger INFO: Stats (1/1):[0m
2025-11-26T14:44:50.275443868Z [92m20251126-14:44:50.274 StatsLogger INFO:
2025-11-26T14:44:50.275477888Z â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â••
2025-11-26T14:44:50.275500728Z â”‚ grpo_actor/actor_loss/avg        â”‚ -1.7895e-04 â”‚ grpo_actor/entropy/min             â”‚  2.5842e-07 â”‚ grpo_actor/vocab_min_logits/min  â”‚ -2.8250e+01 â”‚ grpo_actor/no_eos_ratios/min    â”‚ 0.0000e+00 â”‚
2025-11-26T14:44:50.275509998Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275515418Z â”‚ grpo_actor/actor_loss/min        â”‚ -3.2800e+00 â”‚ grpo_actor/entropy/max             â”‚  6.0762e+00 â”‚ grpo_actor/vocab_min_logits/max  â”‚ -5.7500e+00 â”‚ grpo_actor/no_eos_ratios/max    â”‚ 1.0000e+00 â”‚
2025-11-26T14:44:50.275522058Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275527608Z â”‚ grpo_actor/actor_loss/max        â”‚  3.2408e+00 â”‚ grpo_actor/grad_norm               â”‚  3.9159e+00 â”‚ grpo_actor/advantages/avg        â”‚  1.1673e-08 â”‚ grpo_actor/prompt_len/avg       â”‚ 9.8875e+01 â”‚
2025-11-26T14:44:50.275533098Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275538788Z â”‚ grpo_actor/approx_kl/avg         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/avg   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/min        â”‚ -2.8474e+00 â”‚ grpo_actor/prompt_len/min       â”‚ 8.4000e+01 â”‚
2025-11-26T14:44:50.275544428Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275550028Z â”‚ grpo_actor/approx_kl/min         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/min   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/max        â”‚  2.8628e+00 â”‚ grpo_actor/prompt_len/max       â”‚ 1.1400e+02 â”‚
2025-11-26T14:44:50.275555828Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275568188Z â”‚ grpo_actor/approx_kl/max         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/max   â”‚  1.0000e+00 â”‚ grpo_actor/behav_imp_weight_cap  â”‚  5.0000e+00 â”‚ grpo_actor/seq_len/avg          â”‚ 3.5419e+02 â”‚
2025-11-26T14:44:50.275573708Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275579198Z â”‚ grpo_actor/behave_approx_kl/avg  â”‚ -3.4528e-04 â”‚ grpo_actor/lr                      â”‚  1.7000e-05 â”‚ grpo_actor/correct_seq_len/avg   â”‚  3.2900e+02 â”‚ grpo_actor/seq_len/min          â”‚ 3.2900e+02 â”‚
2025-11-26T14:44:50.275586838Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275593578Z â”‚ grpo_actor/behave_approx_kl/min  â”‚ -4.0136e-01 â”‚ grpo_actor/n_tokens                â”‚  5.6670e+03 â”‚ grpo_actor/correct_seq_len/min   â”‚  3.2900e+02 â”‚ grpo_actor/seq_len/max          â”‚ 3.7000e+02 â”‚
2025-11-26T14:44:50.275600238Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275606878Z â”‚ grpo_actor/behave_approx_kl/max  â”‚  2.4817e-01 â”‚ grpo_actor/n_valid_tokens          â”‚  4.0850e+03 â”‚ grpo_actor/correct_seq_len/max   â”‚  3.2900e+02 â”‚ grpo_actor/task_reward/avg      â”‚ 6.2500e-02 â”‚
2025-11-26T14:44:50.275613578Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275620938Z â”‚ grpo_actor/behave_imp_weight/avg â”‚  1.0003e+00 â”‚ grpo_actor/new_logp/avg            â”‚ -5.4119e-01 â”‚ grpo_actor/eps_clip              â”‚  4.0000e-01 â”‚ grpo_actor/task_reward/min      â”‚ 0.0000e+00 â”‚
2025-11-26T14:44:50.275636128Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275643598Z â”‚ grpo_actor/behave_imp_weight/min â”‚  6.6941e-01 â”‚ grpo_actor/new_logp/min            â”‚ -1.3626e+01 â”‚ grpo_actor/final_reward/avg      â”‚  0.0000e+00 â”‚ grpo_actor/task_reward/max      â”‚ 1.0000e+00 â”‚
2025-11-26T14:44:50.275650678Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275662398Z â”‚ grpo_actor/behave_imp_weight/max â”‚  1.2817e+00 â”‚ grpo_actor/new_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/final_reward/min      â”‚ -7.0711e-01 â”‚ grpo_actor/use_dual_clip        â”‚ 0.0000e+00 â”‚
2025-11-26T14:44:50.275670108Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275677178Z â”‚ grpo_actor/clip_ratio/avg        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/avg            â”‚ -5.4084e-01 â”‚ grpo_actor/final_reward/max      â”‚  7.0711e-01 â”‚ timeperf/compute_advantage      â”‚ 1.6558e-01 â”‚
2025-11-26T14:44:50.275683818Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275691508Z â”‚ grpo_actor/clip_ratio/min        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/min            â”‚ -1.3613e+01 â”‚ grpo_actor/incorrect_seq_len/avg â”‚  3.5587e+02 â”‚ timeperf/recompute_logp         â”‚ 1.6262e+00 â”‚
2025-11-26T14:44:50.275698168Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275712818Z â”‚ grpo_actor/clip_ratio/max        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/incorrect_seq_len/min â”‚  3.4000e+02 â”‚ timeperf/rollout                â”‚ 7.5845e+00 â”‚
2025-11-26T14:44:50.275720208Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275726758Z â”‚ grpo_actor/clipped_tokens        â”‚  0.0000e+00 â”‚ grpo_actor/unclipped_behave_tokens â”‚  4.0850e+03 â”‚ grpo_actor/incorrect_seq_len/max â”‚  3.7000e+02 â”‚ timeperf/checkpoint_for_recover â”‚ 5.7010e-05 â”‚
2025-11-26T14:44:50.275733268Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275739868Z â”‚ grpo_actor/dual_clip_ratio/avg   â”‚  0.0000e+00 â”‚ grpo_actor/update_successful       â”‚  1.0000e+00 â”‚ grpo_actor/kl_rewards/avg        â”‚  0.0000e+00 â”‚ timeperf/eval                   â”‚ 6.5669e-05 â”‚
2025-11-26T14:44:50.275747198Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275754408Z â”‚ grpo_actor/dual_clip_ratio/min   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/avg    â”‚  2.5269e+01 â”‚ grpo_actor/kl_rewards/min        â”‚  0.0000e+00 â”‚ timeperf/save                   â”‚ 1.8060e-04 â”‚
2025-11-26T14:44:50.275761228Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275767848Z â”‚ grpo_actor/dual_clip_ratio/max   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/min    â”‚  1.2938e+01 â”‚ grpo_actor/kl_rewards/max        â”‚  0.0000e+00 â”‚ timeperf/train_step             â”‚ 1.8245e+00 â”‚
2025-11-26T14:44:50.275774018Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275788278Z â”‚ grpo_actor/dual_clipped_tokens   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/max    â”‚  3.9750e+01 â”‚ grpo_actor/mask_no_eos_with_zero â”‚  0.0000e+00 â”‚ timeperf/update_weights         â”‚ 3.3882e+00 â”‚
2025-11-26T14:44:50.275795718Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:44:50.275802258Z â”‚ grpo_actor/entropy/avg           â”‚  5.0280e-01 â”‚ grpo_actor/vocab_min_logits/avg    â”‚ -1.5376e+01 â”‚ grpo_actor/no_eos_ratios/avg     â”‚  1.2500e-01 â”‚ rollout/reward                  â”‚ 1.4583e-01 â”‚
2025-11-26T14:44:50.275808588Z â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•›[0m
2025-11-26T14:44:50.327092481Z [37m20251126-14:44:50.326 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4995, 330], padded to: [5120, 512], padding lengths: [125, 182][0m
2025-11-26T14:44:51.133134039Z [37m20251126-14:44:51.132 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 3.70, memory reserved (GB): 10.18, device memory used/total (GB): 77.09/79.25[0m
2025-11-26T14:44:52.788809283Z [37m20251126-14:44:52.788 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 3.70, memory reserved (GB): 10.18, device memory used/total (GB): 77.13/79.25[0m
2025-11-26T14:44:52.950344178Z [37m20251126-14:44:52.949 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4995, 330], padded to: [5120, 512], padding lengths: [125, 182][0m
2025-11-26T14:44:53.634195024Z Traceback (most recent call last):
2025-11-26T14:44:53.635545062Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:44:53.635559252Z     main(sys.argv[1:])
2025-11-26T14:44:53.635994901Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:44:53.636004961Z     stats = actor.ppo_update(batch)
2025-11-26T14:44:53.636008401Z             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:53.636362171Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:44:53.636377161Z     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:44:53.636384091Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:53.636744350Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:44:53.636756990Z     train_stat = self.engine.train_batch(
2025-11-26T14:44:53.636762430Z                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:53.637091829Z   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:44:53.637111759Z     loss.backward()
2025-11-26T14:44:53.637456449Z   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:44:53.637463679Z     torch.autograd.backward(
2025-11-26T14:44:53.637915608Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:44:53.637947318Z     _engine_run_backward(
2025-11-26T14:44:53.638410357Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:44:53.638425977Z     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:44:53.638432327Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:53.638850666Z torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.41 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.12 GiB is free. Process 1229005 has 64.87 GiB memory in use. Process 1229603 has 13.25 GiB memory in use. Of the allocated memory 11.09 GiB is allocated by PyTorch, and 91.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:44:53.642828700Z [rank0]: Traceback (most recent call last):
2025-11-26T14:44:53.642843690Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:44:53.642849560Z [rank0]:     main(sys.argv[1:])
2025-11-26T14:44:53.642854590Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:44:53.642859750Z [rank0]:     stats = actor.ppo_update(batch)
2025-11-26T14:44:53.642864620Z [rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:53.642869520Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:44:53.642890060Z [rank0]:     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:44:53.642899150Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:53.642904210Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:44:53.642908910Z [rank0]:     train_stat = self.engine.train_batch(
2025-11-26T14:44:53.642913800Z [rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:53.642918620Z [rank0]:   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:44:53.642923660Z [rank0]:     loss.backward()
2025-11-26T14:44:53.642928330Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:44:53.642933270Z [rank0]:     torch.autograd.backward(
2025-11-26T14:44:53.642937939Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:44:53.642943079Z [rank0]:     _engine_run_backward(
2025-11-26T14:44:53.642947769Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:44:53.642953329Z [rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:44:53.642958129Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:53.642966669Z [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.41 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.12 GiB is free. Process 1229005 has 64.87 GiB memory in use. Process 1229603 has 13.25 GiB memory in use. Of the allocated memory 11.09 GiB is allocated by PyTorch, and 91.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:44:53.890101139Z [31m20251126-14:44:53.889 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:53.891122577Z [37m20251126-14:44:53.890 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:44:53.892169225Z [31m20251126-14:44:53.891 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:53.892579595Z [37m20251126-14:44:53.892 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:44:53.893029264Z [31m20251126-14:44:53.892 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:53.893411103Z [37m20251126-14:44:53.893 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:44:53.893698483Z [31m20251126-14:44:53.893 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:53.894010702Z [31m20251126-14:44:53.893 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:44:53.894929921Z Traceback (most recent call last):
2025-11-26T14:44:53.895536370Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:44:53.895546850Z     future = loop.run_in_executor(
2025-11-26T14:44:53.895554320Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:53.896103529Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:44:53.896517358Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:44:53.896527078Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:44:53.897009577Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:44:53.898474695Z [31m20251126-14:44:53.898 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:53.898839904Z [37m20251126-14:44:53.898 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:44:53.899292173Z [31m20251126-14:44:53.899 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:53.899720883Z [37m20251126-14:44:53.899 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:44:53.900189342Z [31m20251126-14:44:53.899 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:53.900655871Z [37m20251126-14:44:53.900 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:44:53.901157070Z [31m20251126-14:44:53.900 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:53.901593039Z [31m20251126-14:44:53.901 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:44:53.902098509Z Traceback (most recent call last):
2025-11-26T14:44:53.902594418Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:44:53.902608638Z     future = loop.run_in_executor(
2025-11-26T14:44:53.902616488Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:53.903000917Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:44:53.903355556Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:44:53.903363886Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:44:53.903694196Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:44:53.905805832Z [31m20251126-14:44:53.905 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:53.906160582Z [37m20251126-14:44:53.905 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:44:53.906472281Z [31m20251126-14:44:53.906 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:53.906777481Z [37m20251126-14:44:53.906 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:44:53.907220550Z [31m20251126-14:44:53.906 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:53.907463079Z [37m20251126-14:44:53.907 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:44:53.907793699Z [31m20251126-14:44:53.907 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:53.908100338Z [31m20251126-14:44:53.907 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:44:53.908475748Z Traceback (most recent call last):
2025-11-26T14:44:53.908929337Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:44:53.908953217Z     future = loop.run_in_executor(
2025-11-26T14:44:53.908961517Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:53.909332316Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:44:53.909764326Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:44:53.909770106Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:44:53.910201955Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:44:53.911710092Z [31m20251126-14:44:53.911 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:53.912295211Z [37m20251126-14:44:53.912 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:44:55.156763954Z [31m20251126-14:44:55.156 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:55.158836630Z [37m20251126-14:44:55.158 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:44:55.159685239Z [31m20251126-14:44:55.159 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:55.160352038Z [37m20251126-14:44:55.160 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:44:55.160963237Z [31m20251126-14:44:55.160 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:55.161521176Z [31m20251126-14:44:55.161 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:44:55.162677764Z Traceback (most recent call last):
2025-11-26T14:44:55.163373983Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:44:55.163390813Z     future = loop.run_in_executor(
2025-11-26T14:44:55.163396463Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.163994892Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:44:55.164466421Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:44:55.164474661Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:44:55.164952990Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:44:55.167897235Z [31m20251126-14:44:55.167 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:55.168539344Z [37m20251126-14:44:55.168 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:44:55.169004463Z [31m20251126-14:44:55.168 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:55.169341293Z [37m20251126-14:44:55.169 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:44:55.169658392Z [31m20251126-14:44:55.169 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:55.170007212Z [37m20251126-14:44:55.169 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:44:55.170332981Z [31m20251126-14:44:55.170 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:55.170638780Z [31m20251126-14:44:55.170 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:44:55.171136790Z Traceback (most recent call last):
2025-11-26T14:44:55.172661517Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:44:55.172682227Z     future = loop.run_in_executor(
2025-11-26T14:44:55.172687947Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.173114396Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:44:55.173499246Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:44:55.173547765Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:44:55.173992765Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:44:55.175872812Z [31m20251126-14:44:55.175 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:55.176366731Z [37m20251126-14:44:55.176 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:44:55.176797810Z [31m20251126-14:44:55.176 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:55.177375869Z [37m20251126-14:44:55.177 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:44:55.177909488Z [31m20251126-14:44:55.177 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:55.178436597Z [37m20251126-14:44:55.178 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:44:55.178947926Z [31m20251126-14:44:55.178 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:55.179486925Z [31m20251126-14:44:55.179 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:44:55.179997764Z Traceback (most recent call last):
2025-11-26T14:44:55.180506154Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:44:55.180514604Z     future = loop.run_in_executor(
2025-11-26T14:44:55.180520274Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.180927203Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:44:55.181358672Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:44:55.181372182Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:44:55.181895091Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:44:55.183951888Z [31m20251126-14:44:55.183 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:55.184392377Z [37m20251126-14:44:55.184 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:44:55.184948336Z [31m20251126-14:44:55.184 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:55.185421985Z [37m20251126-14:44:55.185 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:44:55.185866505Z [31m20251126-14:44:55.185 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:55.186612823Z [37m20251126-14:44:55.186 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:44:55.187080972Z [31m20251126-14:44:55.186 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:44:55.187708641Z [31m20251126-14:44:55.187 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:44:55.188291240Z Traceback (most recent call last):
2025-11-26T14:44:55.188928939Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:44:55.188943989Z     future = loop.run_in_executor(
2025-11-26T14:44:55.188949709Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.189381279Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:44:55.189802318Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:44:55.189811678Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:44:55.190291487Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:44:55.192921432Z [31m20251126-14:44:55.190 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 28 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:44:55.192942932Z Traceback (most recent call last):
2025-11-26T14:44:55.192948992Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:44:55.192954582Z     result = await async_task
2025-11-26T14:44:55.192974052Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.192979642Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:44:55.192985482Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:44:55.192991082Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.192996432Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:44:55.193002122Z     reward = await self.async_reward_fn(
2025-11-26T14:44:55.193007482Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.193012762Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:44:55.193018092Z     raise e
2025-11-26T14:44:55.193024152Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:44:55.193029602Z     future = loop.run_in_executor(
2025-11-26T14:44:55.193035452Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.193040822Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:44:55.193047652Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:44:55.193054052Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:44:55.193060702Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:44:55.194796139Z [31m20251126-14:44:55.193 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 26 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:44:55.194818659Z Traceback (most recent call last):
2025-11-26T14:44:55.194824899Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:44:55.194830939Z     result = await async_task
2025-11-26T14:44:55.194837179Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.194842569Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:44:55.194848039Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:44:55.194853819Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.194859199Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:44:55.194864709Z     reward = await self.async_reward_fn(
2025-11-26T14:44:55.194870089Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.194892249Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:44:55.194904169Z     raise e
2025-11-26T14:44:55.194911539Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:44:55.194918629Z     future = loop.run_in_executor(
2025-11-26T14:44:55.194925529Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.194932549Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:44:55.194939729Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:44:55.194946869Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:44:55.194953709Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:44:55.196164237Z [31m20251126-14:44:55.194 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 31 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:44:55.196185197Z Traceback (most recent call last):
2025-11-26T14:44:55.196190987Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:44:55.196196647Z     result = await async_task
2025-11-26T14:44:55.196201907Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.196207287Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:44:55.196212727Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:44:55.196218767Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.196224087Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:44:55.196229607Z     reward = await self.async_reward_fn(
2025-11-26T14:44:55.196234857Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.196250637Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:44:55.196257297Z     raise e
2025-11-26T14:44:55.196263937Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:44:55.196271267Z     future = loop.run_in_executor(
2025-11-26T14:44:55.196278877Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.196285887Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:44:55.196292897Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:44:55.196300337Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:44:55.196307707Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:44:55.197485245Z [31m20251126-14:44:55.196 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 29 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:44:55.197505965Z Traceback (most recent call last):
2025-11-26T14:44:55.197512905Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:44:55.197518225Z     result = await async_task
2025-11-26T14:44:55.197523615Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.197529035Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:44:55.197534525Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:44:55.197540105Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.197545335Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:44:55.197550965Z     reward = await self.async_reward_fn(
2025-11-26T14:44:55.197556415Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.197564035Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:44:55.197570475Z     raise e
2025-11-26T14:44:55.197577135Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:44:55.197584175Z     future = loop.run_in_executor(
2025-11-26T14:44:55.197591155Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.197598215Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:44:55.197605465Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:44:55.197612745Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:44:55.197619864Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:44:55.198752363Z [31m20251126-14:44:55.197 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 25 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:44:55.198769653Z Traceback (most recent call last):
2025-11-26T14:44:55.198777313Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:44:55.198783353Z     result = await async_task
2025-11-26T14:44:55.198790342Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.198797512Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:44:55.198804012Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:44:55.198810762Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.198817082Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:44:55.198822732Z     reward = await self.async_reward_fn(
2025-11-26T14:44:55.198828072Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.198833222Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:44:55.198838652Z     raise e
2025-11-26T14:44:55.198844042Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:44:55.198849502Z     future = loop.run_in_executor(
2025-11-26T14:44:55.198854852Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.198860272Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:44:55.198865732Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:44:55.198871312Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:44:55.198903522Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:44:55.201748767Z [31m20251126-14:44:55.200 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 27 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:44:55.201763627Z Traceback (most recent call last):
2025-11-26T14:44:55.201769437Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:44:55.201775277Z     result = await async_task
2025-11-26T14:44:55.201781877Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.201788147Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:44:55.201794497Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:44:55.201801217Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.201807557Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:44:55.201813377Z     reward = await self.async_reward_fn(
2025-11-26T14:44:55.201818607Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.201823837Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:44:55.201829197Z     raise e
2025-11-26T14:44:55.201834537Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:44:55.201839777Z     future = loop.run_in_executor(
2025-11-26T14:44:55.201845087Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.201850547Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:44:55.201855757Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:44:55.201861137Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:44:55.201866367Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:44:55.208305056Z [31m20251126-14:44:55.207 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 30 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:44:55.208329156Z Traceback (most recent call last):
2025-11-26T14:44:55.208336146Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:44:55.208342276Z     result = await async_task
2025-11-26T14:44:55.208347676Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.208352916Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:44:55.208358306Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:44:55.208363706Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.208368926Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:44:55.208374206Z     reward = await self.async_reward_fn(
2025-11-26T14:44:55.208379406Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.208384756Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:44:55.208389956Z     raise e
2025-11-26T14:44:55.208395276Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:44:55.208400526Z     future = loop.run_in_executor(
2025-11-26T14:44:55.208406016Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:44:55.208411246Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:44:55.208416586Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:44:55.208421816Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:44:55.208427046Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:44:55.924134998Z [1;34mwandb[0m:
2025-11-26T14:44:55.924179288Z [1;34mwandb[0m: ðŸš€ View run [33mtrial_20251126_144334[0m at: [34m[0m
2025-11-26T14:44:55.924187948Z [1;34mwandb[0m: Find logs at: [1;35m../outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144334/wandb/run-20251126_144433-gsm8k-grpo-cloud-fast_trial_20251126_144334_train/logs[0m
2025-11-26T14:44:57.931696173Z [rank0]:[W1126 14:44:57.057503252 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
2025-11-26T14:45:00.243469450Z E1126 14:45:00.241000 1038 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1104) of binary: /usr/bin/python3
2025-11-26T14:45:00.244389059Z Traceback (most recent call last):
2025-11-26T14:45:00.244415849Z   File "/usr/local/bin/torchrun", line 7, in <module>
2025-11-26T14:45:00.244474028Z     sys.exit(main())
2025-11-26T14:45:00.244513958Z              ^^^^^^
2025-11-26T14:45:00.244524208Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
2025-11-26T14:45:00.244813088Z     return f(*args, **kwargs)
2025-11-26T14:45:00.244821848Z            ^^^^^^^^^^^^^^^^^^
2025-11-26T14:45:00.244827328Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 901, in main
2025-11-26T14:45:00.245413847Z     run(args)
2025-11-26T14:45:00.245433127Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 892, in run
2025-11-26T14:45:00.245974946Z     elastic_launch(
2025-11-26T14:45:00.245997086Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 143, in __call__
2025-11-26T14:45:00.246132906Z     return launch_agent(self._config, self._entrypoint, list(args))
2025-11-26T14:45:00.246150956Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:45:00.246158166Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
2025-11-26T14:45:00.246409865Z     raise ChildFailedError(
2025-11-26T14:45:00.246422455Z torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
2025-11-26T14:45:00.246430065Z ============================================================
2025-11-26T14:45:00.246437395Z examples/cloud_gsm8k/gsm8k_grpo_train.py FAILED
2025-11-26T14:45:00.246444185Z ------------------------------------------------------------
2025-11-26T14:45:00.246450645Z Failures:
2025-11-26T14:45:00.246456805Z   <NO_OTHER_FAILURES>
2025-11-26T14:45:00.246462785Z ------------------------------------------------------------
2025-11-26T14:45:00.246469705Z Root Cause (first observed failure):
2025-11-26T14:45:00.246476465Z [0]:
2025-11-26T14:45:00.246483175Z   time      : 2025-11-26_14:45:00
2025-11-26T14:45:00.246498495Z   host      : 01af0063c46e
2025-11-26T14:45:00.246505515Z   rank      : 0 (local_rank: 0)
2025-11-26T14:45:00.246512265Z   exitcode  : 1 (pid: 1104)
2025-11-26T14:45:00.246519135Z   error_file: <N/A>
2025-11-26T14:45:00.246525675Z   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
2025-11-26T14:45:00.246532605Z ============================================================
2025-11-26T14:45:01.147933421Z [37m20251126-14:45:01.147 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [387][0m
2025-11-26T14:45:01.422767884Z Killed
2025-11-26T14:45:01.426349048Z [37m20251126-14:45:01.426 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [1037][0m
2025-11-26T14:45:01.427400736Z Traceback (most recent call last):
2025-11-26T14:45:01.427408936Z   File "<frozen runpy>", line 198, in _run_module_as_main
2025-11-26T14:45:01.427412746Z   File "<frozen runpy>", line 88, in _run_code
2025-11-26T14:45:01.427415516Z   File "/workspace/AReaL/areal/launcher/local.py", line 405, in <module>
2025-11-26T14:45:01.427580636Z     main()
2025-11-26T14:45:01.427605836Z   File "/workspace/AReaL/areal/launcher/local.py", line 260, in main
2025-11-26T14:45:01.427684976Z     local_main(config, run_id=0)
2025-11-26T14:45:01.427713316Z   File "/workspace/AReaL/areal/launcher/local.py", line 399, in local_main
2025-11-26T14:45:01.427807625Z     raise e
2025-11-26T14:45:01.427811685Z   File "/workspace/AReaL/areal/launcher/local.py", line 375, in local_main
2025-11-26T14:45:01.427906645Z     launcher.wait(
2025-11-26T14:45:01.427916535Z   File "/workspace/AReaL/areal/launcher/local.py", line 235, in wait
2025-11-26T14:45:01.427978275Z     raise JobException(
2025-11-26T14:45:01.427982135Z areal.utils.launcher.JobException: Job gsm8k-grpo-cloud-fast_trial_20251126_144334:trainer JobState.COMPLETED at node local
2025-11-26T14:45:01.666024340Z [37m20251126-14:45:01.665 Local Scheduler INFO: Waiting for 0 local running processes, pids: [0m
2025-11-26T14:45:17.335670964Z ==========
2025-11-26T14:45:17.335731284Z == CUDA ==
2025-11-26T14:45:17.335779854Z ==========
2025-11-26T14:45:17.344005410Z CUDA Version 12.9.1
2025-11-26T14:45:17.346404696Z Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2025-11-26T14:45:17.349142241Z This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2025-11-26T14:45:17.349153021Z By pulling and using the container, you accept the terms and conditions of this license:
2025-11-26T14:45:17.349161011Z https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2025-11-26T14:45:17.349175191Z A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
2025-11-26T14:45:17.573452690Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:45:17.773122880Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:45:17.998577796Z Branch 'feature/reasoning_model' set up to track remote branch 'feature/reasoning_model' from 'origin'.
2025-11-26T14:45:17.998609696Z Your branch is up to date with 'origin/feature/reasoning_model'.
2025-11-26T14:45:18.103154689Z Checking AReaL installation...
2025-11-26T14:45:18.192093577Z AReaL already installed. Skipping installation.
2025-11-26T14:45:18.192116707Z Cleaning up any leftover GPU processes...
2025-11-26T14:45:21.215241564Z Checking for processes holding GPU device files...
2025-11-26T14:45:21.908499565Z Found processes holding GPU devices: 1
2025-11-26T14:45:21.908545545Z 144
2025-11-26T14:45:21.908550925Z 145
2025-11-26T14:45:21.908555325Z 93
2025-11-26T14:45:21.908559985Z Killing process 1...
2025-11-26T14:45:21.908565635Z Killing process 144...
2025-11-26T14:45:21.908624934Z Killing process 145...
2025-11-26T14:45:23.912724305Z Using fuser to kill processes on GPU devices...
2025-11-26T14:45:25.938600808Z Checking GPU...
2025-11-26T14:45:25.980368887Z NVIDIA A100 80GB PCIe, 81920 MiB
2025-11-26T14:45:26.006739332Z Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True for better memory management
2025-11-26T14:45:26.006755612Z Detected 1 GPU(s)
2025-11-26T14:45:26.006760352Z Checking GPU status...
2025-11-26T14:45:26.036610562Z index, name, utilization.gpu [%], memory.used [MiB], compute_mode
2025-11-26T14:45:26.043587850Z 0, NVIDIA A100 80GB PCIe, 0, 0, Default
2025-11-26T14:45:26.098095777Z Verifying GPU accessibility...
2025-11-26T14:45:26.930148082Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:45:26.930210691Z   import pynvml  # type: ignore[import]
2025-11-26T14:45:28.228225013Z GPU accessibility verified on attempt 1
2025-11-26T14:45:28.960328418Z Starting training...
2025-11-26T14:45:31.964771837Z Using FAST training configuration (20-30 minutes)
2025-11-26T14:45:31.969519189Z ==========================================
2025-11-26T14:45:31.969534369Z Starting GRPO Training (Cloud)
2025-11-26T14:45:31.969547039Z ==========================================
2025-11-26T14:45:31.969556369Z Config: fast
2025-11-26T14:45:31.969570679Z Config file: examples/cloud_gsm8k/gsm8k_grpo_fast.yaml
2025-11-26T14:45:31.969582129Z Training script: examples/cloud_gsm8k/gsm8k_grpo_train.py
2025-11-26T14:45:31.969590849Z Experiment: gsm8k-grpo-cloud-fast
2025-11-26T14:45:31.969599629Z Trial: trial_20251126_144531
2025-11-26T14:45:31.969608389Z GPU: NVIDIA A100 80GB PCIe (81920 MB)
2025-11-26T14:45:31.969618829Z WandB API key: e1adc5be02...
2025-11-26T14:45:31.969627589Z ==========================================
2025-11-26T14:45:33.043564862Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:45:33.043639892Z   import pynvml  # type: ignore[import]
2025-11-26T14:45:38.344361184Z [37m20251126-14:45:38.343 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:45:38.344419234Z [37m20251126-14:45:38.344 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:45:38.345065313Z [37m20251126-14:45:38.344 name-resolve INFO: No such name resolve path: ./tmp/areal/name_resolve/root/gsm8k-grpo-cloud-fast/trial_20251126_144531[0m
2025-11-26T14:45:38.510473011Z [37m20251126-14:45:38.510 Local Scheduler INFO: LocalLauncher: experiment_name=gsm8k-grpo-cloud-fast, trial_name=trial_20251126_144531, fileroot=/workspace/outputs/grpo, run_id=0, is_recover_run=False[0m
2025-11-26T14:45:38.518557528Z [37m20251126-14:45:38.518 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL CUDA_VISIBLE_DEVICES=0 stdbuf -oL python3 -m areal.launcher.sglang_server examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_144531 sglang.random_seed=1 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144531/llm_server.log[0m
2025-11-26T14:45:39.583726415Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:45:39.583764005Z   import pynvml  # type: ignore[import]
2025-11-26T14:45:41.596399561Z [37m20251126-14:45:41.595 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:45:41.596440801Z [37m20251126-14:45:41.596 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:45:41.762383379Z [37m20251126-14:45:41.761 SGLangServer Wrapper INFO: Launch command: python3 -m sglang.launch_server --host 172.18.0.2 --port 16227 --tokenizer-path Qwen/Qwen2.5-0.5B-Instruct --tokenizer-mode auto --load-format auto --trust-remote-code --device cuda --tp-size 1 --base-gpu-id 0 --nnodes 1 --node-rank 0 --dist-init-addr localhost:26876 --model-path Qwen/Qwen2.5-0.5B-Instruct --random-seed 1 --skip-tokenizer-init --disable-radix-cache --torch-compile-max-bs 32 --triton-attention-num-kv-splits 8 --num-continuous-decode-steps 1 --attention-backend flashinfer --context-length 4096 --mem-fraction-static 0.8 --chunked-prefill-size -1 --max-prefill-tokens 32768 --schedule-policy lpm --schedule-conservativeness 1.0 --cpu-offload-gb 0 --dtype bfloat16 --kv-cache-dtype auto --dp-size 1 --ep-size 1 --max-loaded-loras 1 --max-loras-per-batch 1 --lora-backend triton --log-level warning --log-level-http warning --log-requests-level 0 --enable-metrics --decode-log-interval 1[0m
2025-11-26T14:45:43.188549303Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:45:43.188587283Z   import pynvml  # type: ignore[import]
2025-11-26T14:45:51.681107445Z INFO 11-26 14:45:51 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:45:52.521789434Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:45:53.953224349Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:45:53.953283579Z   import pynvml  # type: ignore[import]
2025-11-26T14:45:53.973061596Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:45:53.973091356Z   import pynvml  # type: ignore[import]
2025-11-26T14:46:00.999784671Z INFO 11-26 14:46:00 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:46:01.038198596Z INFO 11-26 14:46:01 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:46:01.803936003Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:46:02.713043746Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:46:02.717266419Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:46:02.718208887Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:46:02.719086346Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:46:02.765620517Z [2025-11-26 14:46:02] MOE_RUNNER_BACKEND is not initialized, using triton backend
2025-11-26T14:46:03.420798512Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:46:03.420833182Z   warnings.warn(
2025-11-26T14:46:03.420839952Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:46:03.420846082Z   warnings.warn(
2025-11-26T14:46:04.820094842Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:46:04.998681238Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.61it/s]
2025-11-26T14:46:04.998717638Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.60it/s]
2025-11-26T14:46:09.240119942Z   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   4%|â–         | 1/23 [00:00<00:16,  1.35it/s]Capturing batches (bs=152 avail_mem=14.84 GB):   4%|â–         | 1/23 [00:00<00:16,  1.35it/s]Capturing batches (bs=144 avail_mem=14.82 GB):   4%|â–         | 1/23 [00:00<00:16,  1.35it/s]Capturing batches (bs=144 avail_mem=14.82 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.17it/s]Capturing batches (bs=136 avail_mem=14.81 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.17it/s]Capturing batches (bs=128 avail_mem=14.79 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.17it/s]Capturing batches (bs=128 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:00<00:02,  6.81it/s]Capturing batches (bs=120 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:00<00:02,  6.81it/s]Capturing batches (bs=112 avail_mem=14.78 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:01<00:02,  6.81it/s]Capturing batches (bs=112 avail_mem=14.78 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.13it/s]Capturing batches (bs=104 avail_mem=14.76 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.13it/s]Capturing batches (bs=96 avail_mem=14.75 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.13it/s] Capturing batches (bs=96 avail_mem=14.75 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.07it/s]Capturing batches (bs=88 avail_mem=14.73 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.07it/s]Capturing batches (bs=80 avail_mem=14.72 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.07it/s]Capturing batches (bs=80 avail_mem=14.72 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.65it/s]Capturing batches (bs=72 avail_mem=14.71 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.65it/s]Capturing batches (bs=64 avail_mem=14.69 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.65it/s]Capturing batches (bs=64 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 13.84it/s]Capturing batches (bs=56 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 13.84it/s]Capturing batches (bs=48 avail_mem=14.66 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 13.84it/s]Capturing batches (bs=48 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.85it/s]Capturing batches (bs=40 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.85it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.85it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.91it/s]Capturing batches (bs=24 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.91it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.91it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.60it/s]Capturing batches (bs=8 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.60it/s] Capturing batches (bs=4 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.60it/s]Capturing batches (bs=4 avail_mem=14.60 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:01<00:00, 14.65it/s]Capturing batches (bs=2 avail_mem=14.59 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:01<00:00, 14.65it/s]Capturing batches (bs=1 avail_mem=14.57 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:02<00:00, 14.65it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 15.15it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 10.88it/s]
2025-11-26T14:46:14.821692357Z [37m20251126-14:46:14.821 SGLangServer Wrapper INFO: SGLang server launched at: http://172.18.0.2:16227[0m
2025-11-26T14:46:15.527686686Z [37m20251126-14:46:15.527 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:16227[0m
2025-11-26T14:46:15.527727126Z [37m20251126-14:46:15.527 Local Scheduler INFO: LLM inference server launched at: AREAL_LLM_SERVER_ADDRS=172.18.0.2:16227[0m
2025-11-26T14:46:15.528185365Z [37m20251126-14:46:15.528 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL AREAL_LLM_SERVER_ADDRS=172.18.0.2:16227 AREAL_RECOVER_RUN=0 NCCL_CUMEM_ENABLE=0 NCCL_NVLS_ENABLE=0 CUDA_VISIBLE_DEVICES=0 stdbuf -oL torchrun --nnodes 1 --nproc-per-node 1 --master-addr localhost --master-port 37156 examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_144531 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144531/trainer.log[0m
2025-11-26T14:46:15.529286623Z [37m20251126-14:46:15.528 Local Scheduler INFO: Waiting for 2 local running processes, pids: 387 1037[0m
2025-11-26T14:46:16.325189119Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:46:16.325225219Z   import pynvml  # type: ignore[import]
2025-11-26T14:46:18.022958731Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:46:18.022994860Z   import pynvml  # type: ignore[import]
2025-11-26T14:46:27.250990561Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:46:27.251031311Z   warnings.warn(
2025-11-26T14:46:27.251039171Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:46:27.251045761Z   warnings.warn(
2025-11-26T14:46:28.833702868Z [37m20251126-14:46:28.833 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:46:28.833734248Z [37m20251126-14:46:28.833 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:46:29.039399038Z [37m20251126-14:46:29.039 [FSDP Engine Rank 0] INFO: Initializing device mesh with parallel dims (dp=1, sp=1, tp=1, ep=1, etp=1, world_size=1).[0m
2025-11-26T14:46:29.042160744Z [37m20251126-14:46:29.041 [FSDP Engine Rank 0] INFO: Data parallel head 0 and rank 0[0m
2025-11-26T14:46:30.188987103Z [FAST] Limiting dataset from 7473 to 200 samples
2025-11-26T14:46:30.192822326Z [37m20251126-14:46:30.192 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:16227[0m
2025-11-26T14:46:30.192838816Z [37m20251126-14:46:30.192 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:46:30.192949706Z [37m20251126-14:46:30.192 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:46:31.196288109Z [37m20251126-14:46:31.195 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:46:31.199550033Z [37m20251126-14:46:31.199 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:16227[0m
2025-11-26T14:46:31.199578263Z [37m20251126-14:46:31.199 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:46:31.199598363Z [37m20251126-14:46:31.199 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:46:32.203486105Z [37m20251126-14:46:32.203 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:46:32.958722401Z [37m20251126-14:46:32.958 [FSDP Engine Rank 0] INFO: Model creation and loading time: 0.5935251601040363[0m
2025-11-26T14:46:33.484374726Z [37m20251126-14:46:33.483 [FSDP Engine Rank 0] INFO: Applying FSDP2 with N-D parallelism for 0.53 seconds[0m
2025-11-26T14:46:33.485213475Z [37m20251126-14:46:33.484 [FSDP Engine Rank 0] INFO: Create optimizer time: 0.000863947905600071[0m
2025-11-26T14:46:34.010462361Z wandb: Currently logged in as: tong-zhao (tong-zhao-georgia-institute-of-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-11-26T14:46:34.013498856Z wandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.
2025-11-26T14:46:34.854350476Z wandb: setting up run gsm8k-grpo-cloud-fast_trial_20251126_144531_train
2025-11-26T14:46:34.902380244Z wandb: Tracking run with wandb version 0.22.2
2025-11-26T14:46:34.902459424Z wandb: Run data is saved locally in /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144531/wandb/run-20251126_144634-gsm8k-grpo-cloud-fast_trial_20251126_144531_train
2025-11-26T14:46:34.902466614Z wandb: Run `wandb offline` to turn off syncing.
2025-11-26T14:46:34.902591303Z wandb: Syncing run trial_20251126_144531
2025-11-26T14:46:34.902626453Z wandb: â­ï¸ View project at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local
2025-11-26T14:46:34.902642043Z wandb: ðŸš€ View run at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local/runs/gsm8k-grpo-cloud-fast_trial_20251126_144531_train
2025-11-26T14:46:35.248736515Z wandb: Detected [openai] in use.
2025-11-26T14:46:35.248983524Z wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-11-26T14:46:35.249271074Z wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-11-26T14:46:35.254239395Z swanlab: SwanLab run disabled, the data will not be saved or uploaded.
2025-11-26T14:46:35.845707649Z ================================================================================
2025-11-26T14:46:35.846137248Z [FAST MODE]
2025-11-26T14:46:35.846537358Z   Dataset size: 200 samples (limited from 7473)
2025-11-26T14:46:35.846957757Z   Batch size: 8
2025-11-26T14:46:35.847287766Z   Steps per epoch: 25
2025-11-26T14:46:35.847601926Z   Total epochs: 1
2025-11-26T14:46:35.847963685Z   Total steps: 25
2025-11-26T14:46:35.848399274Z   Estimated time: ~25 minutes (~0.4 hours) at ~1 step/min
2025-11-26T14:46:35.848769874Z   Circuit breaker: Enabled (threshold: 50 consecutive zero rewards)
2025-11-26T14:46:35.849042373Z ================================================================================
2025-11-26T14:46:42.925398165Z [37m20251126-14:46:42.924 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4959, 577], padded to: [5120, 768], padding lengths: [161, 191][0m
2025-11-26T14:46:44.323647337Z [37m20251126-14:46:44.323 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 0.93, memory reserved (GB): 3.22, device memory used/total (GB): 69.68/79.25[0m
2025-11-26T14:46:44.490519003Z [37m20251126-14:46:44.490 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 0.93, memory reserved (GB): 3.22, device memory used/total (GB): 69.68/79.25[0m
2025-11-26T14:46:44.525363323Z [37m20251126-14:46:44.524 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4959, 577], padded to: [5120, 768], padding lengths: [161, 191][0m
2025-11-26T14:46:46.196691850Z [37m20251126-14:46:46.196 /workspace/AReaL/areal/utils/device.py INFO: ppo update, memory allocated (GB): 3.70, memory reserved (GB): 9.64, device memory used/total (GB): 76.61/79.25[0m
2025-11-26T14:46:49.567254146Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:46:49.713434477Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.85it/s]
2025-11-26T14:46:49.713474847Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.84it/s]
2025-11-26T14:46:49.743831846Z [37m20251126-14:46:49.742 [Remote Inference Engine Rank 0] INFO: Loading weights from disk done in 3.55s. Respond time: 0.19s.[0m
2025-11-26T14:46:49.755273626Z [92m20251126-14:46:49.754 StatsLogger INFO: Epoch 1/1 Step 1/25 Train step 1/25 done.[0m
2025-11-26T14:46:49.755932485Z [92m20251126-14:46:49.755 StatsLogger INFO: Stats (1/1):[0m
2025-11-26T14:46:49.768489254Z [92m20251126-14:46:49.767 StatsLogger INFO:
2025-11-26T14:46:49.768517154Z â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â••
2025-11-26T14:46:49.768524664Z â”‚ grpo_actor/actor_loss/avg        â”‚  0.0000e+00 â”‚ grpo_actor/entropy/min             â”‚  3.0188e-07 â”‚ grpo_actor/vocab_min_logits/min  â”‚ -3.0375e+01 â”‚ grpo_actor/no_eos_ratios/min    â”‚ 0.0000e+00 â”‚
2025-11-26T14:46:49.768532834Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768538834Z â”‚ grpo_actor/actor_loss/min        â”‚ -0.0000e+00 â”‚ grpo_actor/entropy/max             â”‚  3.5966e+00 â”‚ grpo_actor/vocab_min_logits/max  â”‚ -7.5625e+00 â”‚ grpo_actor/no_eos_ratios/max    â”‚ 1.0000e+00 â”‚
2025-11-26T14:46:49.768545514Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768551564Z â”‚ grpo_actor/actor_loss/max        â”‚ -0.0000e+00 â”‚ grpo_actor/grad_norm               â”‚  0.0000e+00 â”‚ grpo_actor/advantages/avg        â”‚  0.0000e+00 â”‚ grpo_actor/prompt_len/avg       â”‚ 9.8875e+01 â”‚
2025-11-26T14:46:49.768557094Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768562374Z â”‚ grpo_actor/approx_kl/avg         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/avg   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/min        â”‚  0.0000e+00 â”‚ grpo_actor/prompt_len/min       â”‚ 8.4000e+01 â”‚
2025-11-26T14:46:49.768567854Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768584604Z â”‚ grpo_actor/approx_kl/min         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/min   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/max        â”‚  0.0000e+00 â”‚ grpo_actor/prompt_len/max       â”‚ 1.1400e+02 â”‚
2025-11-26T14:46:49.768590414Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768599224Z â”‚ grpo_actor/approx_kl/max         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/max   â”‚  1.0000e+00 â”‚ grpo_actor/behav_imp_weight_cap  â”‚  5.0000e+00 â”‚ grpo_actor/seq_len/avg          â”‚ 3.4600e+02 â”‚
2025-11-26T14:46:49.768604814Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768610744Z â”‚ grpo_actor/behave_approx_kl/avg  â”‚ -2.7122e-04 â”‚ grpo_actor/lr                      â”‚  1.7000e-05 â”‚ grpo_actor/correct_seq_len/avg   â”‚  2.9950e+02 â”‚ grpo_actor/seq_len/min          â”‚ 2.5900e+02 â”‚
2025-11-26T14:46:49.768616604Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768622164Z â”‚ grpo_actor/behave_approx_kl/min  â”‚ -2.5052e-01 â”‚ grpo_actor/n_tokens                â”‚  5.5360e+03 â”‚ grpo_actor/correct_seq_len/min   â”‚  2.5900e+02 â”‚ grpo_actor/seq_len/max          â”‚ 3.7000e+02 â”‚
2025-11-26T14:46:49.768627484Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768632904Z â”‚ grpo_actor/behave_approx_kl/max  â”‚  2.4828e-01 â”‚ grpo_actor/n_valid_tokens          â”‚  3.9540e+03 â”‚ grpo_actor/correct_seq_len/max   â”‚  3.4000e+02 â”‚ grpo_actor/task_reward/avg      â”‚ 1.2500e-01 â”‚
2025-11-26T14:46:49.768638104Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768649874Z â”‚ grpo_actor/behave_imp_weight/avg â”‚  1.0002e+00 â”‚ grpo_actor/new_logp/avg            â”‚ -3.1423e-01 â”‚ grpo_actor/eps_clip              â”‚  4.0000e-01 â”‚ grpo_actor/task_reward/min      â”‚ 0.0000e+00 â”‚
2025-11-26T14:46:49.768655604Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768661213Z â”‚ grpo_actor/behave_imp_weight/min â”‚  7.7840e-01 â”‚ grpo_actor/new_logp/min            â”‚ -1.5318e+01 â”‚ grpo_actor/final_reward/avg      â”‚  0.0000e+00 â”‚ grpo_actor/task_reward/max      â”‚ 1.0000e+00 â”‚
2025-11-26T14:46:49.768666873Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768672163Z â”‚ grpo_actor/behave_imp_weight/max â”‚  1.2818e+00 â”‚ grpo_actor/new_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/final_reward/min      â”‚  0.0000e+00 â”‚ grpo_actor/use_dual_clip        â”‚ 0.0000e+00 â”‚
2025-11-26T14:46:49.768677893Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768683413Z â”‚ grpo_actor/clip_ratio/avg        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/avg            â”‚ -3.1396e-01 â”‚ grpo_actor/final_reward/max      â”‚  0.0000e+00 â”‚ timeperf/compute_advantage      â”‚ 1.6672e-01 â”‚
2025-11-26T14:46:49.768689083Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768694523Z â”‚ grpo_actor/clip_ratio/min        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/min            â”‚ -1.5193e+01 â”‚ grpo_actor/incorrect_seq_len/avg â”‚  3.5264e+02 â”‚ timeperf/recompute_logp         â”‚ 1.5498e+00 â”‚
2025-11-26T14:46:49.768699983Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768711493Z â”‚ grpo_actor/clip_ratio/max        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/incorrect_seq_len/min â”‚  3.1800e+02 â”‚ timeperf/rollout                â”‚ 6.9249e+00 â”‚
2025-11-26T14:46:49.768717203Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768722593Z â”‚ grpo_actor/clipped_tokens        â”‚  0.0000e+00 â”‚ grpo_actor/unclipped_behave_tokens â”‚  3.9540e+03 â”‚ grpo_actor/incorrect_seq_len/max â”‚  3.7000e+02 â”‚ timeperf/checkpoint_for_recover â”‚ 5.6950e-05 â”‚
2025-11-26T14:46:49.768728363Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768733683Z â”‚ grpo_actor/dual_clip_ratio/avg   â”‚  0.0000e+00 â”‚ grpo_actor/update_successful       â”‚  1.0000e+00 â”‚ grpo_actor/kl_rewards/avg        â”‚  0.0000e+00 â”‚ timeperf/eval                   â”‚ 1.0341e-04 â”‚
2025-11-26T14:46:49.768739383Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768747223Z â”‚ grpo_actor/dual_clip_ratio/min   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/avg    â”‚  2.6129e+01 â”‚ grpo_actor/kl_rewards/min        â”‚  0.0000e+00 â”‚ timeperf/save                   â”‚ 1.9324e-04 â”‚
2025-11-26T14:46:49.768752743Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768758313Z â”‚ grpo_actor/dual_clip_ratio/max   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/min    â”‚  1.2938e+01 â”‚ grpo_actor/kl_rewards/max        â”‚  0.0000e+00 â”‚ timeperf/train_step             â”‚ 1.7061e+00 â”‚
2025-11-26T14:46:49.768769723Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768775873Z â”‚ grpo_actor/dual_clipped_tokens   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/max    â”‚  4.1750e+01 â”‚ grpo_actor/mask_no_eos_with_zero â”‚  0.0000e+00 â”‚ timeperf/update_weights         â”‚ 3.5498e+00 â”‚
2025-11-26T14:46:49.768782863Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:46:49.768789353Z â”‚ grpo_actor/entropy/avg           â”‚  3.0695e-01 â”‚ grpo_actor/vocab_min_logits/avg    â”‚ -1.6064e+01 â”‚ grpo_actor/no_eos_ratios/avg     â”‚  1.2500e-01 â”‚ rollout/reward                  â”‚ 1.4583e-01 â”‚
2025-11-26T14:46:49.768795953Z â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•›[0m
2025-11-26T14:46:49.820442325Z [37m20251126-14:46:49.819 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4950], padded to: [5120], padding lengths: [170][0m
2025-11-26T14:46:49.933619193Z [37m20251126-14:46:49.933 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 3.70, memory reserved (GB): 9.65, device memory used/total (GB): 76.50/79.25[0m
2025-11-26T14:46:50.041605239Z [37m20251126-14:46:50.041 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 3.70, memory reserved (GB): 9.65, device memory used/total (GB): 76.50/79.25[0m
2025-11-26T14:46:50.064947739Z [37m20251126-14:46:50.064 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4950], padded to: [5120], padding lengths: [170][0m
2025-11-26T14:46:50.371119499Z Traceback (most recent call last):
2025-11-26T14:46:50.372029617Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:46:50.372060777Z     main(sys.argv[1:])
2025-11-26T14:46:50.372583416Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:46:50.372595376Z     stats = actor.ppo_update(batch)
2025-11-26T14:46:50.372602756Z             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:50.373016515Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:46:50.373034335Z     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:46:50.373039325Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:50.373454625Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:46:50.373489454Z     train_stat = self.engine.train_batch(
2025-11-26T14:46:50.373494004Z                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:50.374027464Z   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:46:50.374051004Z     loss.backward()
2025-11-26T14:46:50.374490983Z   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:46:50.374499553Z     torch.autograd.backward(
2025-11-26T14:46:50.374980992Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:46:50.374999312Z     _engine_run_backward(
2025-11-26T14:46:50.375512661Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:46:50.375524651Z     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:46:50.375530381Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:50.375931800Z torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.40 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.22 GiB is free. Process 1232106 has 64.77 GiB memory in use. Process 1233339 has 13.25 GiB memory in use. Of the allocated memory 11.07 GiB is allocated by PyTorch, and 104.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:46:50.378735245Z [rank0]: Traceback (most recent call last):
2025-11-26T14:46:50.378749065Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:46:50.378755525Z [rank0]:     main(sys.argv[1:])
2025-11-26T14:46:50.378761175Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:46:50.378766695Z [rank0]:     stats = actor.ppo_update(batch)
2025-11-26T14:46:50.378772095Z [rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:50.378777845Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:46:50.378783355Z [rank0]:     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:46:50.378788555Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:50.378794015Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:46:50.378799545Z [rank0]:     train_stat = self.engine.train_batch(
2025-11-26T14:46:50.378804665Z [rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:50.378810095Z [rank0]:   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:46:50.378815495Z [rank0]:     loss.backward()
2025-11-26T14:46:50.378820875Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:46:50.378826445Z [rank0]:     torch.autograd.backward(
2025-11-26T14:46:50.378832105Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:46:50.378837805Z [rank0]:     _engine_run_backward(
2025-11-26T14:46:50.378843425Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:46:50.378848845Z [rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:46:50.378859175Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:50.378866655Z [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.40 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.22 GiB is free. Process 1232106 has 64.77 GiB memory in use. Process 1233339 has 13.25 GiB memory in use. Of the allocated memory 11.07 GiB is allocated by PyTorch, and 104.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:46:52.261569403Z [1;34mwandb[0m:
2025-11-26T14:46:52.261613443Z [1;34mwandb[0m: ðŸš€ View run [33mtrial_20251126_144531[0m at: [34m[0m
2025-11-26T14:46:52.261622653Z [1;34mwandb[0m: Find logs at: [1;35m../outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144531/wandb/run-20251126_144634-gsm8k-grpo-cloud-fast_trial_20251126_144531_train/logs[0m
2025-11-26T14:46:52.469618929Z [31m20251126-14:46:52.469 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.475393439Z [37m20251126-14:46:52.475 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:46:52.477377746Z [31m20251126-14:46:52.477 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.479226263Z [37m20251126-14:46:52.478 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:46:52.481096040Z [31m20251126-14:46:52.480 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.482911606Z [37m20251126-14:46:52.482 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:46:52.484704413Z [31m20251126-14:46:52.484 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.486574950Z [31m20251126-14:46:52.486 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:46:52.488476827Z Traceback (most recent call last):
2025-11-26T14:46:52.492490150Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:46:52.492509860Z     future = loop.run_in_executor(
2025-11-26T14:46:52.492516650Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:52.495712335Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:46:52.499020089Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:46:52.499037709Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:46:52.502454043Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:46:52.602084454Z [31m20251126-14:46:52.600 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 24 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:46:52.602113464Z Traceback (most recent call last):
2025-11-26T14:46:52.602120164Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:46:52.602126574Z     result = await async_task
2025-11-26T14:46:52.602133404Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:46:52.602138654Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:46:52.602144254Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:46:52.602149984Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:52.602155404Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:46:52.602161724Z     reward = await self.async_reward_fn(
2025-11-26T14:46:52.602166944Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:52.602172144Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:46:52.602177404Z     raise e
2025-11-26T14:46:52.602183194Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:46:52.602188404Z     future = loop.run_in_executor(
2025-11-26T14:46:52.602193644Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:52.602198934Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:46:52.602204174Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:46:52.602209384Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:46:52.602214594Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:46:52.659231297Z [31m20251126-14:46:52.658 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.661262213Z [37m20251126-14:46:52.661 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:46:52.663107460Z [31m20251126-14:46:52.662 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.664910197Z [37m20251126-14:46:52.664 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:46:52.666680894Z [31m20251126-14:46:52.666 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.668463921Z [37m20251126-14:46:52.668 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:46:52.670300658Z [31m20251126-14:46:52.670 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.672088595Z [31m20251126-14:46:52.671 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:46:52.673860482Z Traceback (most recent call last):
2025-11-26T14:46:52.677481145Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:46:52.677496785Z     future = loop.run_in_executor(
2025-11-26T14:46:52.677502885Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:52.680693370Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:46:52.683895205Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:46:52.683911155Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:46:52.687166629Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:46:52.690965073Z [31m20251126-14:46:52.690 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.692822009Z [37m20251126-14:46:52.692 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:46:52.695498535Z [31m20251126-14:46:52.694 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.697641191Z [37m20251126-14:46:52.697 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:46:52.700243447Z [31m20251126-14:46:52.699 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.702410133Z [37m20251126-14:46:52.702 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:46:52.704211300Z [31m20251126-14:46:52.703 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.705981857Z [31m20251126-14:46:52.705 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:46:52.707715214Z Traceback (most recent call last):
2025-11-26T14:46:52.710975568Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:46:52.710990398Z     future = loop.run_in_executor(
2025-11-26T14:46:52.710997198Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:52.714433813Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:46:52.717598127Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:46:52.717608577Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:46:52.720787042Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:46:52.724496446Z [31m20251126-14:46:52.724 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.726259352Z [37m20251126-14:46:52.726 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:46:52.728031059Z [31m20251126-14:46:52.727 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.730098856Z [37m20251126-14:46:52.729 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:46:52.731861173Z [31m20251126-14:46:52.731 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.733620330Z [37m20251126-14:46:52.733 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:46:52.735393827Z [31m20251126-14:46:52.735 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.737216554Z [31m20251126-14:46:52.737 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:46:52.738633241Z Traceback (most recent call last):
2025-11-26T14:46:52.740998137Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:46:52.741008667Z     future = loop.run_in_executor(
2025-11-26T14:46:52.741014507Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:52.743344383Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:46:52.746382338Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:46:52.746397908Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:46:52.748684914Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:46:52.751460580Z [31m20251126-14:46:52.751 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.752726907Z [37m20251126-14:46:52.752 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:46:52.754005765Z [31m20251126-14:46:52.753 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.755262803Z [37m20251126-14:46:52.755 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:46:52.756536871Z [31m20251126-14:46:52.756 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.758031188Z [37m20251126-14:46:52.757 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:46:52.759314636Z [31m20251126-14:46:52.759 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.760609374Z [31m20251126-14:46:52.760 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:46:52.761920672Z Traceback (most recent call last):
2025-11-26T14:46:52.764973437Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:46:52.764986477Z     future = loop.run_in_executor(
2025-11-26T14:46:52.764992447Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:52.767483922Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:46:52.769896148Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:46:52.769909258Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:46:52.772566204Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:46:52.775387469Z [31m20251126-14:46:52.775 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.776692807Z [37m20251126-14:46:52.776 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:46:52.778042144Z [31m20251126-14:46:52.777 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.779176352Z [37m20251126-14:46:52.779 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:46:52.780055941Z [31m20251126-14:46:52.779 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.780974369Z [37m20251126-14:46:52.780 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:46:52.781798158Z [31m20251126-14:46:52.781 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.782809846Z [31m20251126-14:46:52.782 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:46:52.783684035Z Traceback (most recent call last):
2025-11-26T14:46:52.785320612Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:46:52.785331882Z     future = loop.run_in_executor(
2025-11-26T14:46:52.785348312Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:52.786939939Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:46:52.788590106Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:46:52.788601286Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:46:52.790187154Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:46:52.792170540Z [31m20251126-14:46:52.792 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.793058119Z [37m20251126-14:46:52.792 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:46:52.793929917Z [31m20251126-14:46:52.793 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.794900376Z [37m20251126-14:46:52.794 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:46:52.795810704Z [31m20251126-14:46:52.795 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.796713693Z [37m20251126-14:46:52.796 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:46:52.797608651Z [31m20251126-14:46:52.797 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.798484810Z [31m20251126-14:46:52.798 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:46:52.799697088Z Traceback (most recent call last):
2025-11-26T14:46:52.801346315Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:46:52.801357405Z     future = loop.run_in_executor(
2025-11-26T14:46:52.801360325Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:52.802992772Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:46:52.804574639Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:46:52.804585809Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:46:52.806173697Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:46:52.808146683Z [31m20251126-14:46:52.808 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.809035202Z [37m20251126-14:46:52.808 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:46:52.809931910Z [31m20251126-14:46:52.809 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.810810799Z [37m20251126-14:46:52.810 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:46:52.811696737Z [31m20251126-14:46:52.811 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.812571986Z [37m20251126-14:46:52.812 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:46:52.813465234Z [31m20251126-14:46:52.813 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:46:52.814360283Z [31m20251126-14:46:52.814 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:46:52.815249641Z Traceback (most recent call last):
2025-11-26T14:46:52.817127998Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:46:52.817138338Z     future = loop.run_in_executor(
2025-11-26T14:46:52.817141138Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:52.818853755Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:46:52.820458252Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:46:52.820469092Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:46:52.822022880Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:46:53.969474137Z [rank0]:[W1126 14:46:53.095219426 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
2025-11-26T14:46:56.074304356Z E1126 14:46:56.072000 1038 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1104) of binary: /usr/bin/python3
2025-11-26T14:46:56.075163305Z Traceback (most recent call last):
2025-11-26T14:46:56.075186385Z   File "/usr/local/bin/torchrun", line 7, in <module>
2025-11-26T14:46:56.075305945Z     sys.exit(main())
2025-11-26T14:46:56.075320135Z              ^^^^^^
2025-11-26T14:46:56.075327265Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
2025-11-26T14:46:56.075654364Z     return f(*args, **kwargs)
2025-11-26T14:46:56.075670354Z            ^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:56.075677474Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 901, in main
2025-11-26T14:46:56.076385293Z     run(args)
2025-11-26T14:46:56.076419473Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 892, in run
2025-11-26T14:46:56.076821532Z     elastic_launch(
2025-11-26T14:46:56.076836982Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 143, in __call__
2025-11-26T14:46:56.076845142Z     return launch_agent(self._config, self._entrypoint, list(args))
2025-11-26T14:46:56.076917732Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:46:56.076933602Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
2025-11-26T14:46:56.077144112Z     raise ChildFailedError(
2025-11-26T14:46:56.077153012Z torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
2025-11-26T14:46:56.077159322Z ============================================================
2025-11-26T14:46:56.077165542Z examples/cloud_gsm8k/gsm8k_grpo_train.py FAILED
2025-11-26T14:46:56.077171852Z ------------------------------------------------------------
2025-11-26T14:46:56.077177212Z Failures:
2025-11-26T14:46:56.077182541Z   <NO_OTHER_FAILURES>
2025-11-26T14:46:56.077188311Z ------------------------------------------------------------
2025-11-26T14:46:56.077194081Z Root Cause (first observed failure):
2025-11-26T14:46:56.077199441Z [0]:
2025-11-26T14:46:56.077204901Z   time      : 2025-11-26_14:46:56
2025-11-26T14:46:56.077210361Z   host      : 01af0063c46e
2025-11-26T14:46:56.077216131Z   rank      : 0 (local_rank: 0)
2025-11-26T14:46:56.077221881Z   exitcode  : 1 (pid: 1104)
2025-11-26T14:46:56.077228581Z   error_file: <N/A>
2025-11-26T14:46:56.077234241Z   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
2025-11-26T14:46:56.077239991Z ============================================================
2025-11-26T14:46:57.542254009Z [37m20251126-14:46:57.541 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [387][0m
2025-11-26T14:46:57.856170215Z Killed
2025-11-26T14:46:57.859791089Z [37m20251126-14:46:57.859 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [1037][0m
2025-11-26T14:46:57.860957047Z Traceback (most recent call last):
2025-11-26T14:46:57.860967637Z   File "<frozen runpy>", line 198, in _run_module_as_main
2025-11-26T14:46:57.860972487Z   File "<frozen runpy>", line 88, in _run_code
2025-11-26T14:46:57.860976627Z   File "/workspace/AReaL/areal/launcher/local.py", line 405, in <module>
2025-11-26T14:46:57.861136227Z     main()
2025-11-26T14:46:57.861162747Z   File "/workspace/AReaL/areal/launcher/local.py", line 260, in main
2025-11-26T14:46:57.861249816Z     local_main(config, run_id=0)
2025-11-26T14:46:57.861274716Z   File "/workspace/AReaL/areal/launcher/local.py", line 399, in local_main
2025-11-26T14:46:57.861371956Z     raise e
2025-11-26T14:46:57.861377206Z   File "/workspace/AReaL/areal/launcher/local.py", line 375, in local_main
2025-11-26T14:46:57.861481116Z     launcher.wait(
2025-11-26T14:46:57.861487816Z   File "/workspace/AReaL/areal/launcher/local.py", line 235, in wait
2025-11-26T14:46:57.861549136Z     raise JobException(
2025-11-26T14:46:57.861554466Z areal.utils.launcher.JobException: Job gsm8k-grpo-cloud-fast_trial_20251126_144531:trainer JobState.COMPLETED at node local
2025-11-26T14:46:58.104791703Z [37m20251126-14:46:58.104 Local Scheduler INFO: Waiting for 0 local running processes, pids: [0m
2025-11-26T14:47:10.977496946Z ==========
2025-11-26T14:47:10.977525076Z == CUDA ==
2025-11-26T14:47:10.977600696Z ==========
2025-11-26T14:47:10.984986363Z CUDA Version 12.9.1
2025-11-26T14:47:10.988317187Z Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2025-11-26T14:47:10.991012243Z This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2025-11-26T14:47:10.991049343Z By pulling and using the container, you accept the terms and conditions of this license:
2025-11-26T14:47:10.991062403Z https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2025-11-26T14:47:10.991074753Z A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
2025-11-26T14:47:11.213094455Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:47:11.421173381Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:47:11.666926253Z Branch 'feature/reasoning_model' set up to track remote branch 'feature/reasoning_model' from 'origin'.
2025-11-26T14:47:11.666956803Z Your branch is up to date with 'origin/feature/reasoning_model'.
2025-11-26T14:47:11.769240779Z Checking AReaL installation...
2025-11-26T14:47:11.864687046Z AReaL already installed. Skipping installation.
2025-11-26T14:47:11.864709376Z Cleaning up any leftover GPU processes...
2025-11-26T14:47:14.897753296Z Checking for processes holding GPU device files...
2025-11-26T14:47:15.597542206Z Found processes holding GPU devices: 1
2025-11-26T14:47:15.597587806Z 144
2025-11-26T14:47:15.597594146Z 145
2025-11-26T14:47:15.597599196Z 93
2025-11-26T14:47:15.597604276Z Killing process 1...
2025-11-26T14:47:15.597609466Z Killing process 144...
2025-11-26T14:47:15.597760056Z Killing process 145...
2025-11-26T14:47:17.601456205Z Using fuser to kill processes on GPU devices...
2025-11-26T14:47:19.629194925Z Checking GPU...
2025-11-26T14:47:19.668605088Z NVIDIA A100 80GB PCIe, 81920 MiB
2025-11-26T14:47:19.696619031Z Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True for better memory management
2025-11-26T14:47:19.696663051Z Detected 1 GPU(s)
2025-11-26T14:47:19.696669751Z Checking GPU status...
2025-11-26T14:47:19.725682831Z index, name, utilization.gpu [%], memory.used [MiB], compute_mode
2025-11-26T14:47:19.732698009Z 0, NVIDIA A100 80GB PCIe, 0, 0, Default
2025-11-26T14:47:19.784514031Z Verifying GPU accessibility...
2025-11-26T14:47:20.586598637Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:47:20.586656897Z   import pynvml  # type: ignore[import]
2025-11-26T14:47:21.886991314Z GPU accessibility verified on attempt 1
2025-11-26T14:47:22.533368155Z Starting training...
2025-11-26T14:47:25.537942603Z Using FAST training configuration (20-30 minutes)
2025-11-26T14:47:25.542983885Z ==========================================
2025-11-26T14:47:25.542992035Z Starting GRPO Training (Cloud)
2025-11-26T14:47:25.542999415Z ==========================================
2025-11-26T14:47:25.543005815Z Config: fast
2025-11-26T14:47:25.543012995Z Config file: examples/cloud_gsm8k/gsm8k_grpo_fast.yaml
2025-11-26T14:47:25.543020105Z Training script: examples/cloud_gsm8k/gsm8k_grpo_train.py
2025-11-26T14:47:25.543026725Z Experiment: gsm8k-grpo-cloud-fast
2025-11-26T14:47:25.543033225Z Trial: trial_20251126_144725
2025-11-26T14:47:25.543038544Z GPU: NVIDIA A100 80GB PCIe (81920 MB)
2025-11-26T14:47:25.543043894Z WandB API key: e1adc5be02...
2025-11-26T14:47:25.543049204Z ==========================================
2025-11-26T14:47:26.586926889Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:47:26.587008108Z   import pynvml  # type: ignore[import]
2025-11-26T14:47:31.943790205Z [37m20251126-14:47:31.943 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:47:31.943850125Z [37m20251126-14:47:31.943 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:47:31.944384944Z [37m20251126-14:47:31.944 name-resolve INFO: No such name resolve path: ./tmp/areal/name_resolve/root/gsm8k-grpo-cloud-fast/trial_20251126_144725[0m
2025-11-26T14:47:32.115531703Z [37m20251126-14:47:32.115 Local Scheduler INFO: LocalLauncher: experiment_name=gsm8k-grpo-cloud-fast, trial_name=trial_20251126_144725, fileroot=/workspace/outputs/grpo, run_id=0, is_recover_run=False[0m
2025-11-26T14:47:32.128171661Z [37m20251126-14:47:32.127 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL CUDA_VISIBLE_DEVICES=0 stdbuf -oL python3 -m areal.launcher.sglang_server examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_144725 sglang.random_seed=1 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144725/llm_server.log[0m
2025-11-26T14:47:33.189240116Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:47:33.189274616Z   import pynvml  # type: ignore[import]
2025-11-26T14:47:35.119711602Z [37m20251126-14:47:35.119 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:47:35.119771132Z [37m20251126-14:47:35.119 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:47:35.260032023Z [37m20251126-14:47:35.258 SGLangServer Wrapper INFO: Launch command: python3 -m sglang.launch_server --host 172.18.0.2 --port 22687 --tokenizer-path Qwen/Qwen2.5-0.5B-Instruct --tokenizer-mode auto --load-format auto --trust-remote-code --device cuda --tp-size 1 --base-gpu-id 0 --nnodes 1 --node-rank 0 --dist-init-addr localhost:38122 --model-path Qwen/Qwen2.5-0.5B-Instruct --random-seed 1 --skip-tokenizer-init --disable-radix-cache --torch-compile-max-bs 32 --triton-attention-num-kv-splits 8 --num-continuous-decode-steps 1 --attention-backend flashinfer --context-length 4096 --mem-fraction-static 0.8 --chunked-prefill-size -1 --max-prefill-tokens 32768 --schedule-policy lpm --schedule-conservativeness 1.0 --cpu-offload-gb 0 --dtype bfloat16 --kv-cache-dtype auto --dp-size 1 --ep-size 1 --max-loaded-loras 1 --max-loras-per-batch 1 --lora-backend triton --log-level warning --log-level-http warning --log-requests-level 0 --enable-metrics --decode-log-interval 1[0m
2025-11-26T14:47:36.797437148Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:47:36.797473838Z   import pynvml  # type: ignore[import]
2025-11-26T14:47:43.823490385Z INFO 11-26 14:47:43 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:47:44.668758557Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:47:46.102690137Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:47:46.102738587Z   import pynvml  # type: ignore[import]
2025-11-26T14:47:46.121408755Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:47:46.121441145Z   import pynvml  # type: ignore[import]
2025-11-26T14:47:52.929515303Z INFO 11-26 14:47:52 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:47:52.975672954Z INFO 11-26 14:47:52 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:47:53.742658469Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:47:54.007941258Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:47:54.012076121Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:47:54.013019709Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:47:54.013966908Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:47:54.060841028Z [2025-11-26 14:47:54] MOE_RUNNER_BACKEND is not initialized, using triton backend
2025-11-26T14:47:54.693998631Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:47:54.694033281Z   warnings.warn(
2025-11-26T14:47:54.694043971Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:47:54.694050141Z   warnings.warn(
2025-11-26T14:47:56.082521419Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:47:56.262372573Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.57it/s]
2025-11-26T14:47:56.262409432Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.56it/s]
2025-11-26T14:47:59.720969269Z   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   4%|â–         | 1/23 [00:00<00:15,  1.41it/s]Capturing batches (bs=152 avail_mem=14.84 GB):   4%|â–         | 1/23 [00:00<00:15,  1.41it/s]Capturing batches (bs=144 avail_mem=14.82 GB):   4%|â–         | 1/23 [00:00<00:15,  1.41it/s]Capturing batches (bs=144 avail_mem=14.82 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.36it/s]Capturing batches (bs=136 avail_mem=14.81 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.36it/s]Capturing batches (bs=128 avail_mem=14.79 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.36it/s]Capturing batches (bs=128 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:00<00:02,  7.04it/s]Capturing batches (bs=120 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:00<00:02,  7.04it/s]Capturing batches (bs=112 avail_mem=14.78 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:01<00:02,  7.04it/s]Capturing batches (bs=112 avail_mem=14.78 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.35it/s]Capturing batches (bs=104 avail_mem=14.76 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.35it/s]Capturing batches (bs=96 avail_mem=14.75 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.35it/s] Capturing batches (bs=96 avail_mem=14.75 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.25it/s]Capturing batches (bs=88 avail_mem=14.73 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.25it/s]Capturing batches (bs=80 avail_mem=14.72 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.25it/s]Capturing batches (bs=80 avail_mem=14.72 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.76it/s]Capturing batches (bs=72 avail_mem=14.71 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.76it/s]Capturing batches (bs=64 avail_mem=14.69 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.76it/s]Capturing batches (bs=64 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 13.92it/s]Capturing batches (bs=56 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 13.92it/s]Capturing batches (bs=48 avail_mem=14.66 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 13.92it/s]Capturing batches (bs=48 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.88it/s]Capturing batches (bs=40 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.88it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.88it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.91it/s]Capturing batches (bs=24 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.91it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.91it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.55it/s]Capturing batches (bs=8 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.55it/s] Capturing batches (bs=4 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.55it/s]Capturing batches (bs=4 avail_mem=14.60 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:01<00:00, 14.62it/s]Capturing batches (bs=2 avail_mem=14.59 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:01<00:00, 14.62it/s]Capturing batches (bs=1 avail_mem=14.57 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:02<00:00, 14.62it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 15.09it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.04it/s]
2025-11-26T14:48:05.349707612Z [37m20251126-14:48:05.349 SGLangServer Wrapper INFO: SGLang server launched at: http://172.18.0.2:22687[0m
2025-11-26T14:48:06.136931933Z [37m20251126-14:48:06.136 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:22687[0m
2025-11-26T14:48:06.136975013Z [37m20251126-14:48:06.136 Local Scheduler INFO: LLM inference server launched at: AREAL_LLM_SERVER_ADDRS=172.18.0.2:22687[0m
2025-11-26T14:48:06.137564552Z [37m20251126-14:48:06.137 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL AREAL_LLM_SERVER_ADDRS=172.18.0.2:22687 AREAL_RECOVER_RUN=0 NCCL_CUMEM_ENABLE=0 NCCL_NVLS_ENABLE=0 CUDA_VISIBLE_DEVICES=0 stdbuf -oL torchrun --nnodes 1 --nproc-per-node 1 --master-addr localhost --master-port 47487 examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_144725 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144725/trainer.log[0m
2025-11-26T14:48:06.138455501Z [37m20251126-14:48:06.138 Local Scheduler INFO: Waiting for 2 local running processes, pids: 387 1037[0m
2025-11-26T14:48:06.883136934Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:48:06.883202344Z   import pynvml  # type: ignore[import]
2025-11-26T14:48:08.580049967Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:48:08.580088607Z   import pynvml  # type: ignore[import]
2025-11-26T14:48:17.598243124Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:48:17.598284254Z   warnings.warn(
2025-11-26T14:48:17.598291474Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:48:17.598297874Z   warnings.warn(
2025-11-26T14:48:20.145177001Z [37m20251126-14:48:20.144 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:48:20.145234281Z [37m20251126-14:48:20.144 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:48:20.434172930Z [37m20251126-14:48:20.433 [FSDP Engine Rank 0] INFO: Initializing device mesh with parallel dims (dp=1, sp=1, tp=1, ep=1, etp=1, world_size=1).[0m
2025-11-26T14:48:20.436655956Z [37m20251126-14:48:20.436 [FSDP Engine Rank 0] INFO: Data parallel head 0 and rank 0[0m
2025-11-26T14:48:22.336983443Z [FAST] Limiting dataset from 7473 to 200 samples
2025-11-26T14:48:22.340793426Z [37m20251126-14:48:22.340 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:22687[0m
2025-11-26T14:48:22.340819116Z [37m20251126-14:48:22.340 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:48:22.340864226Z [37m20251126-14:48:22.340 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:48:23.344533218Z [37m20251126-14:48:23.344 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:48:23.347715983Z [37m20251126-14:48:23.347 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:22687[0m
2025-11-26T14:48:23.347731923Z [37m20251126-14:48:23.347 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:48:23.347856133Z [37m20251126-14:48:23.347 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:48:24.351265786Z [37m20251126-14:48:24.350 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:48:25.329388052Z [37m20251126-14:48:25.328 [FSDP Engine Rank 0] INFO: Model creation and loading time: 0.7980463420972228[0m
2025-11-26T14:48:25.838302476Z [37m20251126-14:48:25.837 [FSDP Engine Rank 0] INFO: Applying FSDP2 with N-D parallelism for 0.51 seconds[0m
2025-11-26T14:48:25.839215594Z [37m20251126-14:48:25.838 [FSDP Engine Rank 0] INFO: Create optimizer time: 0.0008754790760576725[0m
2025-11-26T14:48:26.358449931Z wandb: Currently logged in as: tong-zhao (tong-zhao-georgia-institute-of-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-11-26T14:48:26.361993465Z wandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.
2025-11-26T14:48:27.207128757Z wandb: setting up run gsm8k-grpo-cloud-fast_trial_20251126_144725_train
2025-11-26T14:48:27.234444361Z wandb: Tracking run with wandb version 0.22.2
2025-11-26T14:48:27.234486211Z wandb: Run data is saved locally in /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144725/wandb/run-20251126_144826-gsm8k-grpo-cloud-fast_trial_20251126_144725_train
2025-11-26T14:48:27.234495051Z wandb: Run `wandb offline` to turn off syncing.
2025-11-26T14:48:27.234502341Z wandb: Syncing run trial_20251126_144725
2025-11-26T14:48:27.234508951Z wandb: â­ï¸ View project at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local
2025-11-26T14:48:27.234516371Z wandb: ðŸš€ View run at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local/runs/gsm8k-grpo-cloud-fast_trial_20251126_144725_train
2025-11-26T14:48:27.580830481Z wandb: Detected [openai] in use.
2025-11-26T14:48:27.581127751Z wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-11-26T14:48:27.581391690Z wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-11-26T14:48:27.586377872Z swanlab: SwanLab run disabled, the data will not be saved or uploaded.
2025-11-26T14:48:28.151359871Z ================================================================================
2025-11-26T14:48:28.151778270Z [FAST MODE]
2025-11-26T14:48:28.152327239Z   Dataset size: 200 samples (limited from 7473)
2025-11-26T14:48:28.152748438Z   Batch size: 8
2025-11-26T14:48:28.153149148Z   Steps per epoch: 25
2025-11-26T14:48:28.153511057Z   Total epochs: 1
2025-11-26T14:48:28.153946886Z   Total steps: 25
2025-11-26T14:48:28.154307386Z   Estimated time: ~25 minutes (~0.4 hours) at ~1 step/min
2025-11-26T14:48:28.154650745Z   Circuit breaker: Enabled (threshold: 50 consecutive zero rewards)
2025-11-26T14:48:28.154839385Z ================================================================================
2025-11-26T14:48:35.141761348Z [37m20251126-14:48:35.140 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4929, 540], padded to: [5120, 768], padding lengths: [191, 228][0m
2025-11-26T14:48:36.506610356Z [37m20251126-14:48:36.506 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 0.93, memory reserved (GB): 3.22, device memory used/total (GB): 69.68/79.25[0m
2025-11-26T14:48:36.668572731Z [37m20251126-14:48:36.668 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 0.93, memory reserved (GB): 3.22, device memory used/total (GB): 69.68/79.25[0m
2025-11-26T14:48:36.700599486Z [37m20251126-14:48:36.700 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4929, 540], padded to: [5120, 768], padding lengths: [191, 228][0m
2025-11-26T14:48:38.362766758Z [37m20251126-14:48:38.362 /workspace/AReaL/areal/utils/device.py INFO: ppo update, memory allocated (GB): 3.70, memory reserved (GB): 9.64, device memory used/total (GB): 76.61/79.25[0m
2025-11-26T14:48:41.604746503Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:48:41.751317434Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.84it/s]
2025-11-26T14:48:41.751355283Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.82it/s]
2025-11-26T14:48:41.772369058Z [37m20251126-14:48:41.771 [Remote Inference Engine Rank 0] INFO: Loading weights from disk done in 3.41s. Respond time: 0.12s.[0m
2025-11-26T14:48:41.781963151Z [92m20251126-14:48:41.781 StatsLogger INFO: Epoch 1/1 Step 1/25 Train step 1/25 done.[0m
2025-11-26T14:48:41.782545100Z [92m20251126-14:48:41.782 StatsLogger INFO: Stats (1/1):[0m
2025-11-26T14:48:41.795114759Z [92m20251126-14:48:41.794 StatsLogger INFO:
2025-11-26T14:48:41.795143179Z â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â••
2025-11-26T14:48:41.795149729Z â”‚ grpo_actor/actor_loss/avg        â”‚  4.5551e-04 â”‚ grpo_actor/entropy/min             â”‚  1.6069e-07 â”‚ grpo_actor/vocab_min_logits/min  â”‚ -3.3500e+01 â”‚ grpo_actor/no_eos_ratios/min    â”‚ 0.0000e+00 â”‚
2025-11-26T14:48:41.795155759Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795161329Z â”‚ grpo_actor/actor_loss/min        â”‚ -2.4416e+00 â”‚ grpo_actor/entropy/max             â”‚  4.1245e+00 â”‚ grpo_actor/vocab_min_logits/max  â”‚ -7.5625e+00 â”‚ grpo_actor/no_eos_ratios/max    â”‚ 1.0000e+00 â”‚
2025-11-26T14:48:41.795167729Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795173219Z â”‚ grpo_actor/actor_loss/max        â”‚  2.6596e+00 â”‚ grpo_actor/grad_norm               â”‚  3.8440e+00 â”‚ grpo_actor/advantages/avg        â”‚ -2.0609e-08 â”‚ grpo_actor/prompt_len/avg       â”‚ 9.8875e+01 â”‚
2025-11-26T14:48:41.795178559Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795184069Z â”‚ grpo_actor/approx_kl/avg         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/avg   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/min        â”‚ -2.1127e+00 â”‚ grpo_actor/prompt_len/min       â”‚ 8.4000e+01 â”‚
2025-11-26T14:48:41.795189499Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795211779Z â”‚ grpo_actor/approx_kl/min         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/min   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/max        â”‚  2.1812e+00 â”‚ grpo_actor/prompt_len/max       â”‚ 1.1400e+02 â”‚
2025-11-26T14:48:41.795217769Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795223159Z â”‚ grpo_actor/approx_kl/max         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/max   â”‚  1.0000e+00 â”‚ grpo_actor/behav_imp_weight_cap  â”‚  5.0000e+00 â”‚ grpo_actor/seq_len/avg          â”‚ 3.4181e+02 â”‚
2025-11-26T14:48:41.795228489Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795233839Z â”‚ grpo_actor/behave_approx_kl/avg  â”‚ -6.7722e-04 â”‚ grpo_actor/lr                      â”‚  1.7000e-05 â”‚ grpo_actor/correct_seq_len/avg   â”‚  3.1625e+02 â”‚ grpo_actor/seq_len/min          â”‚ 2.5900e+02 â”‚
2025-11-26T14:48:41.795239629Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795245169Z â”‚ grpo_actor/behave_approx_kl/min  â”‚ -2.2662e-01 â”‚ grpo_actor/n_tokens                â”‚  5.4690e+03 â”‚ grpo_actor/correct_seq_len/min   â”‚  2.5900e+02 â”‚ grpo_actor/seq_len/max          â”‚ 3.7000e+02 â”‚
2025-11-26T14:48:41.795250649Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795258219Z â”‚ grpo_actor/behave_approx_kl/max  â”‚  2.4801e-01 â”‚ grpo_actor/n_valid_tokens          â”‚  3.8870e+03 â”‚ grpo_actor/correct_seq_len/max   â”‚  3.5300e+02 â”‚ grpo_actor/task_reward/avg      â”‚ 2.5000e-01 â”‚
2025-11-26T14:48:41.795263689Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795274559Z â”‚ grpo_actor/behave_imp_weight/avg â”‚  9.9977e-01 â”‚ grpo_actor/new_logp/avg            â”‚ -3.5680e-01 â”‚ grpo_actor/eps_clip              â”‚  4.0000e-01 â”‚ grpo_actor/task_reward/min      â”‚ 0.0000e+00 â”‚
2025-11-26T14:48:41.795280269Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795306749Z â”‚ grpo_actor/behave_imp_weight/min â”‚  7.9722e-01 â”‚ grpo_actor/new_logp/min            â”‚ -1.6677e+01 â”‚ grpo_actor/final_reward/avg      â”‚  0.0000e+00 â”‚ grpo_actor/task_reward/max      â”‚ 1.0000e+00 â”‚
2025-11-26T14:48:41.795312049Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795317369Z â”‚ grpo_actor/behave_imp_weight/max â”‚  1.2815e+00 â”‚ grpo_actor/new_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/final_reward/min      â”‚ -7.0711e-01 â”‚ grpo_actor/use_dual_clip        â”‚ 0.0000e+00 â”‚
2025-11-26T14:48:41.795323029Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795328399Z â”‚ grpo_actor/clip_ratio/avg        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/avg            â”‚ -3.5612e-01 â”‚ grpo_actor/final_reward/max      â”‚  7.0711e-01 â”‚ timeperf/compute_advantage      â”‚ 1.6181e-01 â”‚
2025-11-26T14:48:41.795333949Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795339279Z â”‚ grpo_actor/clip_ratio/min        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/min            â”‚ -1.6653e+01 â”‚ grpo_actor/incorrect_seq_len/avg â”‚  3.5033e+02 â”‚ timeperf/recompute_logp         â”‚ 1.4749e+00 â”‚
2025-11-26T14:48:41.795344539Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795356429Z â”‚ grpo_actor/clip_ratio/max        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/incorrect_seq_len/min â”‚  2.8100e+02 â”‚ timeperf/rollout                â”‚ 6.8768e+00 â”‚
2025-11-26T14:48:41.795362109Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795367529Z â”‚ grpo_actor/clipped_tokens        â”‚  0.0000e+00 â”‚ grpo_actor/unclipped_behave_tokens â”‚  3.8870e+03 â”‚ grpo_actor/incorrect_seq_len/max â”‚  3.7000e+02 â”‚ timeperf/checkpoint_for_recover â”‚ 5.3700e-05 â”‚
2025-11-26T14:48:41.795372909Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795378369Z â”‚ grpo_actor/dual_clip_ratio/avg   â”‚  0.0000e+00 â”‚ grpo_actor/update_successful       â”‚  1.0000e+00 â”‚ grpo_actor/kl_rewards/avg        â”‚  0.0000e+00 â”‚ timeperf/eval                   â”‚ 9.6870e-05 â”‚
2025-11-26T14:48:41.795386479Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795391939Z â”‚ grpo_actor/dual_clip_ratio/min   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/avg    â”‚  2.5732e+01 â”‚ grpo_actor/kl_rewards/min        â”‚  0.0000e+00 â”‚ timeperf/save                   â”‚ 1.2753e-04 â”‚
2025-11-26T14:48:41.795397229Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795402529Z â”‚ grpo_actor/dual_clip_ratio/max   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/min    â”‚  1.2938e+01 â”‚ grpo_actor/kl_rewards/max        â”‚  0.0000e+00 â”‚ timeperf/train_step             â”‚ 1.6944e+00 â”‚
2025-11-26T14:48:41.795413589Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795419139Z â”‚ grpo_actor/dual_clipped_tokens   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/max    â”‚  3.9500e+01 â”‚ grpo_actor/mask_no_eos_with_zero â”‚  0.0000e+00 â”‚ timeperf/update_weights         â”‚ 3.4110e+00 â”‚
2025-11-26T14:48:41.795425608Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:48:41.795432938Z â”‚ grpo_actor/entropy/avg           â”‚  3.6670e-01 â”‚ grpo_actor/vocab_min_logits/avg    â”‚ -1.5886e+01 â”‚ grpo_actor/no_eos_ratios/avg     â”‚  1.2500e-01 â”‚ rollout/reward                  â”‚ 1.8750e-01 â”‚
2025-11-26T14:48:41.795438238Z â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•›[0m
2025-11-26T14:48:41.845808743Z [37m20251126-14:48:41.845 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [5030], padded to: [5120], padding lengths: [90][0m
2025-11-26T14:48:41.964160431Z [37m20251126-14:48:41.963 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 3.70, memory reserved (GB): 9.65, device memory used/total (GB): 76.50/79.25[0m
2025-11-26T14:48:42.072753527Z [37m20251126-14:48:42.072 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 3.70, memory reserved (GB): 9.65, device memory used/total (GB): 76.50/79.25[0m
2025-11-26T14:48:42.096512646Z [37m20251126-14:48:42.096 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [5030], padded to: [5120], padding lengths: [90][0m
2025-11-26T14:48:42.431080967Z Traceback (most recent call last):
2025-11-26T14:48:42.431999835Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:48:42.432028825Z     main(sys.argv[1:])
2025-11-26T14:48:42.432469595Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:48:42.432474565Z     stats = actor.ppo_update(batch)
2025-11-26T14:48:42.432477645Z             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:42.432928084Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:48:42.432952424Z     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:48:42.432960864Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:42.433367313Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:48:42.433392043Z     train_stat = self.engine.train_batch(
2025-11-26T14:48:42.433396533Z                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:42.433805942Z   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:48:42.433812452Z     loss.backward()
2025-11-26T14:48:42.434140672Z   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:48:42.434148062Z     torch.autograd.backward(
2025-11-26T14:48:42.434477311Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:48:42.434483371Z     _engine_run_backward(
2025-11-26T14:48:42.434813641Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:48:42.434826071Z     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:48:42.434832621Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:42.435190590Z torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.14 GiB is free. Process 1235700 has 64.81 GiB memory in use. Process 1236341 has 13.29 GiB memory in use. Of the allocated memory 11.10 GiB is allocated by PyTorch, and 121.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:48:42.438035915Z [rank0]: Traceback (most recent call last):
2025-11-26T14:48:42.438044415Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:48:42.438048385Z [rank0]:     main(sys.argv[1:])
2025-11-26T14:48:42.438051855Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:48:42.438055525Z [rank0]:     stats = actor.ppo_update(batch)
2025-11-26T14:48:42.438058725Z [rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:42.438061735Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:48:42.438065035Z [rank0]:     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:48:42.438068195Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:42.438071195Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:48:42.438074195Z [rank0]:     train_stat = self.engine.train_batch(
2025-11-26T14:48:42.438077255Z [rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:42.438080305Z [rank0]:   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:48:42.438083345Z [rank0]:     loss.backward()
2025-11-26T14:48:42.438086385Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:48:42.438089415Z [rank0]:     torch.autograd.backward(
2025-11-26T14:48:42.438092415Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:48:42.438097925Z [rank0]:     _engine_run_backward(
2025-11-26T14:48:42.438101065Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:48:42.438104965Z [rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:48:42.438108055Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:42.438112605Z [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.42 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.14 GiB is free. Process 1235700 has 64.81 GiB memory in use. Process 1236341 has 13.29 GiB memory in use. Of the allocated memory 11.10 GiB is allocated by PyTorch, and 121.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:48:44.122080560Z [31m20251126-14:48:44.121 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.128001580Z [37m20251126-14:48:44.127 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:48:44.130103007Z [31m20251126-14:48:44.129 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.132101383Z [37m20251126-14:48:44.131 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:48:44.134073290Z [31m20251126-14:48:44.133 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.136049796Z [37m20251126-14:48:44.135 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:48:44.138054813Z [31m20251126-14:48:44.137 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.139960260Z [31m20251126-14:48:44.139 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:48:44.141856037Z Traceback (most recent call last):
2025-11-26T14:48:44.146279259Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:48:44.146292609Z     future = loop.run_in_executor(
2025-11-26T14:48:44.146299949Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.149784573Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:48:44.153336707Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:48:44.153348627Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:48:44.156823871Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:48:44.199731958Z [1;34mwandb[0m:
2025-11-26T14:48:44.199767338Z [1;34mwandb[0m: ðŸš€ View run [33mtrial_20251126_144725[0m at: [34m[0m
2025-11-26T14:48:44.199774208Z [1;34mwandb[0m: Find logs at: [1;35m../outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144725/wandb/run-20251126_144826-gsm8k-grpo-cloud-fast_trial_20251126_144725_train/logs[0m
2025-11-26T14:48:44.396197284Z [31m20251126-14:48:44.395 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.398463790Z [37m20251126-14:48:44.398 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:48:44.400491937Z [31m20251126-14:48:44.400 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.402439443Z [37m20251126-14:48:44.402 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:48:44.404375130Z [31m20251126-14:48:44.404 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.406258417Z [37m20251126-14:48:44.406 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:48:44.408780672Z [31m20251126-14:48:44.408 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.410709699Z [31m20251126-14:48:44.410 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:48:44.412637656Z Traceback (most recent call last):
2025-11-26T14:48:44.416185770Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:48:44.416198880Z     future = loop.run_in_executor(
2025-11-26T14:48:44.416205730Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.419628634Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:48:44.423103958Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:48:44.423116838Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:48:44.426941722Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:48:44.495125106Z [31m20251126-14:48:44.494 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.497258882Z [37m20251126-14:48:44.496 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:48:44.499364858Z [31m20251126-14:48:44.499 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.501287755Z [37m20251126-14:48:44.500 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:48:44.503159942Z [31m20251126-14:48:44.502 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.505003719Z [37m20251126-14:48:44.504 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:48:44.506951415Z [31m20251126-14:48:44.506 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.508777802Z [31m20251126-14:48:44.508 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:48:44.510632229Z Traceback (most recent call last):
2025-11-26T14:48:44.514651772Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:48:44.514673122Z     future = loop.run_in_executor(
2025-11-26T14:48:44.514679852Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.518128096Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:48:44.521505841Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:48:44.521527431Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:48:44.525025175Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:48:44.536924554Z [31m20251126-14:48:44.535 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 26 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:48:44.536949764Z Traceback (most recent call last):
2025-11-26T14:48:44.536956394Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:48:44.536961914Z     result = await async_task
2025-11-26T14:48:44.536970114Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.536975434Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:48:44.536980914Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:48:44.536986934Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.536992214Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:48:44.536997574Z     reward = await self.async_reward_fn(
2025-11-26T14:48:44.537002824Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.537007994Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:48:44.537013194Z     raise e
2025-11-26T14:48:44.537019384Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:48:44.537024534Z     future = loop.run_in_executor(
2025-11-26T14:48:44.537029664Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.537035144Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:48:44.537040284Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:48:44.537045504Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:48:44.537050674Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:48:44.540167489Z [31m20251126-14:48:44.539 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 29 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:48:44.540189649Z Traceback (most recent call last):
2025-11-26T14:48:44.540195629Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:48:44.540201059Z     result = await async_task
2025-11-26T14:48:44.540206339Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.540211699Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:48:44.540217089Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:48:44.540236179Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.540241889Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:48:44.540247839Z     reward = await self.async_reward_fn(
2025-11-26T14:48:44.540253719Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.540258929Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:48:44.540264209Z     raise e
2025-11-26T14:48:44.540270219Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:48:44.540276549Z     future = loop.run_in_executor(
2025-11-26T14:48:44.540282839Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.540289349Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:48:44.540296209Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:48:44.540302819Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:48:44.540309259Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:48:44.543386023Z [31m20251126-14:48:44.542 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 30 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:48:44.543407443Z Traceback (most recent call last):
2025-11-26T14:48:44.543413503Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:48:44.543418913Z     result = await async_task
2025-11-26T14:48:44.543424353Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.543429603Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:48:44.543434953Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:48:44.543440203Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.543445653Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:48:44.543450973Z     reward = await self.async_reward_fn(
2025-11-26T14:48:44.543456163Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.543461303Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:48:44.543466533Z     raise e
2025-11-26T14:48:44.543471813Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:48:44.543477003Z     future = loop.run_in_executor(
2025-11-26T14:48:44.543482223Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.543487703Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:48:44.543493013Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:48:44.543498213Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:48:44.543503393Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:48:44.546493268Z [31m20251126-14:48:44.545 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.548374605Z [37m20251126-14:48:44.548 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:48:44.550316982Z [31m20251126-14:48:44.550 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.552151399Z [37m20251126-14:48:44.551 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:48:44.553986765Z [31m20251126-14:48:44.553 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.556134352Z [37m20251126-14:48:44.555 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:48:44.558287388Z [31m20251126-14:48:44.558 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.560119925Z [31m20251126-14:48:44.559 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:48:44.561937022Z Traceback (most recent call last):
2025-11-26T14:48:44.565331896Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:48:44.565345346Z     future = loop.run_in_executor(
2025-11-26T14:48:44.565363756Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.569110400Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:48:44.572327064Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:48:44.572340714Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:48:44.576067748Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:48:44.579966551Z [31m20251126-14:48:44.579 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.581811328Z [37m20251126-14:48:44.581 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:48:44.583593065Z [31m20251126-14:48:44.583 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.585388532Z [37m20251126-14:48:44.585 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:48:44.587671628Z [31m20251126-14:48:44.587 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.589173176Z [37m20251126-14:48:44.588 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:48:44.590974202Z [31m20251126-14:48:44.590 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.593123159Z [31m20251126-14:48:44.592 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:48:44.594606666Z Traceback (most recent call last):
2025-11-26T14:48:44.597243932Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:48:44.597256722Z     future = loop.run_in_executor(
2025-11-26T14:48:44.597262502Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.600125927Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:48:44.602646683Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:48:44.602662883Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:48:44.605144448Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:48:44.608943072Z [31m20251126-14:48:44.608 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.610312170Z [37m20251126-14:48:44.610 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:48:44.611724677Z [31m20251126-14:48:44.611 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.613116025Z [37m20251126-14:48:44.612 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:48:44.614525522Z [31m20251126-14:48:44.614 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.616457199Z [37m20251126-14:48:44.616 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:48:44.617867237Z [31m20251126-14:48:44.617 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.619282554Z [31m20251126-14:48:44.619 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:48:44.620910302Z Traceback (most recent call last):
2025-11-26T14:48:44.623712947Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:48:44.623728087Z     future = loop.run_in_executor(
2025-11-26T14:48:44.623732267Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.626722392Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:48:44.629365547Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:48:44.629388157Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:48:44.631734363Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:48:44.634794138Z [31m20251126-14:48:44.634 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.636109086Z [37m20251126-14:48:44.635 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:48:44.637436173Z [31m20251126-14:48:44.637 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.638731341Z [37m20251126-14:48:44.638 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:48:44.640051129Z [31m20251126-14:48:44.639 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.641342427Z [37m20251126-14:48:44.641 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:48:44.642662275Z [31m20251126-14:48:44.642 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.643970812Z [31m20251126-14:48:44.643 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:48:44.645274590Z Traceback (most recent call last):
2025-11-26T14:48:44.647968666Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:48:44.647981456Z     future = loop.run_in_executor(
2025-11-26T14:48:44.647987686Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.650177962Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:48:44.651835649Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:48:44.651846509Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:48:44.653518536Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:48:44.655468333Z [31m20251126-14:48:44.655 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.656590231Z [37m20251126-14:48:44.656 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:48:44.657535239Z [31m20251126-14:48:44.657 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.658459648Z [37m20251126-14:48:44.658 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:48:44.659386526Z [31m20251126-14:48:44.659 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.660311905Z [37m20251126-14:48:44.660 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:48:44.661244353Z [31m20251126-14:48:44.661 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:48:44.662169191Z [31m20251126-14:48:44.662 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:48:44.663094750Z Traceback (most recent call last):
2025-11-26T14:48:44.664998757Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:48:44.665009447Z     future = loop.run_in_executor(
2025-11-26T14:48:44.665012796Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:44.666702274Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:48:44.668372731Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:48:44.668383221Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:48:44.670049158Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:48:45.958583756Z [rank0]:[W1126 14:48:45.084379004 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
2025-11-26T14:48:48.029743432Z E1126 14:48:48.027000 1038 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1104) of binary: /usr/bin/python3
2025-11-26T14:48:48.030731311Z Traceback (most recent call last):
2025-11-26T14:48:48.030754690Z   File "/usr/local/bin/torchrun", line 7, in <module>
2025-11-26T14:48:48.030847980Z     sys.exit(main())
2025-11-26T14:48:48.030896110Z              ^^^^^^
2025-11-26T14:48:48.030913150Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
2025-11-26T14:48:48.031242090Z     return f(*args, **kwargs)
2025-11-26T14:48:48.031255750Z            ^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:48.031261570Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 901, in main
2025-11-26T14:48:48.031803109Z     run(args)
2025-11-26T14:48:48.031818119Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 892, in run
2025-11-26T14:48:48.032324428Z     elastic_launch(
2025-11-26T14:48:48.032353108Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 143, in __call__
2025-11-26T14:48:48.032418918Z     return launch_agent(self._config, self._entrypoint, list(args))
2025-11-26T14:48:48.032432258Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:48:48.032437838Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
2025-11-26T14:48:48.032698117Z     raise ChildFailedError(
2025-11-26T14:48:48.032711677Z torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
2025-11-26T14:48:48.032717387Z ============================================================
2025-11-26T14:48:48.032723377Z examples/cloud_gsm8k/gsm8k_grpo_train.py FAILED
2025-11-26T14:48:48.032729407Z ------------------------------------------------------------
2025-11-26T14:48:48.032734867Z Failures:
2025-11-26T14:48:48.032741077Z   <NO_OTHER_FAILURES>
2025-11-26T14:48:48.032746637Z ------------------------------------------------------------
2025-11-26T14:48:48.032752377Z Root Cause (first observed failure):
2025-11-26T14:48:48.032757757Z [0]:
2025-11-26T14:48:48.032763047Z   time      : 2025-11-26_14:48:48
2025-11-26T14:48:48.032768597Z   host      : 01af0063c46e
2025-11-26T14:48:48.032774137Z   rank      : 0 (local_rank: 0)
2025-11-26T14:48:48.032779327Z   exitcode  : 1 (pid: 1104)
2025-11-26T14:48:48.032784517Z   error_file: <N/A>
2025-11-26T14:48:48.032789777Z   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
2025-11-26T14:48:48.032795677Z ============================================================
2025-11-26T14:48:50.153336989Z [37m20251126-14:48:50.152 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [387][0m
2025-11-26T14:48:50.406372599Z Killed
2025-11-26T14:48:50.422939621Z [37m20251126-14:48:50.422 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [1037][0m
2025-11-26T14:48:50.424156709Z Traceback (most recent call last):
2025-11-26T14:48:50.424168069Z   File "<frozen runpy>", line 198, in _run_module_as_main
2025-11-26T14:48:50.424171799Z   File "<frozen runpy>", line 88, in _run_code
2025-11-26T14:48:50.424174439Z   File "/workspace/AReaL/areal/launcher/local.py", line 405, in <module>
2025-11-26T14:48:50.424337758Z     main()
2025-11-26T14:48:50.424388868Z   File "/workspace/AReaL/areal/launcher/local.py", line 260, in main
2025-11-26T14:48:50.424486748Z     local_main(config, run_id=0)
2025-11-26T14:48:50.424512998Z   File "/workspace/AReaL/areal/launcher/local.py", line 399, in local_main
2025-11-26T14:48:50.424562758Z     raise e
2025-11-26T14:48:50.424568178Z   File "/workspace/AReaL/areal/launcher/local.py", line 375, in local_main
2025-11-26T14:48:50.424651748Z     launcher.wait(
2025-11-26T14:48:50.424657558Z   File "/workspace/AReaL/areal/launcher/local.py", line 235, in wait
2025-11-26T14:48:50.424718628Z     raise JobException(
2025-11-26T14:48:50.424722018Z areal.utils.launcher.JobException: Job gsm8k-grpo-cloud-fast_trial_20251126_144725:trainer JobState.COMPLETED at node local
2025-11-26T14:48:50.675134252Z [37m20251126-14:48:50.674 Local Scheduler INFO: Waiting for 0 local running processes, pids: [0m
2025-11-26T14:49:04.602843529Z ==========
2025-11-26T14:49:04.602927749Z == CUDA ==
2025-11-26T14:49:04.602969249Z ==========
2025-11-26T14:49:04.610081277Z CUDA Version 12.9.1
2025-11-26T14:49:04.613366161Z Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2025-11-26T14:49:04.615777137Z This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2025-11-26T14:49:04.615784337Z By pulling and using the container, you accept the terms and conditions of this license:
2025-11-26T14:49:04.615790787Z https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2025-11-26T14:49:04.615802687Z A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
2025-11-26T14:49:04.834628824Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:49:05.041982032Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:49:05.258143214Z Branch 'feature/reasoning_model' set up to track remote branch 'feature/reasoning_model' from 'origin'.
2025-11-26T14:49:05.258189504Z Your branch is up to date with 'origin/feature/reasoning_model'.
2025-11-26T14:49:05.364873512Z Checking AReaL installation...
2025-11-26T14:49:05.455036169Z AReaL already installed. Skipping installation.
2025-11-26T14:49:05.455060019Z Cleaning up any leftover GPU processes...
2025-11-26T14:49:08.484262656Z Checking for processes holding GPU device files...
2025-11-26T14:49:09.176663288Z Found processes holding GPU devices: 1
2025-11-26T14:49:09.176711508Z 144
2025-11-26T14:49:09.176715768Z 145
2025-11-26T14:49:09.176720108Z 93
2025-11-26T14:49:09.176723348Z Killing process 1...
2025-11-26T14:49:09.176727228Z Killing process 144...
2025-11-26T14:49:09.176844228Z Killing process 145...
2025-11-26T14:49:11.181039398Z Using fuser to kill processes on GPU devices...
2025-11-26T14:49:13.209105408Z Checking GPU...
2025-11-26T14:49:13.247312653Z NVIDIA A100 80GB PCIe, 81920 MiB
2025-11-26T14:49:13.276506253Z Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True for better memory management
2025-11-26T14:49:13.276555013Z Detected 1 GPU(s)
2025-11-26T14:49:13.276563603Z Checking GPU status...
2025-11-26T14:49:13.306524992Z index, name, utilization.gpu [%], memory.used [MiB], compute_mode
2025-11-26T14:49:13.313950249Z 0, NVIDIA A100 80GB PCIe, 0, 0, Default
2025-11-26T14:49:13.369122556Z Verifying GPU accessibility...
2025-11-26T14:49:14.158121623Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:49:14.158170313Z   import pynvml  # type: ignore[import]
2025-11-26T14:49:15.377408669Z GPU accessibility verified on attempt 1
2025-11-26T14:49:15.951352782Z Starting training...
2025-11-26T14:49:18.955848781Z Using FAST training configuration (20-30 minutes)
2025-11-26T14:49:18.960909102Z ==========================================
2025-11-26T14:49:18.960915292Z Starting GRPO Training (Cloud)
2025-11-26T14:49:18.960920502Z ==========================================
2025-11-26T14:49:18.960925322Z Config: fast
2025-11-26T14:49:18.960931132Z Config file: examples/cloud_gsm8k/gsm8k_grpo_fast.yaml
2025-11-26T14:49:18.960935942Z Training script: examples/cloud_gsm8k/gsm8k_grpo_train.py
2025-11-26T14:49:18.960940482Z Experiment: gsm8k-grpo-cloud-fast
2025-11-26T14:49:18.960945042Z Trial: trial_20251126_144918
2025-11-26T14:49:18.960981632Z GPU: NVIDIA A100 80GB PCIe (81920 MB)
2025-11-26T14:49:18.960991492Z WandB API key: e1adc5be02...
2025-11-26T14:49:18.960996502Z ==========================================
2025-11-26T14:49:19.949532100Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:49:19.949579650Z   import pynvml  # type: ignore[import]
2025-11-26T14:49:24.996380274Z [37m20251126-14:49:24.995 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:49:24.996430064Z [37m20251126-14:49:24.996 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:49:24.996972323Z [37m20251126-14:49:24.996 name-resolve INFO: No such name resolve path: ./tmp/areal/name_resolve/root/gsm8k-grpo-cloud-fast/trial_20251126_144918[0m
2025-11-26T14:49:25.163709350Z [37m20251126-14:49:25.163 Local Scheduler INFO: LocalLauncher: experiment_name=gsm8k-grpo-cloud-fast, trial_name=trial_20251126_144918, fileroot=/workspace/outputs/grpo, run_id=0, is_recover_run=False[0m
2025-11-26T14:49:25.171781796Z [37m20251126-14:49:25.171 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL CUDA_VISIBLE_DEVICES=0 stdbuf -oL python3 -m areal.launcher.sglang_server examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_144918 sglang.random_seed=1 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144918/llm_server.log[0m
2025-11-26T14:49:26.161763742Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:49:26.161798622Z   import pynvml  # type: ignore[import]
2025-11-26T14:49:28.123825004Z [37m20251126-14:49:28.123 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:49:28.123865244Z [37m20251126-14:49:28.123 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:49:28.269734166Z [37m20251126-14:49:28.269 SGLangServer Wrapper INFO: Launch command: python3 -m sglang.launch_server --host 172.18.0.2 --port 40346 --tokenizer-path Qwen/Qwen2.5-0.5B-Instruct --tokenizer-mode auto --load-format auto --trust-remote-code --device cuda --tp-size 1 --base-gpu-id 0 --nnodes 1 --node-rank 0 --dist-init-addr localhost:43412 --model-path Qwen/Qwen2.5-0.5B-Instruct --random-seed 1 --skip-tokenizer-init --disable-radix-cache --torch-compile-max-bs 32 --triton-attention-num-kv-splits 8 --num-continuous-decode-steps 1 --attention-backend flashinfer --context-length 4096 --mem-fraction-static 0.8 --chunked-prefill-size -1 --max-prefill-tokens 32768 --schedule-policy lpm --schedule-conservativeness 1.0 --cpu-offload-gb 0 --dtype bfloat16 --kv-cache-dtype auto --dp-size 1 --ep-size 1 --max-loaded-loras 1 --max-loras-per-batch 1 --lora-backend triton --log-level warning --log-level-http warning --log-requests-level 0 --enable-metrics --decode-log-interval 1[0m
2025-11-26T14:49:29.737068579Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:49:29.737106109Z   import pynvml  # type: ignore[import]
2025-11-26T14:49:36.718594572Z INFO 11-26 14:49:36 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:49:37.785185837Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:49:39.075242552Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:49:39.075283162Z   import pynvml  # type: ignore[import]
2025-11-26T14:49:39.080764143Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:49:39.080798763Z   import pynvml  # type: ignore[import]
2025-11-26T14:49:45.855107058Z INFO 11-26 14:49:45 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:49:45.893956302Z INFO 11-26 14:49:45 [__init__.py:216] Automatically detected platform cuda.
2025-11-26T14:49:46.837608856Z `torch_dtype` is deprecated! Use `dtype` instead!
2025-11-26T14:49:47.207845036Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:49:47.212213669Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:49:47.213159837Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:49:47.214052716Z [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
2025-11-26T14:49:47.259927018Z [2025-11-26 14:49:47] MOE_RUNNER_BACKEND is not initialized, using triton backend
2025-11-26T14:49:47.904481421Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:49:47.904516561Z   warnings.warn(
2025-11-26T14:49:47.904524141Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:49:47.904530051Z   warnings.warn(
2025-11-26T14:49:49.529621906Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:49:49.720170352Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.26it/s]
2025-11-26T14:49:49.720207702Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.25it/s]
2025-11-26T14:49:53.194454881Z   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   0%|          | 0/23 [00:00<?, ?it/s]Capturing batches (bs=160 avail_mem=14.94 GB):   4%|â–         | 1/23 [00:00<00:15,  1.41it/s]Capturing batches (bs=152 avail_mem=14.84 GB):   4%|â–         | 1/23 [00:00<00:15,  1.41it/s]Capturing batches (bs=144 avail_mem=14.82 GB):   4%|â–         | 1/23 [00:00<00:15,  1.41it/s]Capturing batches (bs=144 avail_mem=14.82 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.37it/s]Capturing batches (bs=136 avail_mem=14.81 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.37it/s]Capturing batches (bs=128 avail_mem=14.79 GB):  13%|â–ˆâ–Ž        | 3/23 [00:00<00:04,  4.37it/s]Capturing batches (bs=128 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:00<00:02,  7.05it/s]Capturing batches (bs=120 avail_mem=14.79 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:00<00:02,  7.05it/s]Capturing batches (bs=112 avail_mem=14.78 GB):  22%|â–ˆâ–ˆâ–       | 5/23 [00:01<00:02,  7.05it/s]Capturing batches (bs=112 avail_mem=14.78 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.36it/s]Capturing batches (bs=104 avail_mem=14.76 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.36it/s]Capturing batches (bs=96 avail_mem=14.75 GB):  30%|â–ˆâ–ˆâ–ˆ       | 7/23 [00:01<00:01,  9.36it/s] Capturing batches (bs=96 avail_mem=14.75 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.29it/s]Capturing batches (bs=88 avail_mem=14.73 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.29it/s]Capturing batches (bs=80 avail_mem=14.72 GB):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 9/23 [00:01<00:01, 11.29it/s]Capturing batches (bs=80 avail_mem=14.72 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.82it/s]Capturing batches (bs=72 avail_mem=14.71 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.82it/s]Capturing batches (bs=64 avail_mem=14.69 GB):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 11/23 [00:01<00:00, 12.82it/s]Capturing batches (bs=64 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 13.99it/s]Capturing batches (bs=56 avail_mem=14.69 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 13.99it/s]Capturing batches (bs=48 avail_mem=14.66 GB):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 13/23 [00:01<00:00, 13.99it/s]Capturing batches (bs=48 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.91it/s]Capturing batches (bs=40 avail_mem=14.66 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.91it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 15/23 [00:01<00:00, 14.91it/s]Capturing batches (bs=32 avail_mem=14.65 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.96it/s]Capturing batches (bs=24 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.96it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 17/23 [00:01<00:00, 14.96it/s]Capturing batches (bs=16 avail_mem=14.63 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.59it/s]Capturing batches (bs=8 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.59it/s] Capturing batches (bs=4 avail_mem=14.60 GB):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 19/23 [00:01<00:00, 13.59it/s]Capturing batches (bs=4 avail_mem=14.60 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:01<00:00, 14.67it/s]Capturing batches (bs=2 avail_mem=14.59 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:01<00:00, 14.67it/s]Capturing batches (bs=1 avail_mem=14.57 GB):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 21/23 [00:02<00:00, 14.67it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 15.14it/s]Capturing batches (bs=1 avail_mem=14.57 GB): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:02<00:00, 11.07it/s]
2025-11-26T14:49:59.321118338Z [37m20251126-14:49:59.320 SGLangServer Wrapper INFO: SGLang server launched at: http://172.18.0.2:40346[0m
2025-11-26T14:50:00.190108490Z [37m20251126-14:50:00.189 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:40346[0m
2025-11-26T14:50:00.190177600Z [37m20251126-14:50:00.189 Local Scheduler INFO: LLM inference server launched at: AREAL_LLM_SERVER_ADDRS=172.18.0.2:40346[0m
2025-11-26T14:50:00.191032108Z [37m20251126-14:50:00.190 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL AREAL_LLM_SERVER_ADDRS=172.18.0.2:40346 AREAL_RECOVER_RUN=0 NCCL_CUMEM_ENABLE=0 NCCL_NVLS_ENABLE=0 CUDA_VISIBLE_DEVICES=0 stdbuf -oL torchrun --nnodes 1 --nproc-per-node 1 --master-addr localhost --master-port 44037 examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_144918 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144918/trainer.log[0m
2025-11-26T14:50:00.191856697Z [37m20251126-14:50:00.191 Local Scheduler INFO: Waiting for 2 local running processes, pids: 387 1037[0m
2025-11-26T14:50:00.946743923Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:50:00.946774993Z   import pynvml  # type: ignore[import]
2025-11-26T14:50:02.641733299Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:50:02.641791159Z   import pynvml  # type: ignore[import]
2025-11-26T14:50:11.753981207Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:50:11.754025856Z   warnings.warn(
2025-11-26T14:50:11.754033316Z /usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
2025-11-26T14:50:11.754039386Z   warnings.warn(
2025-11-26T14:50:14.348145553Z [37m20251126-14:50:14.347 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:50:14.348185643Z [37m20251126-14:50:14.347 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:50:14.628695056Z [37m20251126-14:50:14.628 [FSDP Engine Rank 0] INFO: Initializing device mesh with parallel dims (dp=1, sp=1, tp=1, ep=1, etp=1, world_size=1).[0m
2025-11-26T14:50:14.631203562Z [37m20251126-14:50:14.630 [FSDP Engine Rank 0] INFO: Data parallel head 0 and rank 0[0m
2025-11-26T14:50:16.569161825Z [FAST] Limiting dataset from 7473 to 200 samples
2025-11-26T14:50:16.572283249Z [37m20251126-14:50:16.572 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:40346[0m
2025-11-26T14:50:16.572310549Z [37m20251126-14:50:16.572 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:50:16.572385279Z [37m20251126-14:50:16.572 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:50:17.575843182Z [37m20251126-14:50:17.575 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:50:17.578758167Z [37m20251126-14:50:17.578 Launcher Utils INFO: Found 1 rollout servers: 172.18.0.2:40346[0m
2025-11-26T14:50:17.578786457Z [37m20251126-14:50:17.578 [Remote Inference Engine Rank 0] INFO: Get server addresses from name_resolve.[0m
2025-11-26T14:50:17.578799617Z [37m20251126-14:50:17.578 [Remote Inference Engine Rank 0] INFO: Waiting for server ready...[0m
2025-11-26T14:50:18.581745051Z [37m20251126-14:50:18.581 [Remote Inference Engine Rank 0] INFO: Servers are all ready![0m
2025-11-26T14:50:19.520818813Z [37m20251126-14:50:19.520 [FSDP Engine Rank 0] INFO: Model creation and loading time: 0.7879502000287175[0m
2025-11-26T14:50:19.975305240Z [37m20251126-14:50:19.974 [FSDP Engine Rank 0] INFO: Applying FSDP2 with N-D parallelism for 0.45 seconds[0m
2025-11-26T14:50:19.976208118Z [37m20251126-14:50:19.975 [FSDP Engine Rank 0] INFO: Create optimizer time: 0.000853759003803134[0m
2025-11-26T14:50:20.457642859Z wandb: Currently logged in as: tong-zhao (tong-zhao-georgia-institute-of-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
2025-11-26T14:50:20.460662704Z wandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.
2025-11-26T14:50:21.201545664Z wandb: setting up run gsm8k-grpo-cloud-fast_trial_20251126_144918_train
2025-11-26T14:50:21.448041274Z wandb: Tracking run with wandb version 0.22.2
2025-11-26T14:50:21.448073234Z wandb: Run data is saved locally in /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144918/wandb/run-20251126_145020-gsm8k-grpo-cloud-fast_trial_20251126_144918_train
2025-11-26T14:50:21.448094434Z wandb: Run `wandb offline` to turn off syncing.
2025-11-26T14:50:21.448100364Z wandb: Syncing run trial_20251126_144918
2025-11-26T14:50:21.448106454Z wandb: â­ï¸ View project at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local
2025-11-26T14:50:21.448113024Z wandb: ðŸš€ View run at https://wandb.ai/tong-zhao-georgia-institute-of-technology/gsm8k-grpo-local/runs/gsm8k-grpo-cloud-fast_trial_20251126_144918_train
2025-11-26T14:50:21.781303717Z wandb: Detected [openai] in use.
2025-11-26T14:50:21.781859786Z wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
2025-11-26T14:50:21.782155406Z wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-11-26T14:50:21.787219287Z swanlab: SwanLab run disabled, the data will not be saved or uploaded.
2025-11-26T14:50:22.352093526Z ================================================================================
2025-11-26T14:50:22.352661745Z [FAST MODE]
2025-11-26T14:50:22.353269094Z   Dataset size: 200 samples (limited from 7473)
2025-11-26T14:50:22.353752003Z   Batch size: 8
2025-11-26T14:50:22.354239703Z   Steps per epoch: 25
2025-11-26T14:50:22.354681142Z   Total epochs: 1
2025-11-26T14:50:22.355078501Z   Total steps: 25
2025-11-26T14:50:22.355659660Z   Estimated time: ~25 minutes (~0.4 hours) at ~1 step/min
2025-11-26T14:50:22.356248789Z   Circuit breaker: Enabled (threshold: 50 consecutive zero rewards)
2025-11-26T14:50:22.356542539Z ================================================================================
2025-11-26T14:50:29.972566992Z [37m20251126-14:50:29.971 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4960, 642], padded to: [5120, 768], padding lengths: [160, 126][0m
2025-11-26T14:50:31.330997091Z [37m20251126-14:50:31.330 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 0.93, memory reserved (GB): 3.22, device memory used/total (GB): 69.70/79.25[0m
2025-11-26T14:50:31.488536483Z [37m20251126-14:50:31.488 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 0.93, memory reserved (GB): 3.22, device memory used/total (GB): 69.70/79.25[0m
2025-11-26T14:50:31.520719408Z [37m20251126-14:50:31.520 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [4960, 642], padded to: [5120, 768], padding lengths: [160, 126][0m
2025-11-26T14:50:33.170933771Z [37m20251126-14:50:33.170 /workspace/AReaL/areal/utils/device.py INFO: ppo update, memory allocated (GB): 3.70, memory reserved (GB): 9.92, device memory used/total (GB): 76.91/79.25[0m
2025-11-26T14:50:36.448278365Z Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
2025-11-26T14:50:36.595434895Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.81it/s]
2025-11-26T14:50:36.595471045Z Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.79it/s]
2025-11-26T14:50:36.621429140Z [37m20251126-14:50:36.620 [Remote Inference Engine Rank 0] INFO: Loading weights from disk done in 3.45s. Respond time: 0.13s.[0m
2025-11-26T14:50:36.631268244Z [92m20251126-14:50:36.630 StatsLogger INFO: Epoch 1/1 Step 1/25 Train step 1/25 done.[0m
2025-11-26T14:50:36.631941933Z [92m20251126-14:50:36.631 StatsLogger INFO: Stats (1/1):[0m
2025-11-26T14:50:36.644852321Z [92m20251126-14:50:36.644 StatsLogger INFO:
2025-11-26T14:50:36.644895250Z â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â••
2025-11-26T14:50:36.644920380Z â”‚ grpo_actor/actor_loss/avg        â”‚  1.2779e-04 â”‚ grpo_actor/entropy/min             â”‚  1.1074e-07 â”‚ grpo_actor/vocab_min_logits/min  â”‚ -3.2250e+01 â”‚ grpo_actor/no_eos_ratios/min    â”‚ 0.0000e+00 â”‚
2025-11-26T14:50:36.644926950Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.644935110Z â”‚ grpo_actor/actor_loss/min        â”‚ -2.5446e+00 â”‚ grpo_actor/entropy/max             â”‚  4.5051e+00 â”‚ grpo_actor/vocab_min_logits/max  â”‚ -7.5625e+00 â”‚ grpo_actor/no_eos_ratios/max    â”‚ 1.0000e+00 â”‚
2025-11-26T14:50:36.644941790Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.644947310Z â”‚ grpo_actor/actor_loss/max        â”‚  2.4507e+00 â”‚ grpo_actor/grad_norm               â”‚  3.3836e+00 â”‚ grpo_actor/advantages/avg        â”‚ -2.7519e-08 â”‚ grpo_actor/prompt_len/avg       â”‚ 9.8875e+01 â”‚
2025-11-26T14:50:36.644953070Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.644958340Z â”‚ grpo_actor/approx_kl/avg         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/avg   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/min        â”‚ -2.0186e+00 â”‚ grpo_actor/prompt_len/min       â”‚ 8.4000e+01 â”‚
2025-11-26T14:50:36.644963540Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.644968790Z â”‚ grpo_actor/approx_kl/min         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/min   â”‚  1.0000e+00 â”‚ grpo_actor/advantages/max        â”‚  2.0912e+00 â”‚ grpo_actor/prompt_len/max       â”‚ 1.1400e+02 â”‚
2025-11-26T14:50:36.644974480Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.644985950Z â”‚ grpo_actor/approx_kl/max         â”‚  0.0000e+00 â”‚ grpo_actor/importance_weight/max   â”‚  1.0000e+00 â”‚ grpo_actor/behav_imp_weight_cap  â”‚  5.0000e+00 â”‚ grpo_actor/seq_len/avg          â”‚ 3.5012e+02 â”‚
2025-11-26T14:50:36.644991490Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.644996750Z â”‚ grpo_actor/behave_approx_kl/avg  â”‚  5.4456e-06 â”‚ grpo_actor/lr                      â”‚  1.7000e-05 â”‚ grpo_actor/correct_seq_len/avg   â”‚  3.2925e+02 â”‚ grpo_actor/seq_len/min          â”‚ 3.1300e+02 â”‚
2025-11-26T14:50:36.645002690Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.645008190Z â”‚ grpo_actor/behave_approx_kl/min  â”‚ -2.4917e-01 â”‚ grpo_actor/n_tokens                â”‚  5.6020e+03 â”‚ grpo_actor/correct_seq_len/min   â”‚  3.1300e+02 â”‚ grpo_actor/seq_len/max          â”‚ 3.7000e+02 â”‚
2025-11-26T14:50:36.645013470Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.645018650Z â”‚ grpo_actor/behave_approx_kl/max  â”‚  4.3997e-01 â”‚ grpo_actor/n_valid_tokens          â”‚  4.0200e+03 â”‚ grpo_actor/correct_seq_len/max   â”‚  3.4000e+02 â”‚ grpo_actor/task_reward/avg      â”‚ 2.5000e-01 â”‚
2025-11-26T14:50:36.645023820Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.645029030Z â”‚ grpo_actor/behave_imp_weight/avg â”‚  1.0005e+00 â”‚ grpo_actor/new_logp/avg            â”‚ -3.9460e-01 â”‚ grpo_actor/eps_clip              â”‚  4.0000e-01 â”‚ grpo_actor/task_reward/min      â”‚ 0.0000e+00 â”‚
2025-11-26T14:50:36.645034620Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.645045150Z â”‚ grpo_actor/behave_imp_weight/min â”‚  7.7944e-01 â”‚ grpo_actor/new_logp/min            â”‚ -1.5441e+01 â”‚ grpo_actor/final_reward/avg      â”‚  0.0000e+00 â”‚ grpo_actor/task_reward/max      â”‚ 1.0000e+00 â”‚
2025-11-26T14:50:36.645050460Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.645055900Z â”‚ grpo_actor/behave_imp_weight/max â”‚  1.5527e+00 â”‚ grpo_actor/new_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/final_reward/min      â”‚ -7.0711e-01 â”‚ grpo_actor/use_dual_clip        â”‚ 0.0000e+00 â”‚
2025-11-26T14:50:36.645061510Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.645066750Z â”‚ grpo_actor/clip_ratio/avg        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/avg            â”‚ -3.9461e-01 â”‚ grpo_actor/final_reward/max      â”‚  7.0711e-01 â”‚ timeperf/compute_advantage      â”‚ 1.5735e-01 â”‚
2025-11-26T14:50:36.645071900Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.645083230Z â”‚ grpo_actor/clip_ratio/min        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/min            â”‚ -1.5441e+01 â”‚ grpo_actor/incorrect_seq_len/avg â”‚  3.5708e+02 â”‚ timeperf/recompute_logp         â”‚ 1.4659e+00 â”‚
2025-11-26T14:50:36.645088430Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.645093650Z â”‚ grpo_actor/clip_ratio/max        â”‚  0.0000e+00 â”‚ grpo_actor/old_logp/max            â”‚  0.0000e+00 â”‚ grpo_actor/incorrect_seq_len/min â”‚  3.5100e+02 â”‚ timeperf/rollout                â”‚ 7.5085e+00 â”‚
2025-11-26T14:50:36.645105400Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.645110770Z â”‚ grpo_actor/clipped_tokens        â”‚  0.0000e+00 â”‚ grpo_actor/unclipped_behave_tokens â”‚  4.0200e+03 â”‚ grpo_actor/incorrect_seq_len/max â”‚  3.7000e+02 â”‚ timeperf/checkpoint_for_recover â”‚ 2.8310e-05 â”‚
2025-11-26T14:50:36.645115940Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.645121140Z â”‚ grpo_actor/dual_clip_ratio/avg   â”‚  0.0000e+00 â”‚ grpo_actor/update_successful       â”‚  1.0000e+00 â”‚ grpo_actor/kl_rewards/avg        â”‚  0.0000e+00 â”‚ timeperf/eval                   â”‚ 8.6530e-05 â”‚
2025-11-26T14:50:36.645126810Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.645132100Z â”‚ grpo_actor/dual_clip_ratio/min   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/avg    â”‚  2.5811e+01 â”‚ grpo_actor/kl_rewards/min        â”‚  0.0000e+00 â”‚ timeperf/save                   â”‚ 1.0346e-04 â”‚
2025-11-26T14:50:36.645137440Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.645142800Z â”‚ grpo_actor/dual_clip_ratio/max   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/min    â”‚  1.2938e+01 â”‚ grpo_actor/kl_rewards/max        â”‚  0.0000e+00 â”‚ timeperf/train_step             â”‚ 1.6825e+00 â”‚
2025-11-26T14:50:36.645148000Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.645159540Z â”‚ grpo_actor/dual_clipped_tokens   â”‚  0.0000e+00 â”‚ grpo_actor/vocab_max_logits/max    â”‚  4.1000e+01 â”‚ grpo_actor/mask_no_eos_with_zero â”‚  0.0000e+00 â”‚ timeperf/update_weights         â”‚ 3.4518e+00 â”‚
2025-11-26T14:50:36.645165520Z â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
2025-11-26T14:50:36.645170840Z â”‚ grpo_actor/entropy/avg           â”‚  3.9532e-01 â”‚ grpo_actor/vocab_min_logits/avg    â”‚ -1.5738e+01 â”‚ grpo_actor/no_eos_ratios/avg     â”‚  1.2500e-01 â”‚ rollout/reward                  â”‚ 2.2917e-01 â”‚
2025-11-26T14:50:36.645176070Z â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•›[0m
2025-11-26T14:50:36.694225206Z [37m20251126-14:50:36.693 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [5052], padded to: [5120], padding lengths: [68][0m
2025-11-26T14:50:37.065481095Z [37m20251126-14:50:37.065 /workspace/AReaL/areal/utils/device.py INFO: recompute logp, memory allocated (GB): 3.70, memory reserved (GB): 9.92, device memory used/total (GB): 76.81/79.25[0m
2025-11-26T14:50:38.739819476Z [37m20251126-14:50:38.739 /workspace/AReaL/areal/utils/device.py INFO: compute advantages, memory allocated (GB): 3.70, memory reserved (GB): 9.92, device memory used/total (GB): 76.81/79.25[0m
2025-11-26T14:50:38.896501970Z [37m20251126-14:50:38.896 [FSDP Engine Rank 0] INFO: Microbatch #tokens (rank 0): [5052], padded to: [5120], padding lengths: [68][0m
2025-11-26T14:50:39.613222290Z Traceback (most recent call last):
2025-11-26T14:50:39.613957319Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:50:39.613973139Z     main(sys.argv[1:])
2025-11-26T14:50:39.614392099Z   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:50:39.614423198Z     stats = actor.ppo_update(batch)
2025-11-26T14:50:39.614430058Z             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:39.614666988Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:50:39.614681198Z     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:50:39.614686938Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:39.615130407Z   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:50:39.615155137Z     train_stat = self.engine.train_batch(
2025-11-26T14:50:39.615161357Z                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:39.615505467Z   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:50:39.615513377Z     loss.backward()
2025-11-26T14:50:39.615981646Z   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:50:39.615995146Z     torch.autograd.backward(
2025-11-26T14:50:39.616404725Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:50:39.616412925Z     _engine_run_backward(
2025-11-26T14:50:39.616833284Z   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:50:39.616847524Z     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:50:39.616853694Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:39.617239424Z torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.43 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.09 GiB is free. Process 1238578 has 64.85 GiB memory in use. Process 1239273 has 13.30 GiB memory in use. Of the allocated memory 11.10 GiB is allocated by PyTorch, and 134.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:50:39.619484160Z [rank0]: Traceback (most recent call last):
2025-11-26T14:50:39.619508480Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 621, in <module>
2025-11-26T14:50:39.619516290Z [rank0]:     main(sys.argv[1:])
2025-11-26T14:50:39.619522760Z [rank0]:   File "/workspace/AReaL/examples/cloud_gsm8k/gsm8k_grpo_train.py", line 293, in main
2025-11-26T14:50:39.619529020Z [rank0]:     stats = actor.ppo_update(batch)
2025-11-26T14:50:39.619535690Z [rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:39.619542340Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 292, in ppo_update
2025-11-26T14:50:39.619549020Z [rank0]:     return self.actor.ppo_update(*args, **kwargs)
2025-11-26T14:50:39.619556200Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:39.619562660Z [rank0]:   File "/workspace/AReaL/areal/engine/ppo/actor.py", line 258, in ppo_update
2025-11-26T14:50:39.619569230Z [rank0]:     train_stat = self.engine.train_batch(
2025-11-26T14:50:39.619575570Z [rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:39.619582110Z [rank0]:   File "/workspace/AReaL/areal/engine/fsdp_engine.py", line 586, in train_batch
2025-11-26T14:50:39.619588360Z [rank0]:     loss.backward()
2025-11-26T14:50:39.619594380Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
2025-11-26T14:50:39.619600450Z [rank0]:     torch.autograd.backward(
2025-11-26T14:50:39.619606330Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
2025-11-26T14:50:39.619612810Z [rank0]:     _engine_run_backward(
2025-11-26T14:50:39.619619310Z [rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
2025-11-26T14:50:39.619625980Z [rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-26T14:50:39.619632240Z [rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:39.619640120Z [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.43 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.09 GiB is free. Process 1238578 has 64.85 GiB memory in use. Process 1239273 has 13.30 GiB memory in use. Of the allocated memory 11.10 GiB is allocated by PyTorch, and 134.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-11-26T14:50:40.806216621Z [31m20251126-14:50:40.805 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.806867780Z [37m20251126-14:50:40.806 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:50:40.807113760Z [31m20251126-14:50:40.807 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.807579409Z [37m20251126-14:50:40.807 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:50:40.808007128Z [31m20251126-14:50:40.807 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.808542167Z [37m20251126-14:50:40.808 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:50:40.808740517Z [31m20251126-14:50:40.808 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.808945556Z [31m20251126-14:50:40.808 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:50:40.809698175Z Traceback (most recent call last):
2025-11-26T14:50:40.810067335Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:40.810074095Z     future = loop.run_in_executor(
2025-11-26T14:50:40.810077795Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:40.810389394Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:40.810638484Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:40.810642114Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:40.810855093Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:40.811490232Z [31m20251126-14:50:40.811 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.811678742Z [37m20251126-14:50:40.811 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:50:40.811874361Z [31m20251126-14:50:40.811 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.812072291Z [37m20251126-14:50:40.811 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:50:40.812289941Z [31m20251126-14:50:40.812 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.812465300Z [37m20251126-14:50:40.812 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:50:40.812686440Z [31m20251126-14:50:40.812 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.812891630Z [31m20251126-14:50:40.812 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:50:40.813198789Z Traceback (most recent call last):
2025-11-26T14:50:40.813677508Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:40.813686498Z     future = loop.run_in_executor(
2025-11-26T14:50:40.813692498Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:40.813993128Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:40.814223397Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:40.814228787Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:40.814460187Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:40.815420915Z [31m20251126-14:50:40.815 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.815827865Z [37m20251126-14:50:40.815 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:50:40.816224854Z [31m20251126-14:50:40.816 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.816840103Z [37m20251126-14:50:40.816 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:50:40.817230362Z [31m20251126-14:50:40.817 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.817654342Z [37m20251126-14:50:40.817 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:50:40.818058801Z [31m20251126-14:50:40.817 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.818419220Z [31m20251126-14:50:40.818 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:50:40.818810440Z Traceback (most recent call last):
2025-11-26T14:50:40.819347829Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:40.819360719Z     future = loop.run_in_executor(
2025-11-26T14:50:40.819366169Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:40.819798118Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:40.820349527Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:40.820362437Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:40.820818426Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:40.821910834Z [31m20251126-14:50:40.821 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.822314474Z [37m20251126-14:50:40.822 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:50:40.823208882Z [31m20251126-14:50:40.823 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.823616201Z [37m20251126-14:50:40.823 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:50:40.824030891Z [31m20251126-14:50:40.823 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.824427020Z [37m20251126-14:50:40.824 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:50:40.824773969Z [31m20251126-14:50:40.824 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.825176229Z [31m20251126-14:50:40.824 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:50:40.825624768Z Traceback (most recent call last):
2025-11-26T14:50:40.826261917Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:40.826274457Z     future = loop.run_in_executor(
2025-11-26T14:50:40.826279977Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:40.826712556Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:40.827357855Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:40.827370625Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:40.828093184Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:40.829139122Z [31m20251126-14:50:40.828 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.829549231Z [37m20251126-14:50:40.829 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:50:40.830026230Z [31m20251126-14:50:40.829 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.830500560Z [37m20251126-14:50:40.830 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:50:40.830955699Z [31m20251126-14:50:40.830 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.831555908Z [37m20251126-14:50:40.831 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:50:40.831999077Z [31m20251126-14:50:40.831 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.832400736Z [31m20251126-14:50:40.832 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:50:40.832804116Z Traceback (most recent call last):
2025-11-26T14:50:40.833361225Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:40.833375745Z     future = loop.run_in_executor(
2025-11-26T14:50:40.833381195Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:40.833800774Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:40.834637483Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:40.834663383Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:40.835899541Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:40.837151498Z [31m20251126-14:50:40.836 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.837566188Z [37m20251126-14:50:40.837 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:50:40.838175867Z [31m20251126-14:50:40.837 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.839361105Z [37m20251126-14:50:40.839 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:50:40.844699016Z [31m20251126-14:50:40.844 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.846678192Z [37m20251126-14:50:40.846 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:50:40.848549779Z [31m20251126-14:50:40.848 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.850385906Z [31m20251126-14:50:40.850 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:50:40.852212323Z Traceback (most recent call last):
2025-11-26T14:50:40.855600537Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:40.855610577Z     future = loop.run_in_executor(
2025-11-26T14:50:40.855616107Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:40.859511440Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:40.862924385Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:40.862934375Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:40.866236809Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:40.870062722Z [31m20251126-14:50:40.869 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.871839269Z [37m20251126-14:50:40.871 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:50:40.873658436Z [31m20251126-14:50:40.873 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.875448593Z [37m20251126-14:50:40.875 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:50:40.877667609Z [31m20251126-14:50:40.877 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.879425766Z [37m20251126-14:50:40.879 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:50:40.881257603Z [31m20251126-14:50:40.881 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.883052940Z [31m20251126-14:50:40.882 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:50:40.884839787Z Traceback (most recent call last):
2025-11-26T14:50:40.888165472Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:40.888176932Z     future = loop.run_in_executor(
2025-11-26T14:50:40.888182412Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:40.891473186Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:40.895021220Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:40.895031210Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:40.898289124Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:40.902083158Z [31m20251126-14:50:40.901 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.903857835Z [37m20251126-14:50:40.903 Reward API INFO: Retrying... (attempt 1/4)[0m
2025-11-26T14:50:40.905663012Z [31m20251126-14:50:40.905 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.907207379Z [37m20251126-14:50:40.907 Reward API INFO: Retrying... (attempt 2/4)[0m
2025-11-26T14:50:40.908550547Z [31m20251126-14:50:40.908 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.909856405Z [37m20251126-14:50:40.909 Reward API INFO: Retrying... (attempt 3/4)[0m
2025-11-26T14:50:40.911694932Z [31m20251126-14:50:40.911 Reward API ERROR: Unexpected error in reward computation: cannot schedule new futures after shutdown[0m
2025-11-26T14:50:40.913014809Z [31m20251126-14:50:40.912 Reward API ERROR: Max retries exceeded for unexpected error.[0m
2025-11-26T14:50:40.914345717Z Traceback (most recent call last):
2025-11-26T14:50:40.916782873Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:40.916798873Z     future = loop.run_in_executor(
2025-11-26T14:50:40.916803073Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:40.919158609Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:40.921533825Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:40.921545625Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:40.924180720Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:41.018397170Z [31m20251126-14:50:41.016 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 25 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:50:41.018412880Z Traceback (most recent call last):
2025-11-26T14:50:41.018419090Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:50:41.018425000Z     result = await async_task
2025-11-26T14:50:41.018431140Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.018436720Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:50:41.018442410Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:50:41.018448370Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.018453940Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:50:41.018459730Z     reward = await self.async_reward_fn(
2025-11-26T14:50:41.018465280Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.018471280Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:50:41.018477170Z     raise e
2025-11-26T14:50:41.018483450Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:41.018489030Z     future = loop.run_in_executor(
2025-11-26T14:50:41.018494660Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.018500220Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:41.018505770Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:41.018511380Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:41.018517050Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:41.021162285Z [31m20251126-14:50:41.020 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 29 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:50:41.021188925Z Traceback (most recent call last):
2025-11-26T14:50:41.021194945Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:50:41.021200425Z     result = await async_task
2025-11-26T14:50:41.021205825Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.021211235Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:50:41.021216745Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:50:41.021222125Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.021227345Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:50:41.021232865Z     reward = await self.async_reward_fn(
2025-11-26T14:50:41.021238105Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.021244295Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:50:41.021250535Z     raise e
2025-11-26T14:50:41.021257065Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:41.021279805Z     future = loop.run_in_executor(
2025-11-26T14:50:41.021287005Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.021293845Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:41.021300745Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:41.021307545Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:41.021314235Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:41.024238090Z [31m20251126-14:50:41.023 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 27 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:50:41.024249840Z Traceback (most recent call last):
2025-11-26T14:50:41.024255570Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:50:41.024260930Z     result = await async_task
2025-11-26T14:50:41.024266240Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.024271480Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:50:41.024284850Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:50:41.024290100Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.024295270Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:50:41.024300650Z     reward = await self.async_reward_fn(
2025-11-26T14:50:41.024305800Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.024310920Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:50:41.024316070Z     raise e
2025-11-26T14:50:41.024321380Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:41.024326480Z     future = loop.run_in_executor(
2025-11-26T14:50:41.024331870Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.024337090Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:41.024342290Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:41.024347480Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:41.024352630Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:41.027400025Z [31m20251126-14:50:41.026 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 30 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:50:41.027425735Z Traceback (most recent call last):
2025-11-26T14:50:41.027431685Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:50:41.027437315Z     result = await async_task
2025-11-26T14:50:41.027443175Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.027448425Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:50:41.027453815Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:50:41.027459345Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.027464755Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:50:41.027470235Z     reward = await self.async_reward_fn(
2025-11-26T14:50:41.027476705Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.027482975Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:50:41.027489355Z     raise e
2025-11-26T14:50:41.027495935Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:41.027502695Z     future = loop.run_in_executor(
2025-11-26T14:50:41.027509225Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.027515755Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:41.027522404Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:41.027529234Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:41.027536114Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:41.030466429Z [31m20251126-14:50:41.029 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 28 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:50:41.030486759Z Traceback (most recent call last):
2025-11-26T14:50:41.030492259Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:50:41.030497589Z     result = await async_task
2025-11-26T14:50:41.030502939Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.030508319Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:50:41.030513479Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:50:41.030518889Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.030523989Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:50:41.030529199Z     reward = await self.async_reward_fn(
2025-11-26T14:50:41.030534349Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.030539489Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:50:41.030544629Z     raise e
2025-11-26T14:50:41.030549869Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:41.030563319Z     future = loop.run_in_executor(
2025-11-26T14:50:41.030568729Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.030573949Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:41.030579209Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:41.030584379Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:41.030589519Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:41.033575134Z [31m20251126-14:50:41.032 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 24 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:50:41.033600964Z Traceback (most recent call last):
2025-11-26T14:50:41.033607664Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:50:41.033613034Z     result = await async_task
2025-11-26T14:50:41.033618394Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.033623804Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:50:41.033629294Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:50:41.033634874Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.033640224Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:50:41.033645754Z     reward = await self.async_reward_fn(
2025-11-26T14:50:41.033652584Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.033658874Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:50:41.033665604Z     raise e
2025-11-26T14:50:41.033672414Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:41.033679264Z     future = loop.run_in_executor(
2025-11-26T14:50:41.033685984Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.033692524Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:41.033699374Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:41.033706504Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:41.033713374Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:41.036627989Z [31m20251126-14:50:41.035 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 31 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:50:41.036639029Z Traceback (most recent call last):
2025-11-26T14:50:41.036644549Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:50:41.036649849Z     result = await async_task
2025-11-26T14:50:41.036655099Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.036660379Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:50:41.036665609Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:50:41.036678659Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.036683969Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:50:41.036689269Z     reward = await self.async_reward_fn(
2025-11-26T14:50:41.036694369Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.036699469Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:50:41.036704589Z     raise e
2025-11-26T14:50:41.036709889Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:41.036715269Z     future = loop.run_in_executor(
2025-11-26T14:50:41.036720539Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.036725699Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:41.036730789Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:41.036735899Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:41.036741029Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:41.039724794Z [31m20251126-14:50:41.038 [Remote Inference Engine Rank 0] ERROR: AsyncTaskRunner: Task 26 failed with exception: cannot schedule new futures after shutdown [0m
2025-11-26T14:50:41.039750494Z Traceback (most recent call last):
2025-11-26T14:50:41.039756564Z   File "/workspace/AReaL/areal/core/async_task_runner.py", line 319, in _run_async_loop
2025-11-26T14:50:41.039762164Z     result = await async_task
2025-11-26T14:50:41.039767644Z              ^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.039772894Z   File "/workspace/AReaL/areal/core/workflow_executor.py", line 367, in _execute_workflow
2025-11-26T14:50:41.039778324Z     traj = await task_input.workflow.arun_episode(
2025-11-26T14:50:41.039783664Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.039789184Z   File "/workspace/AReaL/areal/workflow/rlvr.py", line 93, in arun_episode
2025-11-26T14:50:41.039794544Z     reward = await self.async_reward_fn(
2025-11-26T14:50:41.039800734Z              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.039807194Z   File "/workspace/AReaL/areal/api/reward_api.py", line 163, in __call__
2025-11-26T14:50:41.039813734Z     raise e
2025-11-26T14:50:41.039820464Z   File "/workspace/AReaL/areal/api/reward_api.py", line 118, in __call__
2025-11-26T14:50:41.039827314Z     future = loop.run_in_executor(
2025-11-26T14:50:41.039833924Z              ^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:41.039841424Z   File "uvloop/loop.pyx", line 2747, in uvloop.loop.Loop.run_in_executor
2025-11-26T14:50:41.039848214Z   File "/usr/lib/python3.12/concurrent/futures/process.py", line 807, in submit
2025-11-26T14:50:41.039855314Z     raise RuntimeError('cannot schedule new futures after shutdown')
2025-11-26T14:50:41.039862113Z RuntimeError: cannot schedule new futures after shutdown
2025-11-26T14:50:41.527943213Z [1;34mwandb[0m:
2025-11-26T14:50:41.527987173Z [1;34mwandb[0m: ðŸš€ View run [33mtrial_20251126_144918[0m at: [34m[0m
2025-11-26T14:50:41.527995083Z [1;34mwandb[0m: Find logs at: [1;35m../outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_144918/wandb/run-20251126_145020-gsm8k-grpo-cloud-fast_trial_20251126_144918_train/logs[0m
2025-11-26T14:50:43.407699065Z [rank0]:[W1126 14:50:43.533530274 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
2025-11-26T14:50:45.595051564Z E1126 14:50:45.593000 1038 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1104) of binary: /usr/bin/python3
2025-11-26T14:50:45.595984692Z Traceback (most recent call last):
2025-11-26T14:50:45.596007452Z   File "/usr/local/bin/torchrun", line 7, in <module>
2025-11-26T14:50:45.596113342Z     sys.exit(main())
2025-11-26T14:50:45.596146732Z              ^^^^^^
2025-11-26T14:50:45.596153372Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
2025-11-26T14:50:45.596401902Z     return f(*args, **kwargs)
2025-11-26T14:50:45.596413842Z            ^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:45.596418762Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 901, in main
2025-11-26T14:50:45.597010011Z     run(args)
2025-11-26T14:50:45.597051340Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 892, in run
2025-11-26T14:50:45.597719679Z     elastic_launch(
2025-11-26T14:50:45.597744329Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 143, in __call__
2025-11-26T14:50:45.597751239Z     return launch_agent(self._config, self._entrypoint, list(args))
2025-11-26T14:50:45.597757619Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-11-26T14:50:45.597763509Z   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
2025-11-26T14:50:45.597990389Z     raise ChildFailedError(
2025-11-26T14:50:45.598015619Z torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
2025-11-26T14:50:45.598022619Z ============================================================
2025-11-26T14:50:45.598029299Z examples/cloud_gsm8k/gsm8k_grpo_train.py FAILED
2025-11-26T14:50:45.598036049Z ------------------------------------------------------------
2025-11-26T14:50:45.598041569Z Failures:
2025-11-26T14:50:45.598048359Z   <NO_OTHER_FAILURES>
2025-11-26T14:50:45.598054479Z ------------------------------------------------------------
2025-11-26T14:50:45.598060349Z Root Cause (first observed failure):
2025-11-26T14:50:45.598066009Z [0]:
2025-11-26T14:50:45.598071629Z   time      : 2025-11-26_14:50:45
2025-11-26T14:50:45.598077279Z   host      : 01af0063c46e
2025-11-26T14:50:45.598082899Z   rank      : 0 (local_rank: 0)
2025-11-26T14:50:45.598088429Z   exitcode  : 1 (pid: 1104)
2025-11-26T14:50:45.598094019Z   error_file: <N/A>
2025-11-26T14:50:45.598099959Z   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
2025-11-26T14:50:45.598133049Z ============================================================
2025-11-26T14:50:46.285112280Z [37m20251126-14:50:46.284 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [387][0m
2025-11-26T14:50:46.568238988Z Killed
2025-11-26T14:50:46.579139050Z [37m20251126-14:50:46.578 Local Scheduler INFO: Stopping local process with signal SIGTERM, pid: [1037][0m
2025-11-26T14:50:46.580209478Z Traceback (most recent call last):
2025-11-26T14:50:46.580232358Z   File "<frozen runpy>", line 198, in _run_module_as_main
2025-11-26T14:50:46.580240548Z   File "<frozen runpy>", line 88, in _run_code
2025-11-26T14:50:46.580244548Z   File "/workspace/AReaL/areal/launcher/local.py", line 405, in <module>
2025-11-26T14:50:46.580565597Z     main()
2025-11-26T14:50:46.580585497Z   File "/workspace/AReaL/areal/launcher/local.py", line 260, in main
2025-11-26T14:50:46.580666447Z     local_main(config, run_id=0)
2025-11-26T14:50:46.580671417Z   File "/workspace/AReaL/areal/launcher/local.py", line 399, in local_main
2025-11-26T14:50:46.580775197Z     raise e
2025-11-26T14:50:46.580780827Z   File "/workspace/AReaL/areal/launcher/local.py", line 375, in local_main
2025-11-26T14:50:46.580871297Z     launcher.wait(
2025-11-26T14:50:46.580888467Z   File "/workspace/AReaL/areal/launcher/local.py", line 235, in wait
2025-11-26T14:50:46.581049226Z     raise JobException(
2025-11-26T14:50:46.581075126Z areal.utils.launcher.JobException: Job gsm8k-grpo-cloud-fast_trial_20251126_144918:trainer JobState.COMPLETED at node local
2025-11-26T14:50:46.811736214Z [37m20251126-14:50:46.810 Local Scheduler INFO: Waiting for 0 local running processes, pids: [0m
2025-11-26T14:51:05.144480256Z ==========
2025-11-26T14:51:05.144525896Z == CUDA ==
2025-11-26T14:51:05.144620935Z ==========
2025-11-26T14:51:05.150221706Z CUDA Version 12.9.1
2025-11-26T14:51:05.152451202Z Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2025-11-26T14:51:05.154221509Z This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2025-11-26T14:51:05.154252969Z By pulling and using the container, you accept the terms and conditions of this license:
2025-11-26T14:51:05.154268039Z https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2025-11-26T14:51:05.154300949Z A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.
2025-11-26T14:51:05.368792644Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:51:05.576717080Z Writing to /root/.config/pip/pip.conf
2025-11-26T14:51:05.799966881Z Branch 'feature/reasoning_model' set up to track remote branch 'feature/reasoning_model' from 'origin'.
2025-11-26T14:51:05.799998411Z Your branch is up to date with 'origin/feature/reasoning_model'.
2025-11-26T14:51:05.901652798Z Checking AReaL installation...
2025-11-26T14:51:05.992175714Z AReaL already installed. Skipping installation.
2025-11-26T14:51:05.992198384Z Cleaning up any leftover GPU processes...
2025-11-26T14:51:09.021136031Z Checking for processes holding GPU device files...
2025-11-26T14:51:09.711372136Z Found processes holding GPU devices: 1
2025-11-26T14:51:09.711429596Z 144
2025-11-26T14:51:09.711438326Z 145
2025-11-26T14:51:09.711444956Z 93
2025-11-26T14:51:09.711450776Z Killing process 1...
2025-11-26T14:51:09.711457526Z Killing process 144...
2025-11-26T14:51:09.711466736Z Killing process 145...
2025-11-26T14:51:11.715580407Z Using fuser to kill processes on GPU devices...
2025-11-26T14:51:13.742457188Z Checking GPU...
2025-11-26T14:51:13.781141253Z NVIDIA A100 80GB PCIe, 81920 MiB
2025-11-26T14:51:13.807420098Z Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True for better memory management
2025-11-26T14:51:13.807461908Z Detected 1 GPU(s)
2025-11-26T14:51:13.807468348Z Checking GPU status...
2025-11-26T14:51:13.838842864Z index, name, utilization.gpu [%], memory.used [MiB], compute_mode
2025-11-26T14:51:13.846457591Z 0, NVIDIA A100 80GB PCIe, 0, 0, Default
2025-11-26T14:51:13.900598099Z Verifying GPU accessibility...
2025-11-26T14:51:14.700191679Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:51:14.700251189Z   import pynvml  # type: ignore[import]
2025-11-26T14:51:15.914455203Z GPU accessibility verified on attempt 1
2025-11-26T14:51:16.491112082Z Starting training...
2025-11-26T14:51:19.494939472Z Using FAST training configuration (20-30 minutes)
2025-11-26T14:51:19.499548244Z ==========================================
2025-11-26T14:51:19.499554274Z Starting GRPO Training (Cloud)
2025-11-26T14:51:19.499559144Z ==========================================
2025-11-26T14:51:19.499563664Z Config: fast
2025-11-26T14:51:19.499568324Z Config file: examples/cloud_gsm8k/gsm8k_grpo_fast.yaml
2025-11-26T14:51:19.499573094Z Training script: examples/cloud_gsm8k/gsm8k_grpo_train.py
2025-11-26T14:51:19.499577254Z Experiment: gsm8k-grpo-cloud-fast
2025-11-26T14:51:19.499581214Z Trial: trial_20251126_145119
2025-11-26T14:51:19.499591744Z GPU: NVIDIA A100 80GB PCIe (81920 MB)
2025-11-26T14:51:19.499596184Z WandB API key: e1adc5be02...
2025-11-26T14:51:19.499600864Z ==========================================
2025-11-26T14:51:20.500645361Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:51:20.500706941Z   import pynvml  # type: ignore[import]
2025-11-26T14:51:25.557001719Z [37m20251126-14:51:25.556 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:51:25.557055839Z [37m20251126-14:51:25.556 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:51:25.557439968Z [37m20251126-14:51:25.557 name-resolve INFO: No such name resolve path: ./tmp/areal/name_resolve/root/gsm8k-grpo-cloud-fast/trial_20251126_145119[0m
2025-11-26T14:51:25.729012766Z [37m20251126-14:51:25.728 Local Scheduler INFO: LocalLauncher: experiment_name=gsm8k-grpo-cloud-fast, trial_name=trial_20251126_145119, fileroot=/workspace/outputs/grpo, run_id=0, is_recover_run=False[0m
2025-11-26T14:51:25.737658621Z [37m20251126-14:51:25.737 Local Scheduler INFO: Starting local process with command: TOKENIZERS_PARALLELISM=true PYTORCH_KERNEL_CACHE_PATH=/tmp/areal/.cache/root/torch/kernels/ TRITON_CACHE_DIR=/tmp/areal/.cache/root/triton/ VLLM_CACHE_ROOT=/tmp/areal/.cache/root/vllm/ CUDA_DEVICE_MAX_CONNECTIONS=1 PYTHONPATH=/workspace/AReaL:/workspace/AReaL CUDA_VISIBLE_DEVICES=0 stdbuf -oL python3 -m areal.launcher.sglang_server examples/cloud_gsm8k/gsm8k_grpo_train.py --config examples/cloud_gsm8k/gsm8k_grpo_fast.yaml experiment_name=gsm8k-grpo-cloud-fast trial_name=trial_20251126_145119 sglang.random_seed=1 2>&1 | tee -a /workspace/outputs/grpo/logs/root/gsm8k-grpo-cloud-fast/trial_20251126_145119/llm_server.log[0m
2025-11-26T14:51:26.818195193Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:51:26.818233503Z   import pynvml  # type: ignore[import]
2025-11-26T14:51:28.817456932Z [37m20251126-14:51:28.816 Platform init INFO: Detected CUDA device: NVIDIA A100 80GB PCIE[0m
2025-11-26T14:51:28.817489082Z [37m20251126-14:51:28.817 Platform init INFO: Initializing CUDA platform (NVIDIA).[0m
2025-11-26T14:51:28.965016301Z [37m20251126-14:51:28.964 SGLangServer Wrapper INFO: Launch command: python3 -m sglang.launch_server --host 172.18.0.2 --port 13200 --tokenizer-path Qwen/Qwen2.5-0.5B-Instruct --tokenizer-mode auto --load-format auto --trust-remote-code --device cuda --tp-size 1 --base-gpu-id 0 --nnodes 1 --node-rank 0 --dist-init-addr localhost:18466 --model-path Qwen/Qwen2.5-0.5B-Instruct --random-seed 1 --skip-tokenizer-init --disable-radix-cache --torch-compile-max-bs 32 --triton-attention-num-kv-splits 8 --num-continuous-decode-steps 1 --attention-backend flashinfer --context-length 4096 --mem-fraction-static 0.8 --chunked-prefill-size -1 --max-prefill-tokens 32768 --schedule-policy lpm --schedule-conservativeness 1.0 --cpu-offload-gb 0 --dtype bfloat16 --kv-cache-dtype auto --dp-size 1 --ep-size 1 --max-loaded-loras 1 --max-loras-per-batch 1 --lora-backend triton --log-level warning --log-level-http warning --log-requests-level 0 --enable-metrics --decode-log-interval 1[0m
2025-11-26T14:51:30.561124685Z /usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
2025-11-26T14:51:30.561156535Z   import pynvml  # type: ignore[import]