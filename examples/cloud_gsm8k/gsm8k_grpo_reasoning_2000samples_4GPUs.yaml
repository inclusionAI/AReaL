# Reasoning Model Training Configuration (4x A40 GPUs, 2000 Samples)
# This config trains a reasoning model using XML format: <reasoning>...</reasoning><answer>...</answer>
# Optimized for 4x A40 GPUs (48GB each) with 2000 samples from GSM8K dataset
# Based on Unsloth's reasoning model training approach

experiment_name: gsm8k-grpo-reasoning-4gpu-2000samples
trial_name: trial0

# Training configuration
max_train_samples: 2000  # 2000 samples for multi-GPU training
training_mode: "REASONING-2000-SAMPLES-4GPUS"
circuit_breaker_enabled: true
circuit_breaker_threshold: 10

seed: 1
total_train_epochs: 3  # 3 epochs for better convergence
tokenizer_path: ${actor.path}
async_training: true

cluster:
  n_nodes: 1
  n_gpus_per_node: 4  # 4 A40 GPUs
  fileroot: /workspace/outputs/grpo  # Absolute path to mounted volume
  name_resolve:
    type: nfs
    nfs_record_root: ./tmp/areal/name_resolve

# Allocation: 1 GPU for SGLang inference, 3 GPUs for training (actor + ref)
allocation_mode: sglang.d1t1p1+d3t1p1

rollout:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  max_concurrent_rollouts: 32  # Increased for multi-GPU
  queue_size: null
  consumer_batch_size: ${train_dataset.batch_size}
  max_head_offpolicyness: 2
  enable_rollout_tracing: false
  request_timeout: 7200  # 2 hours
  request_retries: 5
  setup_timeout: 300  # 5 minutes

gconfig:
  n_samples: 4  # More samples for better training
  min_new_tokens: 0
  max_new_tokens: 1024  # Increased to allow longer reasoning chains
  greedy: false
  temperature: 1.0

actor:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  path: Qwen/Qwen2.5-0.5B-Instruct
  init_from_scratch: false
  disable_dropout: true
  gradient_checkpointing: true  # Enable for A40 memory optimization
  dtype: bfloat16
  mb_spec:
    max_tokens_per_mb: 5120  # Optimized for A40
  optimizer:
    type: adam
    lr: 1.70e-5
    weight_decay: 0.017
    beta1: 0.9
    beta2: 0.999
    eps: 1e-8
    lr_scheduler_type: constant
    gradient_clipping: 1.0
    warmup_steps_proportion: 0.001
  backend: fsdp
  group_size: ${gconfig.n_samples}
  eps_clip: 0.4
  temperature: ${gconfig.temperature}
  reward_scaling: 10.0
  reward_bias: -0.5
  kl_ctl: 0.0
  ppo_n_minibatches: 1
  recompute_logprob: true
  use_decoupled_loss: true
  behav_imp_weight_cap: 5.0
  dynamic_sampling: false
  reward_norm:
    mean_level: group
    std_level: group
    group_size: ${gconfig.n_samples}
  adv_norm:
    mean_level: batch
    std_level: batch
  max_new_tokens: ${gconfig.max_new_tokens}

ref:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  path: ${actor.path}
  init_from_scratch: false
  disable_dropout: true
  dtype: ${actor.dtype}
  mb_spec:
    max_tokens_per_mb: 5120
  optimizer: null
  backend: fsdp

# SGLang - Optimized for A40 (48GB)
sglang:
  model_path: ${actor.path}
  random_seed: ${seed}
  skip_tokenizer_init: true
  dtype: ${actor.dtype}
  max_running_requests: null
  context_length: 4096
  mem_fraction_static: 0.6  # Reduced for A40 to leave room for inference
  attention_backend: flashinfer  # Required for RTX 5090 (SM 100+)

# datasets - Use reasoning variant by adding "-reasoning" to path
train_dataset:
  batch_size: 6  # Divisible by 3 (training world size)
  shuffle: true
  pin_memory: true
  num_workers: 0
  path: openai/gsm8k-reasoning  # "-reasoning" suffix triggers reasoning dataset loader
  type: rl
  max_length: 2048  # Increased to accommodate reasoning chains

valid_dataset:
  batch_size: 6
  shuffle: true
  pin_memory: true
  num_workers: 0
  path: openai/gsm8k-reasoning  # Use reasoning format for validation too
  type: rl
  max_length: 2048

# Utilities
saver:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  freq_epochs: 1
  freq_steps: null
  freq_secs: null

recover:
  mode: disabled
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  freq_epochs: 1
  freq_steps: null
  freq_secs: 3600

evaluator:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  freq_epochs: 1
  freq_steps: null
  freq_secs: null

stats_logger:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  wandb:
    mode: disabled  # Set to 'online' and provide WANDB_API_KEY to enable WandB logging
    wandb_api_key: ${oc.env:WANDB_API_KEY,""}
    project: gsm8k-grpo-reasoning-cloud

launcher:
  inference_server_cpus_per_gpu: 4
  inference_server_mem_per_gpu: 16384
  trainer_cpus_per_gpu: 4
  trainer_mem_per_gpu: 16384

