from dataclasses import dataclass, field
from typing import Dict, Iterable, List, Optional

import torch
from openai._types import Body
from openai.types.chat import ChatCompletion, ChatCompletionToolParam

from areal.api.io_struct import ModelResponse


@dataclass
class CompletionWithTokenLogpReward:
    """Internal structure to store completion with its reward."""

    completion: ChatCompletion
    response: ModelResponse
    messages: List[dict] = field(default_factory=list)
    reward: float | None = None
    parent: Optional["CompletionWithTokenLogpReward"] | None = None
    tools: Iterable[ChatCompletionToolParam] | None = None
    extra_body: Body | Dict = field(default_factory=dict)
    seq_tokens: List[int] | None = None

    def to_tensor_dict(self) -> Dict[str, torch.Tensor]:
        resp = self.response
        tokenizer = resp.tokenizer
        if self.parent:
            # NOTE: If we export the completion with concat style, we cannot directly
            # use resp.input_tokens + resp.output_tokens as the entire sequence,
            # because `tokenizer.apply_chat_template` may remove some tokens in the parent's output
            # (e.g., thinking outputs), making the input_tokens inconsistent with the real generation history.
            # To train with think tokens, we need to reconstruct the entire sequence using
            # the input messages provided by users and the actual messages generated by the model.
            # TODO: This is a hacky implementation, only tested for Qwen3 models,
            # think about other solutions.
            parent_res = self.parent.to_tensor_dict()
            assert self.parent.seq_tokens is not None, "Parent seq_tokens must be set."
            assert (
                tokenizer is not None
            ), "Tokenizer must be provided in ModelResponse if completion is exported with concat style."
            parent_messages = self.parent.messages
            new_input_tokens = tokenizer.apply_chat_template(
                self.messages[len(parent_messages) + 1 :],
                tools=self.tools,
                add_generation_prompt=True,
                tokenize=True,
                **self.extra_body.get("chat_template_kwargs", {}),
            )
            # This is the number of input tokens that are not part of the parent's sequence
            new_input_tokens_length = len(new_input_tokens)
            assert new_input_tokens_length >= 0, (
                f"New input tokens length must be non-negative if a parent is present, got {new_input_tokens_length}."
                "This usually indicates an unexpected behavior when tokenizer applying chat template."
                "Expected behaviors include removing thinking outputs and adding delimiter tokens."
            )
            # Complete the entire sequence including parent's output tokens
            # removed by tokenizer.apply_chat_template
            self.seq_tokens = seq = (
                self.parent.seq_tokens
                + resp.input_tokens[-new_input_tokens_length:]
                + resp.output_tokens
            )
            parent_logprobs = parent_res["logprobs"].squeeze(0).tolist()
            parent_loss_mask = parent_res["loss_mask"].squeeze(0).tolist()
            parent_versions = parent_res["versions"].squeeze(0).tolist()
            parent_len = len(parent_logprobs)
            assert parent_len == len(parent_loss_mask) == len(parent_versions)

            logprobs = (
                parent_logprobs + [0.0] * new_input_tokens_length + resp.output_logprobs
            )
            loss_mask = (
                parent_loss_mask + [0] * new_input_tokens_length + [1] * resp.output_len
            )
            versions = (
                parent_versions + [-1] * new_input_tokens_length + resp.output_versions
            )
        else:
            self.seq_tokens = seq = resp.input_tokens + resp.output_tokens
            logprobs = [0.0] * resp.input_len + resp.output_logprobs
            loss_mask = [0] * resp.input_len + [1] * resp.output_len
            versions = [-1] * resp.input_len + resp.output_versions
        reward = self.reward if self.reward is not None else 0.0
        return dict(
            # unsqueeze to add an additional batch dimension
            input_ids=torch.tensor(seq).unsqueeze(0),
            loss_mask=torch.tensor(loss_mask).unsqueeze(0),
            logprobs=torch.tensor(logprobs).unsqueeze(0),
            versions=torch.tensor(versions).unsqueeze(0),
            attention_mask=torch.ones(len(seq), dtype=torch.bool).unsqueeze(0),
            # reward
            rewards=torch.tensor([float(reward)]),
        )
