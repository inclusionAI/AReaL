diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py
index 452a6d5a..019ce39d 100644
--- a/python/sglang/srt/managers/scheduler.py
+++ b/python/sglang/srt/managers/scheduler.py
@@ -274,38 +274,43 @@ class Scheduler(
         context = zmq.Context(2)
         self.idle_sleeper = None
 
-        if self.pp_rank == 0 and self.attn_tp_rank == 0:
-            self.recv_from_tokenizer = get_zmq_socket(
-                context, zmq.PULL, port_args.scheduler_input_ipc_name, False
-            )
+        if self.attn_tp_rank == 0:
             self.send_to_tokenizer = get_zmq_socket(
                 context, zmq.PUSH, port_args.tokenizer_ipc_name, False
             )
             self.send_metrics_from_scheduler = get_zmq_socket(
                 context, zmq.PUSH, port_args.metrics_ipc_name, False
             )
-
-            if server_args.skip_tokenizer_init:
-                # Directly send to the TokenizerManager
-                self.send_to_detokenizer = get_zmq_socket(
-                    context, zmq.PUSH, port_args.tokenizer_ipc_name, False
-                )
-            else:
-                # Send to the DetokenizerManager
-                self.send_to_detokenizer = get_zmq_socket(
-                    context, zmq.PUSH, port_args.detokenizer_ipc_name, False
+            if self.pp_rank == 0:
+                self.recv_from_tokenizer = get_zmq_socket(
+                    context, zmq.PULL, port_args.scheduler_input_ipc_name, False
                 )
 
-            self.recv_from_rpc = get_zmq_socket(
-                context, zmq.DEALER, port_args.rpc_ipc_name, False
-            )
-            if self.server_args.sleep_on_idle:
-                self.idle_sleeper = IdleSleeper(
-                    [
-                        self.recv_from_tokenizer,
-                        self.recv_from_rpc,
-                    ]
+                if server_args.skip_tokenizer_init:
+                    # Directly send to the TokenizerManager
+                    self.send_to_detokenizer = get_zmq_socket(
+                        context, zmq.PUSH, port_args.tokenizer_ipc_name, False
+                    )
+                else:
+                    # Send to the DetokenizerManager
+                    self.send_to_detokenizer = get_zmq_socket(
+                        context, zmq.PUSH, port_args.detokenizer_ipc_name, False
+                    )
+
+                self.recv_from_rpc = get_zmq_socket(
+                    context, zmq.DEALER, port_args.rpc_ipc_name, False
                 )
+                if self.server_args.sleep_on_idle:
+                    self.idle_sleeper = IdleSleeper(
+                        [
+                            self.recv_from_tokenizer,
+                            self.recv_from_rpc,
+                        ]
+                    )
+            else:
+                self.recv_from_tokenizer = None
+                self.recv_from_rpc = None
+                self.send_to_detokenizer = SimpleNamespace(send_pyobj=lambda x: None)
         else:
             self.recv_from_tokenizer = None
             self.recv_from_rpc = None
@@ -1292,7 +1297,7 @@ class Scheduler(
         kv_metrics.gpu_prefix_cache_hit_rate = self.stats.cache_hit_rate
         kv_metrics.data_parallel_rank = self.dp_rank if self.dp_rank is not None else 0
 
-        if not self.send_metrics_from_scheduler.closed:
+        if self.send_metrics_from_scheduler and not self.send_metrics_from_scheduler.closed:
             self.send_metrics_from_scheduler.send_pyobj(kv_metrics)
 
     def log_prefill_stats(
@@ -1819,7 +1824,8 @@ class Scheduler(
             # This is used to prevent the health check signal being blocked by long context prefill.
             # However, one minor issue is that this code path does not check the status of detokenizer manager.
             self.return_health_check_ct -= 1
-            self.send_to_tokenizer.send_pyobj(HealthCheckOutput())
+            if self.pp_rank == 0:
+                self.send_to_tokenizer.send_pyobj(HealthCheckOutput())
 
     def prepare_mlp_sync_batch(self, local_batch: ScheduleBatch):
         return self.prepare_mlp_sync_batch_raw(
@@ -2235,8 +2241,9 @@ class Scheduler(
             # This only works for requests that have not started anything.
             # We still need to send something back to TokenizerManager to clean up the state.
             req = self.waiting_queue.pop(i)
-            self.send_to_tokenizer.send_pyobj(AbortReq(req.rid))
-            logger.debug(f"Abort queued request. {req.rid=}")
+            if self.pp_rank == 0:
+                self.send_to_tokenizer.send_pyobj(AbortReq(req.rid))
+                logger.debug(f"Abort queued request. {req.rid=}")
 
         # Delete the requests in the grammar queue
         for req in self.grammar_queue:
diff --git a/python/sglang/srt/managers/tokenizer_manager.py b/python/sglang/srt/managers/tokenizer_manager.py
index 81f36faa..ba3fea19 100644
--- a/python/sglang/srt/managers/tokenizer_manager.py
+++ b/python/sglang/srt/managers/tokenizer_manager.py
@@ -959,7 +959,7 @@ class TokenizerManager:
     ) -> Tuple[bool, str]:
         self.send_to_scheduler.send_pyobj(obj)
         self.model_update_result = asyncio.Future()
-        if self.server_args.dp_size == 1:
+        if self.server_args.dp_size == 1 and self.server_args.pp_size == 1:
             result = await self.model_update_result
             if result.success:
                 self.served_model_name = obj.model_path
@@ -967,7 +967,7 @@ class TokenizerManager:
                 self.server_args.load_format = obj.load_format
                 self.model_path = obj.model_path
             return result.success, result.message, result.num_paused_requests
-        else:  # self.server_args.dp_size > 1
+        else:  # self.server_args.dp_size > 1 or self.server_args.pp_size > 1
             self.model_update_tmp = []
             result = await self.model_update_result
 
@@ -1685,12 +1685,12 @@ class TokenizerManager:
         )
 
     def _handle_update_weights_from_disk_req_output(self, recv_obj):
-        if self.server_args.dp_size == 1:
+        if self.server_args.dp_size == 1 and self.server_args.pp_size == 1:
             self.model_update_result.set_result(recv_obj)
-        else:  # self.server_args.dp_size > 1
+        else:  # self.server_args.dp_size > 1 or self.server_args.pp_size > 1
             self.model_update_tmp.append(recv_obj)
             # set future if the all results are received
-            if len(self.model_update_tmp) == self.server_args.dp_size:
+            if len(self.model_update_tmp) == self.server_args.dp_size * self.server_args.pp_size:
                 self.model_update_result.set_result(self.model_update_tmp)
 
     async def score_request(
diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py
index 051f2b75..e307eb31 100644
--- a/python/sglang/srt/model_executor/model_runner.py
+++ b/python/sglang/srt/model_executor/model_runner.py
@@ -766,7 +766,7 @@ class ModelRunner:
         ), "Default torch process group must be initialized"
         assert group_name != "", "Group name cannot be empty"
 
-        rank = rank_offset + self.tp_rank
+        rank = rank_offset + self.tp_rank * self.pp_size + self.pp_rank
 
         logger.info(
             f"init custom process group: master_address={master_address}, master_port={master_port}, "
diff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py
index c44f53f7..9fd3d08c 100644
--- a/python/sglang/srt/server_args.py
+++ b/python/sglang/srt/server_args.py
@@ -1784,7 +1784,7 @@ class PortArgs:
         else:
             port = server_args.nccl_port
 
-        if not server_args.enable_dp_attention:
+        if not server_args.enable_dp_attention and server_args.pp_size == 1:
             # Normal case, use IPC within a single node
             return PortArgs(
                 tokenizer_ipc_name=f"ipc://{tempfile.NamedTemporaryFile(delete=False).name}",
