import unittest

import torch

from arealite.dataset.distributed_batch_memory import DistributedBatchMemory
from realhf.base.datapack import ffd_allocate


class TestSplitByGroups(unittest.TestCase):

    def test_split_by_groups(self):
        batch_size = 512
        group_size = 8
        n = 32

        data = {
            "input_ids": torch.arange(batch_size),
            "other_key": torch.randint(0, 2, (batch_size,)),
        }

        memory = DistributedBatchMemory(data)
        batches = memory.split_by_groups(group_size=group_size, n=n)
        print(f"batches: {batches}")
        self.assertEqual(len(batches), n)

        # 每组包含块数
        k = (batch_size // group_size) // n
        first_batch = batches[0]
        input_ids = first_batch["input_ids"]
        other_key = first_batch["other_key"]
        self.assertEqual(input_ids.shape[0], k * group_size)
        self.assertEqual(other_key.shape[0], k * group_size)
        # expected = tensor([0,1,2,3,4,5,6,7,256,257,258,259,260,261,262,263])
        expected = torch.cat([torch.arange(0, 8), torch.arange(256, 264)])
        print(f"input_ids: {input_ids}, expected: {expected}")

        self.assertTrue(torch.equal(input_ids, expected))

    def test_split_by_groups_with_invalid_total_chunks(self):
        # 测试总块数不能被 n 整除的情况
        data = {"input_ids": torch.arange(16)}

        memory = DistributedBatchMemory(data)

        with self.assertRaises(AssertionError):
            memory.split_by_groups(group_size=4, n=3)  # 16 / 4 = 4 chunks, 4 % 3 != 0

    def test_split_by_groups_with_invalid_group_size(self):
        # 测试 batch_size 不能被 group_size 整除的情况
        data = {"input_ids": torch.arange(15)}

        memory = DistributedBatchMemory(data)

        with self.assertRaises(AssertionError):
            memory.split_by_groups(group_size=4, n=2)

    def test_split_by_seqlen_ffd_helper(self):
        tensor = torch.tensor(
            [
                [1] * 1 + [0] * 9,
                [1] * 6 + [0] * 4,
                [1] * 3 + [0] * 7,
                [1] * 5 + [0] * 5,
                [1] * 3 + [0] * 7,
                [1] * 8 + [0] * 2,
                [1] * 10 + [0] * 0,  # [0]*0 is empty, equivalent to no zeros
                [1] * 5 + [0] * 5,
            ]
        )

        """
        输入长度为1, 6, 3, 5, 3, 8, 10, 5, 每2个为一个group，按group分组为[[1,6], [3,5], [3,8], [10,5]]
        group seq长度为相加 = [7,8,11,15], balance后[7,15], [8,11]放一起为最佳组合，输出
        ===> 'seqlen', tensor([10, 5, 1, 6])
        ===> 'seqlen', tensor([3, 8, 3, 5])
        """

        data = {"input_ids": tensor, "seqlen": torch.tensor([1, 6, 3, 5, 3, 8, 10, 5])}
        memory = DistributedBatchMemory(data)
        results = memory._split_by_seqlen_ffd_helper(2, 2)
        assert len(results) == 2
        print("-===========================")
        print(results[0]["seqlen"])
        print(results[1]["seqlen"])
        assert torch.equal(results[0]["seqlen"], torch.tensor([1, 6, 10, 5]))
        assert torch.equal(results[1]["seqlen"], torch.tensor([3, 5, 3, 8]))
        print(f"result: {results}")

    def test_split_by_seqlen_ffd(self):
        tensor = torch.tensor(
            [
                [1] * 1 + [0] * 9,
                [1] * 6 + [0] * 4,
                [1] * 3 + [0] * 7,
                [1] * 5 + [0] * 5,
                [1] * 3 + [0] * 7,
                [1] * 8 + [0] * 2,
                [1] * 10 + [0] * 0,  # [0]*0 is empty, equivalent to no zeros
                [1] * 5 + [0] * 5,
                [1] * 1 + [0] * 9,
                [1] * 6 + [0] * 4,
                [1] * 3 + [0] * 7,
                [1] * 5 + [0] * 5,
                [1] * 3 + [0] * 7,
                [1] * 8 + [0] * 2,
                [1] * 10 + [0] * 0,  # [0]*0 is empty, equivalent to no zeros
                [1] * 5 + [0] * 5,
            ]
        )

        data = {
            "input_ids": tensor,
            "seqlen": torch.tensor([1, 6, 3, 5, 3, 8, 10, 5, 1, 6, 3, 5, 3, 8, 10, 5]),
        }
        memory = DistributedBatchMemory(data)
        result = memory.split_by_seqlen_ffd(2, 2)
        print(f"result: {result}")

    def test_split_by_seqlen_ffd_2(self):

        sglang_output_length = [
            [
                123888,
                123888,
                94029,
                94013,
                123888,
                109085,
                108940,
                123888,
                123888,
                123888,
                108828,
                123888,
                108912,
                108881,
                94294,
                94167,
            ],
            [
                109025,
                108832,
                109007,
                123888,
                64239,
                64326,
                94386,
                109019,
                93943,
                94144,
                108916,
                109049,
                108955,
                109113,
                94008,
                123888,
            ],
            [
                108896,
                108894,
                49134,
                94002,
                94015,
                123888,
                108858,
                93756,
                78879,
                64548,
                79123,
                109201,
                93858,
                79043,
                79147,
                94105,
            ],
            [
                108976,
                64316,
                109030,
                109025,
                79212,
                109025,
                79048,
                79054,
                108885,
                108883,
                94120,
                94298,
                108870,
                123888,
                123888,
                109034,
            ],
        ]

        sglang_output = [
            [
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                554,
                15486,
                15486,
                559,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                600,
                15486,
                15486,
                15486,
                497,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                683,
                15486,
                15486,
                15486,
                15486,
                15486,
                538,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                426,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                510,
                15486,
                15486,
                15486,
                479,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                557,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                821,
                15486,
                15486,
                15486,
                635,
                15486,
                15486,
                15486,
                616,
            ],
            [
                623,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                430,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                605,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                607,
                15486,
                15486,
                570,
                15486,
                15486,
                559,
                559,
                670,
                15486,
                15486,
                15486,
                524,
                664,
                524,
                15486,
                15486,
                15486,
                694,
                15486,
                15486,
                15486,
                15486,
                776,
                15486,
                15486,
                15486,
                15486,
                617,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                630,
                397,
                15486,
                15486,
                15486,
                15486,
                726,
                502,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                514,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                647,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                553,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                711,
                15486,
                720,
                15486,
                15486,
                15486,
                15486,
                15486,
                372,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
            ],
            [
                15486,
                494,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                492,
                500,
                576,
                558,
                15486,
                423,
                15486,
                15486,
                619,
                15486,
                15486,
                532,
                15486,
                15486,
                15486,
                15486,
                554,
                623,
                15486,
                15486,
                15486,
                476,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                456,
                15486,
                15486,
                15486,
                15486,
                382,
                15486,
                15486,
                458,
                462,
                15486,
                15486,
                15486,
                571,
                15486,
                416,
                15486,
                15486,
                638,
                1032,
                611,
                15486,
                323,
                15486,
                15486,
                467,
                15486,
                15486,
                15486,
                419,
                807,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                799,
                15486,
                15486,
                358,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                584,
                429,
                448,
                15486,
                15486,
                15486,
                15486,
                15486,
                736,
                540,
                605,
                15486,
                15486,
                15486,
                15486,
                572,
                15486,
                15486,
                15486,
                15486,
                15486,
                665,
                15486,
                15486,
                524,
            ],
            [
                15486,
                574,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                693,
                15486,
                506,
                440,
                15486,
                733,
                15486,
                628,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                623,
                15486,
                15486,
                633,
                694,
                15486,
                455,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                623,
                15486,
                15486,
                15486,
                442,
                609,
                567,
                15486,
                15486,
                15486,
                15486,
                15486,
                586,
                365,
                15486,
                15486,
                15486,
                673,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                483,
                15486,
                15486,
                15486,
                481,
                15486,
                15486,
                15486,
                15486,
                15486,
                630,
                15486,
                15486,
                15486,
                15486,
                574,
                15486,
                15486,
                619,
                15486,
                15486,
                15486,
                15486,
                763,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                468,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                632,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
                15486,
            ],
        ]

        data = {"seqlen": torch.tensor(sglang_output).flatten()}
        memory = DistributedBatchMemory(data)
        results = memory.split_by_seqlen_ffd_2(8, 4)
        print(f"results: {results}")
        for result in results:
            print(result["seqlen"].view(-1, 8).sum(1))

    def test_ffd(self):
        sglang_output_length = [
            [
                123888,
                123888,
                94029,
                94013,
                123888,
                109085,
                108940,
                123888,
                123888,
                123888,
                108828,
                123888,
                108912,
                108881,
                94294,
                94167,
            ],
            [
                109025,
                108832,
                109007,
                123888,
                64239,
                64326,
                94386,
                109019,
                93943,
                94144,
                108916,
                109049,
                108955,
                109113,
                94008,
                123888,
            ],
            [
                108896,
                108894,
                49134,
                94002,
                94015,
                123888,
                108858,
                93756,
                78879,
                64548,
                79123,
                109201,
                93858,
                79043,
                79147,
                94105,
            ],
            [
                108976,
                64316,
                109030,
                109025,
                79212,
                109025,
                79048,
                79054,
                108885,
                108883,
                94120,
                94298,
                108870,
                123888,
                123888,
                109034,
            ],
        ]

        l = torch.tensor(sglang_output_length).flatten().tolist()

        indices = ffd_allocate(l, int(1e12), 4)
        print(f"indices: {indices}")
        for indexes in indices:
            print(f"result: {torch.tensor(l)[indexes]}")


if __name__ == "__main__":
    unittest.main()
