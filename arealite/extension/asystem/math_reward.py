import traceback
from typing import List, Union
from math_verify import parse, verify

# math-verify==0.5.2


def process_results(answer, solution):
    try:
        extracted_answer = parse(answer)
        extracted_solution = parse(solution)
        if len(extracted_answer) == 0 or len(extracted_solution) == 0:
            return 0, (extracted_answer, extracted_solution)
        ret = verify(extracted_solution, extracted_answer)
    except Exception as e:
        traceback.print_exc()
        print(
            f"failed to caculate the math result , answer:{answer}, solution: {solution}, error: {str(e)}, "
        )
        return 0, ("None", "None")
    return int(ret), ("", "")


def reward_fn(
    prompt: str,
    completion: str,
    prompt_ids: List[int],
    completion_ids: List[int],
    **kwargs,
):
    # prompt: prompt string (the task this data needs to complete)
    # completion: trajectory string generated by the model based on the task
    # prompt_ids: token ids of the prompt
    # completion_ids: token ids of the trajectory generated by the model
    # kwargs: all other attributes of this data in the dataset,
    #         for example, solutions, input_outputs, etc.

    solutions = kwargs.get("solutions")

    label = 0
    for sol in solutions:
        label, meta = label or process_results(completion, sol)

        print(f"ret:{label}, meta: {meta}")
    return label


if __name__ == "__main__":
    answer = "<answer>\n28\n</answer>"
    solutions = [
        "<answer>\n28.0\n</answer>",
    ]

    print(
        reward_fn(
            None,
            answer,
            None,
            None,
            solutions=solutions,
        )
    )
